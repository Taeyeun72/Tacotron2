{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784f4d7d",
   "metadata": {},
   "source": [
    "# Implementation of Tacotron2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501b1a4",
   "metadata": {},
   "source": [
    "- Tacotron2는 학부생 수준에서 **GitHub를 그대로 따라쓰지 않고**, 스스로 구현할 수 있는 부분을 최대화하는 것을 목표로 구현해보려고 한다.\n",
    "    1. GitHub의 기본적인 Module을 참고하면서 대략적인 모델 구조를 직접 그려본다.\n",
    "    2. 전처리는 Tacotron1에서 구현했던 방식과 유사하게 재구현한다.\n",
    "    3. GitHub를 참고하게 된다면, 다음 GitHub를 참고할 예정이다.\n",
    "        - **1. chldkato**: `https://github.com/chldkato/Tacotron-pytorch`\n",
    "        - **2. NVIDIA**: `https://github.com/NVIDIA/tacotron2`\n",
    "        - **3. hccho2**: `https://github.com/hccho2/Tacotron2-Wavenet-Korean-TTS`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caae241",
   "metadata": {},
   "source": [
    "- 먼저, NVIDIA의 모델 구조는 다음과 같다.\n",
    "<img src=\"NVIDIA Model Description.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373fbeeb",
   "metadata": {},
   "source": [
    "# 남은 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfb54b",
   "metadata": {},
   "source": [
    "- 이 파트는 구현하다가 기록해둬야 하는 내용들을 적는 곳이며, **구현이 모두 마무리되면 지운다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd2c48",
   "metadata": {},
   "source": [
    "[필수]  \n",
    "1. **Inference**가 아직 구현되지 않았다. 학습하기 전, 반드시 모든 코드에 대하여 추론 과정을 구현한다!\n",
    "2. **loss**를 구하도록 target을 만들지 않음. 학습하기 전, 반드시 구현해야 한다.\n",
    "\n",
    "[선택]  \n",
    "1. **Xavier 초기화**가 아직 구현되지 않았다. 구현을 마무리할 때 Xavier 초기화를 추가한다.\n",
    "2. **hyperparameter의 soft coding화**가 필요하다. magic number를 이용한 hard coding(하이퍼파라미터를 따로 변수로 만들지 않고 숫자로 사용하는 것)은 코드의 유지보수가 어려워질 수 있다.\n",
    "3. **SubPixelConvolution**레이어를 구현하지 않았다. Wavenet에서 local_condition을 upsampling할 때 transposed convolution layer가 제대로 작동하지 않을 경우 추가로 구현해서 대체할 필요가 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd536b6",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "- Tacotron 1에서의 구현을 가져오기로 한다.\n",
    "- chldkato Github: `https://github.com/chldkato/Tacotron-pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500e6ee",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca555a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, librosa, re, glob, scipy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import numpy as np\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3629a1",
   "metadata": {},
   "source": [
    "## 1.0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e7d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2. Audio Preprocessing\n",
    "sample_rate = 22050\n",
    "preemphasis = 0.97\n",
    "n_fft = 1024\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "ref_db = 20\n",
    "max_db = 100\n",
    "mel_dim = 80\n",
    "\n",
    "# Major Hyperparameters\n",
    "batch_size = 64\n",
    "checkpoint_step = 100\n",
    "\n",
    "# 2.1. Encoder\n",
    "symbol_length = 70 # len(symbols) = 70 (PAD + EOS + VALID_CHARS)\n",
    "embedding_dim = 512\n",
    "\n",
    "# 2.2. Decoder\n",
    "\n",
    "# 2.3. WaveNet\n",
    "dilations = [1, 2, 4, 8, 16, 32]*2\n",
    "upsampling_factors = [16, 16] # 곱이 반드시 hop_length와 일치해야 함. 16 x 16 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530bab7",
   "metadata": {},
   "source": [
    "## 1.1. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bac740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jamo in c:\\users\\poco\\anaconda3\\lib\\site-packages (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install jamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218c1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jamo import hangul_to_jamo\n",
    "\n",
    "PAD = '_'\n",
    "EOS = '~'\n",
    "SPACE = ' '\n",
    "\n",
    "JAMO_LEADS = \"\".join([chr(_) for _ in range(0x1100, 0x1113)])\n",
    "JAMO_VOWELS = \"\".join([chr(_) for _ in range(0x1161, 0x1176)])\n",
    "JAMO_TAILS = \"\".join([chr(_) for _ in range(0x11A8, 0x11C3)])\n",
    "\n",
    "VALID_CHARS = JAMO_LEADS + JAMO_VOWELS + JAMO_TAILS + SPACE\n",
    "symbols = PAD + EOS + VALID_CHARS\n",
    "\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "\n",
    "# text를 초성, 중성, 종성으로 분리하여 id로 반환하는 함수\n",
    "def text_to_sequence(text):\n",
    "    sequence = []\n",
    "    if not 0x1100 <= ord(text[0]) <= 0x1113:\n",
    "        text = ''.join(list(hangul_to_jamo(text)))\n",
    "    for s in text:\n",
    "        sequence.append(_symbol_to_id[s])\n",
    "    sequence.append(_symbol_to_id['~'])\n",
    "    return sequence\n",
    "\n",
    "def sequence_to_text(sequence):\n",
    "    result = ''\n",
    "    for symbol_id in sequence:\n",
    "        if symbol_id in _id_to_symbol:\n",
    "            s = _id_to_symbol[symbol_id]\n",
    "            result += s\n",
    "    return result.replace('}{', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b02b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 12854/12854 [00:06<00:00, 1949.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, librosa, re, glob, scipy\n",
    "from tqdm import tqdm\n",
    "\n",
    "text_dir = './archive/transcript.v.1.4.txt'\n",
    "filters = '([.,!?])'\n",
    "\n",
    "metadata = pd.read_csv(text_dir, dtype='object', sep='|', header=None)\n",
    "text = metadata[3].values\n",
    "\n",
    "out_dir = './data'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "os.makedirs(out_dir + '/text', exist_ok=True)\n",
    "os.makedirs(out_dir + '/wav', exist_ok=True)\n",
    "os.makedirs(out_dir + '/mel', exist_ok=True)\n",
    "# os.makedirs(out_dir + '/dec', exist_ok=True) # Tacotron2 내부에서 이미 구현함.\n",
    "# os.makedirs(out_dir + '/spec', exist_ok=True) # Tacotron2에서는 필요하지 않음.\n",
    "\n",
    "# text\n",
    "print('Load Text')\n",
    "text_len = []\n",
    "for idx, s in enumerate(tqdm(text)):\n",
    "    # 문자열에서 정규표현식을 이용하여 특정 문자열을 필터링하고,\n",
    "    # 이를 빈 문자열('')로 대체한다.\n",
    "    sentence = re.sub(re.compile(filters), '', s)\n",
    "    sentence = text_to_sequence(sentence)\n",
    "    \n",
    "    text_len.append(len(sentence))\n",
    "    text_name = 'kss-text-%05d.npy' % idx\n",
    "    np.save(os.path.join(out_dir + '/text', text_name), sentence, allow_pickle=False)\n",
    "np.save(os.path.join(out_dir + '/text_len.npy'), np.array(text_len))\n",
    "print('Text Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310f66c",
   "metadata": {},
   "source": [
    "## 1.2. Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e46c7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 12854/12854 [16:07<00:00, 13.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# audio\n",
    "wav_dir = metadata[0].values\n",
    "\n",
    "print('Load Audio')\n",
    "mel_len_list = []\n",
    "for idx, fn in enumerate(tqdm(wav_dir)):\n",
    "    file_dir = './archive/kss/' + fn\n",
    "    wav, _ = librosa.load(file_dir, sr=sample_rate)\n",
    "    wav, _ = librosa.effects.trim(wav) # 묵음 제거\n",
    "\n",
    "    # y[n] = x[n] - preemphasis * x[n-1]\n",
    "    # 아래 필터는 고주파 성분 강조를 통해 음성 인식 성능을 향상시킨다.\n",
    "    filtered_wav = scipy.signal.lfilter([1, -preemphasis], [1], wav)\n",
    "    # stft 결과값(복소수) (진폭 정보를 추출)\n",
    "    stft = librosa.stft(filtered_wav, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "    stft = np.abs(stft)\n",
    "    mel_filter = librosa.filters.mel(sample_rate, n_fft, mel_dim)\n",
    "    mel_spec = np.dot(mel_filter, stft) # Mel-spec 생성\n",
    "    \n",
    "    # dB 스케일(dB scale)을 적용 (사람의 청각 특성에 맞게 설계된 척도)\n",
    "    mel_spec = 20 * np.log10(np.maximum(1e-5, mel_spec))\n",
    "    stft = 20 * np.log10(np.maximum(1e-5, stft))\n",
    "    \n",
    "    # 정규화 및 클리핑\n",
    "    mel_spec = np.clip((mel_spec - ref_db + max_db) / max_db, 1e-8, 1)\n",
    "    stft = np.clip((stft - ref_db + max_db) / max_db, 1e-8, 1)\n",
    "    \n",
    "    mel_spec = mel_spec.T.astype(np.float32) # (Frames, 80)\n",
    "    stft = stft.T.astype(np.float32) # (Frames, 513)\n",
    "    \n",
    "    mel_len_list.append([mel_spec.shape[0], idx]) # Stack of Frames\n",
    "    \n",
    "    mel_name = 'kss-mel-%05d.npy' % idx\n",
    "    np.save(os.path.join(out_dir + '/mel', mel_name), mel_spec, allow_pickle=False)\n",
    "    \n",
    "    # Wavenet vocoder에서 이용할 wav 파일을 추가로 저장함.\n",
    "    wav_name = 'kss-wav-%05d.npy' % idx\n",
    "    np.save(os.path.join(out_dir + '/wav', wav_name), wav, allow_pickle=False)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Tacotron2에서는 아래 과정이 필요하지 않음.\n",
    "    stft_name = 'kss-spec-%05d.npy' % idx\n",
    "    np.save(os.path.join(out_dir + '/spec', stft_name), stft, allow_pickle=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Tacotron2에서는 아래 과정을 모델 내부에 구현함.\n",
    "    \n",
    "    # Decoder Input\n",
    "    mel_spec = mel_spec.reshape((-1, mel_dim))\n",
    "    # 맨 앞에 <GO> frame이 결합된 mel-spectrogram. 맨 뒤의 한 frame은 제거한다.\n",
    "    dec_input = np.concatenate((np.zeros_like(mel_spec[:1, :]), mel_spec[:-1, :]), axis=0)\n",
    "    dec_input = dec_input[:, -mel_dim:]\n",
    "    dec_name = 'kss-dec-%05d.npy' % idx\n",
    "    np.save(os.path.join(out_dir + '/dec', dec_name), dec_input, allow_pickle=False)\n",
    "    \"\"\"\n",
    "\n",
    "mel_len = sorted(mel_len_list)\n",
    "np.save(os.path.join(out_dir + '/mel_len.npy'), np.array(mel_len))\n",
    "print('Audio Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce956e22",
   "metadata": {},
   "source": [
    "## 1.3. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71cbdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TextMelDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.text_list = sorted(glob.glob(os.path.join(data_dir + '/text', '*.npy')))\n",
    "        self.mel_list = sorted(glob.glob(os.path.join(data_dir + '/mel', '*.npy')))\n",
    "        self.wav_list = sorted(glob.glob(os.path.join(data_dir + '/wav', '*.npy')))\n",
    "\n",
    "        self.text_len = np.load(os.path.join(data_dir + '/text_len.npy'))\n",
    "        self.mel_len = np.load(os.path.join(data_dir + '/mel_len.npy'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.from_numpy(np.load(self.text_list[idx]))\n",
    "        text_len = len(text)\n",
    "        \n",
    "        mel = torch.from_numpy(np.load(self.mel_list[idx]))\n",
    "        mel_len = mel.shape[0]\n",
    "        \n",
    "        wav = torch.from_numpy(np.load(self.wav_list[idx]))\n",
    "        wav_len = wav.shape[0]\n",
    "        return (text, text_len, mel, mel_len, wav, wav_len)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text = []\n",
    "    text_len = []\n",
    "    mel = []\n",
    "    mel_len = []\n",
    "    wav = []\n",
    "    wav_len = []\n",
    "    \n",
    "    for t, tl, m, ml, w, wl in batch:\n",
    "        text.append(t)\n",
    "        text_len.append(tl)\n",
    "        mel.append(m)\n",
    "        mel_len.append(ml)\n",
    "        wav.append(w)\n",
    "        wav_len.append(wl)\n",
    "        \n",
    "    max_text_len = max(text_len)\n",
    "    max_mel_len = max(mel_len)\n",
    "    max_wav_len = max(wav_len)\n",
    "    \n",
    "    # text zero_padding\n",
    "    padded_text_batch = torch.zeros((len(batch), max_text_len), dtype=torch.int32)\n",
    "    for i, x in enumerate(text):\n",
    "        padded_text_batch[i, :len(x)] = torch.Tensor(x)\n",
    "    \n",
    "    # mel zero_padding\n",
    "    padded_mel_batch = torch.zeros((len(batch), max_mel_len, mel_dim), dtype=torch.float32)\n",
    "    for i, x in enumerate(mel):\n",
    "        padded_mel_batch[i, :x.shape[0], :x.shape[1]] = torch.Tensor(x)\n",
    "        \n",
    "    # wav zero_padding\n",
    "    padded_wav_batch = torch.zeros((len(batch), max_wav_len), dtype=torch.float32)\n",
    "    for i, x in enumerate(wav):\n",
    "        padded_wav_batch[i, :x.shape[0]] = torch.Tensor(x)\n",
    "        \n",
    "    return padded_text_batch, text_len, padded_mel_batch, mel_len, padded_wav_batch, wav_len\n",
    "\n",
    "data_dir = './data'\n",
    "dataset = TextMelDataset(data_dir)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658d4401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0](text) shape(B, Max_T): torch.Size([64, 69])\n",
      "batch[1](text_len) len(B): 64\n",
      "batch[2](mel) shape(B, Max_F, mel_dim): torch.Size([64, 383, 80])\n",
      "batch[3](mel_len) len(B): 64\n",
      "batch[4](wav) shape(B, Max_L, mel_dim): torch.Size([64, 97792])\n",
      "batch[5](wav_len) len(B): 64\n",
      "Total: 12854\n",
      "num_of_batches: 201\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "print('batch[0](text) shape(B, Max_T):', batch[0].shape) # Max_T: The number of text length\n",
    "print('batch[1](text_len) len(B):', len(batch[1]))\n",
    "print('batch[2](mel) shape(B, Max_F, mel_dim):', batch[2].shape) # Max_F: The number of Mel-spectrogram frames\n",
    "print('batch[3](mel_len) len(B):', len(batch[3]))\n",
    "print('batch[4](wav) shape(B, Max_L, mel_dim):', batch[4].shape) # Max_L: length of time(seconds) * sampling_rate(22050)\n",
    "print('batch[5](wav_len) len(B):', len(batch[5]))\n",
    "print('Total:', dataset.__len__())\n",
    "print('num_of_batches:', len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b3375",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8651b0c",
   "metadata": {},
   "source": [
    "- Tacotron2 Encoder와 Decoder 구현은 다음 Github를 참고한다.\n",
    "- NVIDIA Github: `https://github.com/NVIDIA/tacotron2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d56438",
   "metadata": {},
   "source": [
    "## 2.1. Tacotron2 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e0b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Character Embedding\n",
    "        self.embedding = nn.Embedding(symbol_length, embedding_dim) # (70, 512) | (B, T) -> (B, T, 512)\n",
    "        \n",
    "        # 3 Conv Layers\n",
    "        convolutions = []\n",
    "        for i in range(3):\n",
    "            conv = nn.Sequential(\n",
    "                nn.Conv1d(embedding_dim, embedding_dim, kernel_size=5, stride=1, padding=3, dilation=1),\n",
    "                nn.BatchNorm1d(embedding_dim), nn.ReLU()) # (B, 512, T) -> (B, 512, T)\n",
    "            convolutions.append(conv)\n",
    "        self.conv = nn.ModuleList(convolutions) # 3 layers of Conv1d-BatchNorm-ReLU\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim // 2, batch_first=True, bidirectional=True) # (B, T, 512) -> (B, T, 256*2)\n",
    "        \n",
    "    def forward(self, text, text_len):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        text: (B, Max_T)\n",
    "        text_len: (B)\n",
    "        =====outputs=====\n",
    "        outputs: (B, Max_T, 512) # memory\n",
    "        \"\"\"\n",
    "        x = self.embedding(text) # (B, T) -> (B, T, 512)\n",
    "        \n",
    "        x = x.transpose(1, 2) # (B, T, 512) -> (B, 512, T)\n",
    "        for conv in self.conv:\n",
    "            x = conv(x) # (B, 512, T) -> (B, 512, T)\n",
    "            \n",
    "        x = x.transpose(1, 2) # (B, 512, T) -> (B, T, 512)\n",
    "        # rnn이 실제 입력값이 있는 부분만을 고려하여 연산을 수행할 수 있도록 함.\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, text_len, batch_first=True,\n",
    "                                              enforce_sorted=False) # padding seq -> packing seq\n",
    "        self.lstm.flatten_parameters() # CUDA ERROR FIX\n",
    "        x, _ = self.lstm(x) # (B, T, 512) -> (B, T, 256*2)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True) # packing seq -> padding seq\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d119fd",
   "metadata": {},
   "source": [
    "## 2.2. Tacotron2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9b610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PreNet, self).__init__()\n",
    "        # 2 Layers Pre-Net\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(80, 256, bias=False),\n",
    "            nn.ReLU()) # (B, 80) -> (B, 256)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(256, 256, bias=False),\n",
    "            nn.ReLU()) # (B, 256) -> (B, 256)\n",
    "    def forward(self, uni_mel):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        uni_mel: (B, 80) # mel로부터 하나씩 추출해낸 것이다.\n",
    "        =====outputs=====\n",
    "        outputs: (B, 256)\n",
    "        \"\"\"\n",
    "        x = F.dropout(self.fc1(uni_mel), p=0.5, training=True) # 항상 dropout\n",
    "        outputs = F.dropout(self.fc2(x), p=0.5, training=True) # 항상 dropout\n",
    "        return outputs\n",
    "    \n",
    "class LocationLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LocationLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(2, 32, kernel_size=31, stride=1, padding=15, dilation=1) # (B, 2, Max_T) -> (B, 32, Max_T)\n",
    "        self.fc = nn.Linear(32, 128, bias=False) # (B, Max_T, 32) -> (B, Max_T, 128)\n",
    "        \n",
    "    def forward(self, attention_weights_cat):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        attention_weights_cat: (B, 2, Max_T) # 이전 time step의 attention weights과 attention_weights_cum의 concat\n",
    "        =====outputs=====\n",
    "        outputs: (B, Max_T, 128)\n",
    "        \"\"\"\n",
    "        x = self.conv(attention_weights_cat)\n",
    "        x = x.transpose(1, 2)\n",
    "        outputs = self.fc(x)\n",
    "        return outputs\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.fc_memory = nn.Linear(embedding_dim, 128, bias=False) # Embedding outputs\n",
    "        self.fc_query = nn.Linear(1024, 128, bias=False) # Query\n",
    "        self.fc_location = LocationLayer() # Previous Attention Weight\n",
    "        \n",
    "        self.v = nn.Linear(128, 1, bias=True)\n",
    "        \n",
    "    def get_attention_weights(self, memory, query, attention_weights_cat, text_len):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        memory: (B, Max_T, 512) # Encoder의 outputs\n",
    "        query: (B, 1024) # Attention LSTM의 outputs\n",
    "        attention_weights_cat: (B, 2, Max_T) # 이전 time step의 attention weights과 attention_weights_cum의 concat\n",
    "        text_len: (B) # text_len이 담긴 list\n",
    "        =====outputs=====\n",
    "        attention_weights: (B, Max_T) # 현재 time step의 attention weight. attention weight가 모여 alignment가 형성된다.\n",
    "        \"\"\"\n",
    "        h = self.fc_memory(memory) # (B, Max_T, 512) -> (B, Max_T, 128)\n",
    "        d = self.fc_query(query.unsqueeze(1)) # (B, 1024) -> (B, 1, 128)\n",
    "        f = self.fc_location(attention_weights_cat) # (B, 2, Max_T) -> (B, Max_T, 128)\n",
    "        \n",
    "        score = self.v(torch.tanh(h + d + f)) # (B, Max_T, 1)\n",
    "        score = score.squeeze(dim=2) # (B, Max_T)\n",
    "        \n",
    "        for idx, length in enumerate(text_len):\n",
    "            score[idx, length:] = -torch.inf\n",
    "        \n",
    "        attention_weights = F.softmax(score, dim=1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def forward(self, memory, query, attention_weights_cat, text_len):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        memory: (B, Max_T, 512) # Encoder의 outputs\n",
    "        query: (B, 1024) # Attention LSTM의 outputs\n",
    "        attention_weights_cat: (B, 2, Max_T) # 이전 time step의 attention weights과 attention_weights_cum의 concat\n",
    "        text_len: (B)\n",
    "        =====outputs=====\n",
    "        context: (B, 512)\n",
    "        attention_weights: (B, Max_T) # 현재 time step의 attention weight\n",
    "        \"\"\"\n",
    "        attention_weights = self.get_attention_weights(memory, query, attention_weights_cat, text_len) # (B, Max_T)\n",
    "        \n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), memory) # bmm: batch matrix-matrix product\n",
    "        # (B, 1, Max_T)@(B, Max_T, 512) = (B, 1, 512)\n",
    "        context = context.squeeze(1) # (B, 512)\n",
    "\n",
    "        return context, attention_weights # (B, 512), (B, Max_T)\n",
    "\n",
    "class PostNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PostNet, self).__init__()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        \n",
    "        self.convolutions.append(nn.Sequential(\n",
    "            nn.Conv1d(80, 512, kernel_size=5, stride=1, padding=2, dilation=1), # (B, 80, Max_F) -> (B, 512, Max_F)\n",
    "            nn.BatchNorm1d(512), nn.Tanh()))\n",
    "        \n",
    "        for i in range(3):\n",
    "            self.convolutions.append(nn.Sequential(\n",
    "                nn.Conv1d(512, 512, kernel_size=5, stride=1, padding=2, dilation=1), # (B, 512, Max_F) -> (B, 512, Max_F)\n",
    "                nn.BatchNorm1d(512), nn.Tanh())) # x3\n",
    "            \n",
    "        self.convolutions.append(nn.Sequential(\n",
    "            nn.Conv1d(512, 80, kernel_size=5, stride=1, padding=2, dilation=1), # (B, 512, Max_F) -> (B, 80, Max_F)\n",
    "            nn.BatchNorm1d(80)))\n",
    "        \n",
    "    def forward(self, mel_outputs):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        mel_outputs: (B, 80, Max_T) # Decoder outputs\n",
    "        =====outputs=====\n",
    "        outputs: (B, 80, Max_T)\n",
    "        \"\"\"\n",
    "        x = mel_outputs\n",
    "        for conv in self.convolutions:\n",
    "            x = F.dropout(conv(x), p=0.5, training=self.training)\n",
    "        outputs = x\n",
    "        return outputs\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        ##### Layers #####\n",
    "        # Pre-Net\n",
    "        self.prenet = PreNet()\n",
    "        # Attention LSTM Cell\n",
    "        self.attention_lstm = nn.LSTMCell(256 + 512, 1024)\n",
    "        # Attention\n",
    "        self.attention = Attention()\n",
    "        # Decoder LSTM Cell\n",
    "        self.decoder_lstm = nn.LSTMCell(1024 + 512, 1024)\n",
    "        # Linear Projection\n",
    "        self.linear_projection = nn.Linear(1024 + 512, 80)\n",
    "        # Gate Linear Projection\n",
    "        self.gate = nn.Sequential(nn.Linear(1024 + 512, 1), nn.Sigmoid())\n",
    "        \n",
    "        # Post-Net\n",
    "        self.postnet = PostNet()\n",
    "        \n",
    "    def init_weights(self, memory):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        memory: (B, Max_T, 512)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        B = memory.size(0)\n",
    "        Max_T = memory.size(1)\n",
    "        ##### Weight #####\n",
    "        self.attention_hidden = torch.zeros((B, 1024), dtype=torch.float32).to(device)\n",
    "        self.attention_cell = torch.zeros((B, 1024), dtype=torch.float32).to(device)\n",
    "        \n",
    "        self.decoder_hidden = torch.zeros((B, 1024), dtype=torch.float32).to(device)\n",
    "        self.decoder_cell = torch.zeros((B, 1024), dtype=torch.float32).to(device)\n",
    "        \n",
    "        self.attention_weights = torch.zeros((B, Max_T), dtype=torch.float32).to(device)\n",
    "        self.attention_weights_cum = torch.zeros((B, Max_T), dtype=torch.float32).to(device)\n",
    "        self.context = torch.zeros((B, 512), dtype=torch.float32).to(device)\n",
    "        \n",
    "    def forward(self, mel, memory, text_len):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        mel: (B, Max_F, mel_dim=80)\n",
    "        memory: (B, Max_T, 512)\n",
    "        text_len: (B)\n",
    "        =====outputs=====\n",
    "        mel_outputs: (B, 80, Max_F) # 중간 mel_spec\n",
    "        postnet_mel_outputs: (B, 80, Max_F) # 최종 mel_spec | 두 mel_sepc의 loss의 합이 손실함수에 포함된다.\n",
    "        gate_outputs: (B, Max_F)\n",
    "        alignments: (B, Max_T, Max_F)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        self.init_weights(memory) # 초기 가중치 초기화\n",
    "        \n",
    "        mel_outputs = []\n",
    "        gate_outputs = []\n",
    "        alignments = []\n",
    "        \n",
    "        B = mel.size(0)\n",
    "        mel_dim = mel.size(2)\n",
    "        GO = torch.zeros((B, 1, mel_dim), dtype=torch.float32).to(device) # GO frame\n",
    "        mel_inputs = torch.cat((GO, mel), dim=1) # (B, 1 + Max_F, mel_dim)\n",
    "        \n",
    "        for idx in range(mel_inputs.size(1) - 1): # Max_F번 반복\n",
    "            mel_input = mel_inputs[:, idx, :] # (B, mel_dim=80)\n",
    "            x = self.prenet(mel_input) # (B, 80) -> (B, 256)\n",
    "\n",
    "            # Attention LSTM Cell\n",
    "            x = torch.cat((x, self.context), dim=1) # (B, 256 + 512)\n",
    "            self.attention_hidden, self.attention_cell = self.attention_lstm(x, (self.attention_hidden, self.attention_cell))\n",
    "            # (B, 256 + 512) -> (B, 1024)\n",
    "            self.attention_hidden = F.dropout(self.attention_hidden, p=0.1, training=self.training)\n",
    "            query = self.attention_hidden\n",
    "            \n",
    "            # Attention\n",
    "            attention_weights_cat = torch.cat((self.attention_weights.unsqueeze(1),\n",
    "                                              self.attention_weights_cum.unsqueeze(1)), dim=1) # (B, 2, Max_T)\n",
    "            self.context, self.attention_weights = self.attention(memory, query, attention_weights_cat, text_len)\n",
    "            # (B, 512), (B, Max_T)\n",
    "            self.attention_weights_cum += self.attention_weights # Attention weights 누적\n",
    "            \n",
    "            # Decoder LSTM Cell\n",
    "            x = torch.cat((query, self.context), dim=1) # (B, 1024 + 512)\n",
    "            self.decoder_hidden, self_decoder_cell = self.decoder_lstm(x, (self.decoder_hidden, self.decoder_cell))\n",
    "            # (B, 1024 + 512) -> (B, 1024)\n",
    "            self.decoder_hidden = F.dropout(self.decoder_hidden, p=0.1, training=self.training)\n",
    "            x = self.decoder_hidden\n",
    "            \n",
    "            x = torch.cat((x, self.context), dim=1) # (B, 1024 + 512)\n",
    "            mel_output = self.linear_projection(x) # (B, 1024 + 512) -> (B, 80)\n",
    "            gate_output = self.gate(x) # (B, 1024 + 512) -> (B, 1)\n",
    "            \n",
    "            mel_outputs.append(mel_output) # final: (B, 80) * Max_F\n",
    "            gate_outputs.append(gate_output) # final: (B, 1) * Max_F\n",
    "            alignments.append(self.attention_weights) # final: (B, Max_T) * Max_F\n",
    "            \n",
    "        mel_outputs = torch.stack(mel_outputs, dim=2) # (B, 80, Max_F)\n",
    "        gate_outputs = torch.stack(gate_outputs, dim=1) # (B, Max_F, 1)\n",
    "        gate_outputs = gate_outputs.squeeze(2) # (B, Max_F)\n",
    "        alignments = torch.stack(alignments, dim=2) # (B, Max_T, Max_F)\n",
    "        \n",
    "        # Post-Net\n",
    "        postnet_outputs = self.postnet(mel_outputs) # (B, 80, Max_F) -> (B, 80, Max_F)\n",
    "        postnet_mel_outputs = mel_outputs + postnet_outputs # (B, 80, Max_F)\n",
    "        \n",
    "        return mel_outputs, postnet_mel_outputs, gate_outputs, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83824fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tacotron2, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, text, text_len, mel):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        text: (B, Max_T)\n",
    "        text_len: (B)\n",
    "        mel: (B, 80, Max_F)\n",
    "        =====outputs=====\n",
    "        mel_outputs: (B, 80, Max_F)\n",
    "        postnet_mel_outputs: (B, 80, Max_F)\n",
    "        gate_outputs: (B, Max_F)\n",
    "        alignments: (B, Max_T, Max_F)\n",
    "        \"\"\"\n",
    "        memory = self.encoder(text, text_len)\n",
    "        mel_outputs, postnet_mel_outputs, gate_outputs, alignments = self.decoder(mel, memory, text_len)\n",
    "        return mel_outputs, postnet_mel_outputs, gate_outputs, alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c64b15",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "016ab75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Attention Test #####\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_memory = nn.Linear(embedding_dim, 128, bias=False) # Embedding outputs\n",
    "        self.fc_query = nn.Linear(1024, 128, bias=False) # Query\n",
    "        \n",
    "    def forward(self, memory, query):\n",
    "        q = self.fc_query(query.unsqueeze(1)).unsqueeze(2) # (B, 1024) -> (B, 1, 1, 128)\n",
    "        v = self.fc_memory(memory).unsqueeze(1) # (B, Max_T, 512) -> (B, 1, Max_T, 128)\n",
    "        \n",
    "        score = torch.sum(torch.tanh(q + v), dim=-1) # (B, 1, Max_T)\n",
    "\n",
    "        attention_weights = F.softmax(score, dim=-1) # (B, 1, Max_T)\n",
    "        \n",
    "        context = torch.matmul(attention_weights, memory) # bmm: batch matrix-matrix product\n",
    "        # (B, 1, Max_T)@(B, Max_T, 512) = (B, 1, 512)\n",
    "        context = context.squeeze(1) # (B, 512)\n",
    "        attention_weights = attention_weights.squeeze(1)#  (B, Max_T)\n",
    "        \n",
    "        return context, attention_weights\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        ##### Layers #####\n",
    "        # Pre-Net\n",
    "        self.prenet = PreNet()\n",
    "        # Attention LSTM Cell\n",
    "        self.attention_lstm = nn.LSTMCell(256 + 512, 1024)\n",
    "        # Attention\n",
    "        self.attention = BahdanauAttention()\n",
    "        # Decoder LSTM Cell\n",
    "        self.decoder_lstm = nn.LSTMCell(1024 + 512, 1024)\n",
    "        # Linear Projection\n",
    "        self.linear_projection = nn.Linear(1024 + 512, 80)\n",
    "        # Gate Linear Projection\n",
    "        self.gate = nn.Sequential(nn.Linear(1024 + 512, 1), nn.Sigmoid())\n",
    "        \n",
    "        # Post-Net\n",
    "        self.postnet = PostNet()\n",
    "        \n",
    "    def init_weights(self, memory):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        memory: (B, Max_T, 512)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        B = memory.size(0)\n",
    "        Max_T = memory.size(1)\n",
    "        ##### Weight #####\n",
    "        self.attention_hidden = torch.zeros((B, 1024), dtype=torch.float32).to(device)\n",
    "        self.attention_cell = torch.zeros((B, 1024), dtype=torch.float32).to(device)\n",
    "        \n",
    "        self.decoder_hidden = torch.zeros((B, 1024), dtype=torch.float32).to(device)\n",
    "        self.decoder_cell = torch.zeros((B, 1024), dtype=torch.float32).to(device)\n",
    "        \n",
    "        self.attention_weights = torch.zeros((B, Max_T), dtype=torch.float32).to(device)\n",
    "        self.context = torch.zeros((B, 512), dtype=torch.float32).to(device)\n",
    "        \n",
    "    def forward(self, mel, memory):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        mel: (B, Max_F, mel_dim=80)\n",
    "        memory: (B, Max_T, 512)\n",
    "        =====outputs=====\n",
    "        mel_outputs: (B, 80, Max_F) # 중간 mel_spec\n",
    "        postnet_mel_outputs: (B, 80, Max_F) # 최종 mel_spec | 두 mel_sepc의 loss의 합이 손실함수에 포함된다.\n",
    "        gate_outputs: (B, Max_F)\n",
    "        alignments: (B, Max_T, Max_F)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        self.init_weights(memory) # 초기 가중치 초기화\n",
    "        \n",
    "        mel_outputs = []\n",
    "        gate_outputs = []\n",
    "        alignments = []\n",
    "        \n",
    "        B = mel.size(0)\n",
    "        mel_dim = mel.size(2)\n",
    "        GO = torch.zeros((B, 1, mel_dim), dtype=torch.float32).to(device) # GO frame\n",
    "        mel_inputs = torch.cat((GO, mel), dim=1) # (B, 1 + Max_F, mel_dim)\n",
    "        \n",
    "        for idx in range(mel_inputs.size(1) - 1): # Max_F번 반복\n",
    "            mel_input = mel_inputs[:, idx, :] # (B, mel_dim=80)\n",
    "            x = self.prenet(mel_input) # (B, 80) -> (B, 256)\n",
    "\n",
    "            # Attention LSTM Cell\n",
    "            x = torch.cat((x, self.context), dim=1) # (B, 256 + 512)\n",
    "            self.attention_hidden, self.attention_cell = self.attention_lstm(x, (self.attention_hidden, self.attention_cell))\n",
    "            # (B, 256 + 512) -> (B, 1024)\n",
    "            self.attention_hidden = F.dropout(self.attention_hidden, p=0.1, training=self.training)\n",
    "            query = self.attention_hidden\n",
    "            \n",
    "            # Attention\n",
    "            self.context, self.attention_weights = self.attention(memory, query)\n",
    "            # (B, 512), (B, Max_T)\n",
    "            \n",
    "            x = torch.cat((query, self.context), dim=1) # (B, 1024 + 512)\n",
    "            \n",
    "            # Decoder LSTM Cell\n",
    "            self.decoder_hidden, self_decoder_cell = self.decoder_lstm(x, (self.decoder_hidden, self.decoder_cell))\n",
    "            # (B, 1024 + 512) -> (B, 1024)\n",
    "            self.decoder_hidden = F.dropout(self.decoder_hidden, p=0.1, training=self.training)\n",
    "            x = self.decoder_hidden\n",
    "            \n",
    "            x = torch.cat((x, self.context), dim=1) # (B, 1024 + 512)\n",
    "            mel_output = self.linear_projection(x) # (B, 1024 + 512) -> (B, 80)\n",
    "            gate_output = self.gate(x) # (B, 1024 + 512) -> (B, 1)\n",
    "            \n",
    "            mel_outputs.append(mel_output) # final: (B, 80) * Max_F\n",
    "            gate_outputs.append(gate_output) # final: (B, 1) * Max_F\n",
    "            alignments.append(self.attention_weights) # final: (B, Max_T) * Max_F\n",
    "            \n",
    "        mel_outputs = torch.stack(mel_outputs, dim=2) # (B, 80, Max_F)\n",
    "        gate_outputs = torch.stack(gate_outputs, dim=1) # (B, Max_F, 1)\n",
    "        gate_outputs = gate_outputs.squeeze(2) # (B, Max_F)\n",
    "        alignments = torch.stack(alignments, dim=2) # (B, Max_T, Max_F)\n",
    "        \n",
    "        # Post-Net\n",
    "        postnet_outputs = self.postnet(mel_outputs) # (B, 80, Max_F) -> (B, 80, Max_F)\n",
    "        postnet_mel_outputs = mel_outputs + postnet_outputs # (B, 80, Max_F)\n",
    "        \n",
    "        return mel_outputs, postnet_mel_outputs, gate_outputs, alignments\n",
    "    \n",
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tacotron2, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, text, text_len, mel):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        text: (B, Max_T)\n",
    "        text_len: (B)\n",
    "        mel: (B, 80, Max_F)\n",
    "        =====outputs=====\n",
    "        mel_outputs: (B, 80, Max_F)\n",
    "        postnet_mel_outputs: (B, 80, Max_F)\n",
    "        gate_outputs: (B, Max_F)\n",
    "        alignments: (B, Max_T, Max_F)\n",
    "        \"\"\"\n",
    "        memory = self.encoder(text, text_len)\n",
    "        mel_outputs, postnet_mel_outputs, gate_outputs, alignments = self.decoder(mel, memory)\n",
    "        return mel_outputs, postnet_mel_outputs, gate_outputs, alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a9c73",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0adf998e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 80, 659]) torch.Size([64, 80, 659]) torch.Size([64, 659]) torch.Size([64, 93, 659])\n",
      "cpu:  30.782493591308594\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "device = 'cpu'\n",
    "\n",
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Tacotron에 대입\n",
    "a, b, c, d = Tacotron2()(batch[0], batch[1], batch[2])\n",
    "print(a.shape, b.shape, c.shape, d.shape)\n",
    "\n",
    "end = time.time()\n",
    "print('cpu: ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7b2dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 85]) 64 torch.Size([64, 479, 80])\n",
      "torch.Size([64, 80, 479]) torch.Size([64, 80, 479]) torch.Size([64, 479]) torch.Size([64, 85, 479])\n",
      "cuda:  1.3982977867126465\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Tacotron2()\n",
    "model = model.to(device)\n",
    "\n",
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Tacotron에 대입\n",
    "a, b, c, d = model(batch[0].to(device), batch[1], batch[2].to(device))\n",
    "print(batch[0].size(), len(batch[1]), batch[2].size())\n",
    "print(a.shape, b.shape, c.shape, d.shape)\n",
    "\n",
    "end = time.time()\n",
    "print('cuda: ', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d45d4",
   "metadata": {},
   "source": [
    "## 2.3. Wavenet Vocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3148a491",
   "metadata": {},
   "source": [
    "- Wavenet Vocoder 구현은 다음 Github를 참고한다.\n",
    "- hccho2 Github: `https://github.com/hccho2/Tacotron2-Wavenet-Korean-TTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae43e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, dilation):\n",
    "        \"\"\"\n",
    "        in_channel: (causal) 1 (dilated) 128\n",
    "        out_channel: (causal) 128 (dilated) 512\n",
    "        dilation: (causal) 1 (dilated) 1, 2, 4, 8, 16, 32, 64, 128, 256, 512 중 하나\n",
    "        \"\"\"\n",
    "        super(DilatedConv, self).__init__()\n",
    "        self.padding = (3 - 1)*dilation\n",
    "        self.dilated_conv = nn.Conv1d(in_channel, out_channel, kernel_size=3, stride=1, padding=0, dilation=dilation)\n",
    "        # (B, in_channel, ?) -> (B, out_channel, ?) # ?: (training) Max_L-1, (inference) 1\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        * ?: (training) Max_L-1, (inference) 1 (* 이후에도 동일함)\n",
    "        inputs: (causal) (B, 1, ?) (dilation) (B, 128, ?)\n",
    "        =====outputs=====\n",
    "        output: (causal) (B, 128, ?) (dilation) (B, 512, ?)\n",
    "        \"\"\"\n",
    "        B, C, L = inputs.size()\n",
    "        zero_pad = torch.zeros(B, C, self.padding).to(device)\n",
    "        padded_inputs = torch.cat([zero_pad, inputs], dim=-1) # receptive field에 맞도록 padding\n",
    "        \n",
    "        outputs = self.dilated_conv(padded_inputs) # (B, in_channel, ?+padding) -> (B, out_channel, ?)\n",
    "        return outputs # (B, out_channel, ?)\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dilation):\n",
    "        \"\"\"\n",
    "        dilation: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512 중 하나\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.dilated_conv = DilatedConv(128, 512, dilation) # (B, 128, ?) -> (B, 512, ?)\n",
    "        \"\"\"\n",
    "        메모리 용량을 줄이기 위해 삭제\n",
    "        self.filter_conv = nn.Conv1d(256, 256, kernel_size=1, stride=1, padding=0, dilation=1) # (B, 256, ?) -> (B, 256, ?)\n",
    "        self.gate_conv = nn.Conv1d(256, 256, kernel_size=1, stride=1, padding=0, dilation=1) # (B, 256, ?) -> (B, 256, ?)\n",
    "        \"\"\"\n",
    "        self.local_filter_conv = nn.Conv1d(80, 256, kernel_size=1, stride=1, padding=0, dilation=1) # (B, 80, ?) -> (B, 256, ?)\n",
    "        self.local_gate_conv = nn.Conv1d(80, 256, kernel_size=1, stride=1, padding=0, dilation=1) # (B, 80, ?) -> (B, 256, ?)\n",
    "        self.global_filter_conv = nn.Conv1d(32, 256, kernel_size=1, stride=1, padding=0, dilation=1) # (B, 32, ?) -> (B, 256, ?)\n",
    "        self.global_gate_conv = nn.Conv1d(32, 256, kernel_size=1, stride=1, padding=0, dilation=1) # (B, 32, ?) -> (B, 256, ?)\n",
    "        self.residual_conv = nn.Conv1d(256, 128, kernel_size=1, stride=1, padding=0, dilation=1) # (B, 256, ?) -> (B, 128, ?)\n",
    "        self.skip_conv = nn.Conv1d(256, 128, kernel_size=1, stride=1, padding=0, dilation=1) # (B, 256, ?) -> (B, 128, ?)\n",
    "        \n",
    "    def forward(self, inputs, local_condition, global_condition):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        inputs: (B, 128, ?)\n",
    "        local_condition: (B, 80, ?) # upscaled 된 tacotron2의 mel_outputs\n",
    "        global_condition: (B, 32, 1) # embedded 된 speaker_id\n",
    "        =====outputs=====\n",
    "        residual_outputs: (B, 128, ?)\n",
    "        skip_outputs: (B, 128, ?)\n",
    "        \"\"\"\n",
    "        x = self.dilated_conv(inputs) # (B, 128, ?) -> (B, 512, ?)\n",
    "        x_filter, x_gate = torch.chunk(x, chunks=2, dim=1) # x를 dim=1 기준으로 둘로 나눔: (B, 256, ?), (B, 256, ?)\n",
    "        \"\"\"\n",
    "        메모리 용량을 줄이기 위해 삭제\n",
    "        x_filter = self.filter_conv(x_filter) # (B, 256, ?)\n",
    "        x_gate = self.gate_conv(x_gate) # (B, 256, ?)\n",
    "        \"\"\"\n",
    "        \n",
    "        if local_condition is not None:\n",
    "            x_filter = x_filter + self.local_filter_conv(local_condition) # (B, 256, ?)\n",
    "            x_gate = x_gate + self.local_gate_conv(local_condition) # (B, 256, ?)\n",
    "        if global_condition is not None:\n",
    "            x_filter = x_filter + self.global_filter_conv(global_condition) # (B, 256, ?)\n",
    "            x_gate = x_gate + self.global_gate_conv(global_condition) # (B, 256, ?)\n",
    "        x = torch.tanh(x_filter) * torch.sigmoid(x_gate) # (B, 256, ?)\n",
    "        \n",
    "        residual_outputs = self.residual_conv(x) # (B, 256, ?) -> (B, 128, ?)\n",
    "        residual_outputs = inputs + residual_outputs # (B, 128, ?)\n",
    "        skip_outputs = self.skip_conv(x) # (B, 256, ?) -> (B, 128, ?)\n",
    "        \n",
    "        return residual_outputs, skip_outputs\n",
    "    \n",
    "class StackOfResidualBlocks(nn.Module):\n",
    "    def __init__(self, dilations):\n",
    "        \"\"\"\n",
    "        dilations = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]*3\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.stack_of_residual_blocks = nn.ModuleList()\n",
    "        for dilation in dilations:\n",
    "            residual_block = ResidualBlock(dilation)\n",
    "            self.stack_of_residual_blocks.append(residual_block)        \n",
    "        \n",
    "    def forward(self, inputs, local_condition, global_condition):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        inputs: (B, 128, ?)\n",
    "        local_condition: (B, 80, ?) # upscaled 된 tacotron2의 mel_outputs\n",
    "        global_condition: (B, 32, 1) # embedded 된 speaker_id\n",
    "        =====outputs=====\n",
    "        sum_of_skip_outputs: (B, 128, ?) # skip_outputs을 skip-connection한 결과\n",
    "        \"\"\"\n",
    "        residual_outputs = inputs\n",
    "        stack_of_skip_outputs = []\n",
    "        for residual_block in self.stack_of_residual_blocks:\n",
    "            residual_outputs, skip_outputs = residual_block(residual_outputs, local_condition, global_condition)\n",
    "            # (B, 128, ?), (B, 128, ?)\n",
    "            stack_of_skip_outputs.append(skip_outputs)\n",
    "            \n",
    "        sum_of_skip_outputs = torch.zeros_like(stack_of_skip_outputs[0])\n",
    "        for skip_outputs in stack_of_skip_outputs:\n",
    "            sum_of_skip_outputs += skip_outputs # (B, 128, ?)\n",
    "        \n",
    "        return sum_of_skip_outputs\n",
    "    \n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, upsampling_factors, dilations, num_of_speakers):\n",
    "        \"\"\"\n",
    "        upsampling_factors = [16, 16]\n",
    "        dilations = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]*3\n",
    "        num_of_speakers: int # 화자의 수, global_condition의 원소의 종류\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Local condition Upsampling\n",
    "        self.upsampling_convs = nn.ModuleList()\n",
    "        for upscale in upsampling_factors:\n",
    "            upsampling_conv = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(1, upscale), stride=(1, upscale),\n",
    "                                                 padding=0, output_padding=0, bias=False, dilation=1)\n",
    "            # (B, 1, 80, F) -> (B, 1, 80, upscale*F)\n",
    "            self.upsampling_convs.append(upsampling_conv)\n",
    "        # Global condition Embedding\n",
    "        self.global_embedding = nn.Embedding(num_of_speakers, 32) # (B) -> (B, 32)\n",
    "        \n",
    "        # Casual Conv\n",
    "        self.casual_conv = DilatedConv(in_channel=1, out_channel=128, dilation=1)\n",
    "        # (B, 1, ?) -> (B, 128, ?)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.residual_blocks = StackOfResidualBlocks(dilations)\n",
    "        \n",
    "        # Post Layers\n",
    "        self.post_layers = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=1, stride=1, padding=0, dilation=1), # (B, 128, ?) -> (B, 128, ?)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 30, kernel_size=1, stride=1, padding=0, dilation=1) # (B, 128, ?) -> (B, 30, ?)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_waveform, local_condition, global_condition):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        * ?: (train) L-1, (inference) 1\n",
    "        input_waveform: (B, ?)\n",
    "        local_condition: (B, 80, Max_F) # Tacotron2의 mel_outputs\n",
    "        global_condition: (B) # Speaker의 ID\n",
    "        =====outputs=====\n",
    "        outputs: (B, 30, Max_L) # MoL의 모수(가중치)에 대한 features\n",
    "        \"\"\"\n",
    "        L = input_waveform.size(dim=1)\n",
    "        # Preprocessing of input_waveform\n",
    "        if self.training == True:\n",
    "            input_waveform = input_waveform[:, :-1] # 마지막은 제외 | (B, L) -> (B, L-1)\n",
    "        # input_waveform: (train) (B, L-1), (inference) (B, 1)\n",
    "        # 아래부터는 ?: L-1 or 1\n",
    "        input_waveform = input_waveform.unsqueeze(dim=1) # (B, ?) -> (B, 1, ?)\n",
    "        \n",
    "        # Preprocessing of local_condition\n",
    "        local_condition = local_condition.unsqueeze(dim=1) # (B, 80, F) -> (B, 1, 80, F)\n",
    "        for upsampling_conv in self.upsampling_convs:\n",
    "            local_condition = upsampling_conv(local_condition)\n",
    "            # (B, 1, 80, F) -> (B, 1, 80, 16*F) -> (B, 1, 80, 256*F)\n",
    "        local_condition = local_condition[:, 0, :, :L-1] # (B, 80, L-1)\n",
    "        \n",
    "        # Preprocessing of global_condition\n",
    "        global_condition = self.global_embedding(global_condition) # (B) -> (B, 32)\n",
    "        global_condition = global_condition.unsqueeze(dim=-1) # (B, 32, 1)\n",
    "        \n",
    "        # WaveNet\n",
    "        x = self.casual_conv(input_waveform) # (B, 1, ?)\n",
    "        x = self.residual_blocks(x, local_condition, global_condition) # (B, 128, ?)\n",
    "        outputs = self.post_layers(x) # (B, 128, ?) -> (B, 30, ?)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40451216",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce698fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7985889280 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24432\\981918191.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_waveform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24432\\808338414.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_waveform, local_condition, global_condition)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# WaveNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasual_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_waveform\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B, 1, ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresidual_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_condition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_condition\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B, 128, ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B, 128, ?) -> (B, 30, ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24432\\808338414.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, local_condition, global_condition)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mstack_of_skip_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mresidual_block\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack_of_residual_blocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mresidual_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresidual_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresidual_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_condition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_condition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[1;31m# (B, 128, ?), (B, 128, ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mstack_of_skip_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskip_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24432\\808338414.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, local_condition, global_condition)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mx_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_filter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_filter_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_condition\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B, 256, ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mx_gate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_gate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_gate_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_condition\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B, 256, ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_filter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_gate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B, 256, ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mresidual_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresidual_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B, 256, ?) -> (B, 128, ?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7985889280 bytes."
     ]
    }
   ],
   "source": [
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "input_waveform = batch[4]\n",
    "local_condition = batch[2].transpose(1, 2)\n",
    "global_condition = torch.randint(low=0, high=2, size=(batch_size,))\n",
    "num_of_speakers = 2\n",
    "\n",
    "net = WaveNet(upsampling_factors, dilations, num_of_speakers)\n",
    "net = net.to(device)\n",
    "\n",
    "output = net(input_waveform.to(device), local_condition.to(device), global_condition.to(device))\n",
    "print(output.size())\n",
    "\n",
    "end = time.time()\n",
    "print(device, ':', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5634f79",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39a26c",
   "metadata": {},
   "source": [
    "## 3.0. Plot Alignment\n",
    "- Plot Alignment는 Tacotron 1에서의 구현을 그대로 가져온다.\n",
    "- 이후 main함수 구현도 Tacotron1을 참고하겠다.\n",
    "- chldkato Github: `https://github.com/chldkato/Tacotron-pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d32f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib으로 그린 그래프를 파일로 저장할 때,\n",
    "# 'Agg' 백엔드를 사용하여 비트맵 그래픽스로 렌더링하도록 설정한다.\n",
    "# 즉, 수학적으로 표현하지 않고 이미지를 픽셀 단위로 분할하여 저장한다.\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Matplotlib에서 한글 폰트를 설정\n",
    "font_name = fm.FontProperties(fname=\"malgun.ttf\").get_name()\n",
    "matplotlib.rc('font', family=font_name, size=14)\n",
    "\n",
    "def plot_alignment(alignment, path, text, step, loss):\n",
    "    text = text.rstrip('_').rstrip('~')\n",
    "    alignment = alignment[:len(text)]\n",
    "    \n",
    "    # 하나의 그림(fig) 객체와 하나의 축(ax) 객체를 생성\n",
    "    _, ax = plt.subplots(figsize=(len(text)/3, 5))\n",
    "    # 생성한 축(ax) 객체에 이미지를 출력\n",
    "    im = ax.imshow(np.transpose(alignment), aspect='auto', origin='lower')\n",
    "    \n",
    "    plt.xlabel('Encoder timestep')\n",
    "    plt.ylabel('Decoder timestep')\n",
    "    # 공백 문자 ' '를 빈 문자열 ''로 변환\n",
    "    text = [x if x != ' ' else '' for x in list(text)]\n",
    "    # x축의 눈금과 레이블을 설정\n",
    "    plt.xticks(range(len(text)), text)\n",
    "    \n",
    "    plt.title(f\"step: {step}, loss: {loss:.5f}\", loc=\"center\", pad=10)\n",
    "    \n",
    "    # 그래프의 레이아웃을 조정\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(im, ax=ax) # colorbar\n",
    "    plt.savefig(path, format='png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674e0c3",
   "metadata": {},
   "source": [
    "## 3.1. Tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f42552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, mel_outputs, postnet_mel_outputs, gate_outputs,\n",
    "                mel_targets, gate_targets):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        mel_outputs: (B, 80, Max_F)\n",
    "        postnet_mel_outputs: (B, 80, Max_F)\n",
    "        gate_outputs: (B, Max_F)\n",
    "        mel_targets: (B, 80, Max_F)\n",
    "        gate_targets: (B, Max_F)\n",
    "        =====outputs=====\n",
    "        \n",
    "        \"\"\"\n",
    "        mel_targets.requires_grad = False\n",
    "        gate_targets.requires_grad = False\n",
    "        \n",
    "        mel_loss =  nn.MSELoss()(mel_outputs, mel_targets)\n",
    "        postnet_mel_loss = nn.MSELoss()(postnet_mel_outputs, mel_targets)\n",
    "        gate_loss = nn.BCEWithLogitsLoss()(gate_outputs, gate_targets)\n",
    "        return mel_loss + postnet_mel_loss + gate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51408933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, check_step):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    train_loader = dataloader # 1.3에서 정의함.\n",
    "    model = Tacotron2()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_of_batches = len(dataloader)\n",
    "    \n",
    "    os.makedirs('ckpt/' + model_name + \"/1\", exist_ok=True)\n",
    "    \n",
    "    epoch, step = 1, 0\n",
    "    if check_step is not None:\n",
    "        check_point = \"./ckpt/\" + model_name + \"/1/ckpt-\" + str(check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        step = ckpt['step']\n",
    "        epoch = skpt['epoch']\n",
    "        print(f'Load Status: Epoch {epoch}, Step {step}')\n",
    "\n",
    "    # PyTorch에서 CUDA 연산을 더 빠르게 수행하기 위한 기능 중 하나\n",
    "    torch.backends.cudnn.benchmark = True    \n",
    "    \n",
    "    start = time.time()\n",
    "    while True:\n",
    "        for i in range(num_of_batches):\n",
    "            text, text_len, mel, mel_len, wav, _ = next(iter(train_loader))\n",
    "            text = text.to(device)\n",
    "            mel = mel.to(device) # (B, Max_F, 80)\n",
    "            wav = wav.to(device)\n",
    "\n",
    "            # gate_targets 생성\n",
    "            B, Max_F, _ = mel.size()\n",
    "            gate_targets = torch.ones((B, Max_F), dtype=torch.float32).to(device)\n",
    "            for idx, length in enumerate(mel_len):\n",
    "                gate_targets[idx, :length] = 0\n",
    "                \n",
    "            # mel_targets 생성\n",
    "            mel_targets = mel.transpose(1, 2) # (B, 80, Max_F)\n",
    "\n",
    "            mel_outputs, postnet_mel_outputs, gate_outputs, alignments = model(text, text_len, mel)\n",
    "            loss = Tacotron2Loss()(mel_outputs, postnet_mel_outputs, gate_outputs,\n",
    "                                   mel_targets, gate_targets)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 기울기를 1로 cliping \n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            step += 1\n",
    "            if step % 10 == 0:\n",
    "                print(f'| epoch: {epoch} | step: {step} | loss: {loss:.5f} | grad_norm: {grad_norm:.5f} | {time.time()-start:.3f} sec / 10 steps')\n",
    "                start = time.time() # start time 초기화\n",
    "\n",
    "            if step % checkpoint_step == 0:\n",
    "                save_dir = './ckpt/' + model_name + '/1'\n",
    "                input_seq = sequence_to_text(text[0].cpu().numpy())\n",
    "                input_seq = input_seq[:text_len[0]]\n",
    "                alignment_dir = os.path.join(save_dir, f'step-{step}-align.png')\n",
    "                plot_alignment(alignments[0].detach().cpu().numpy(), alignment_dir, input_seq, step, loss)\n",
    "                torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'step': step,\n",
    "                    'epoch': epoch\n",
    "                }, os.path.join(save_dir, 'ckpt-{}.pt'.format(step)))\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc8015",
   "metadata": {},
   "source": [
    "- model_01: Original\n",
    "- model_02: Original + grad_cliping\n",
    "- model_03: grad_cliping + Bahdanau Attention\n",
    "- model_04: grad_cliping + Bahdanau Attention + LSTM 사이에 concat 추가\n",
    "- model_05: grad_cliping + Original Bahdanau Attention + LSTM 사이에 concat 추가\n",
    "- model_06: Original + grad_cliping + LSTM 사이에 concat 추가\n",
    "- model_07: Original + grad_cliping + LSTM 사이에 concat 추가 + mask\n",
    "- model_08: model_07 + attention_weight_cum 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d92b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT #####\n",
    "model_name = 'model_08'\n",
    "check_step = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2bcefee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 1 | step: 10 | loss: 2.37144 | grad_norm: 0.46209 | 38.918 sec / 10 steps\n",
      "| epoch: 1 | step: 20 | loss: 2.05989 | grad_norm: 0.39781 | 36.900 sec / 10 steps\n",
      "| epoch: 1 | step: 30 | loss: 1.99175 | grad_norm: 0.34701 | 36.949 sec / 10 steps\n",
      "| epoch: 1 | step: 40 | loss: 1.94190 | grad_norm: 0.45892 | 33.225 sec / 10 steps\n",
      "| epoch: 1 | step: 50 | loss: 1.90032 | grad_norm: 0.32706 | 34.105 sec / 10 steps\n",
      "| epoch: 1 | step: 60 | loss: 1.87740 | grad_norm: 0.33003 | 36.536 sec / 10 steps\n",
      "| epoch: 1 | step: 70 | loss: 1.83515 | grad_norm: 0.38787 | 33.756 sec / 10 steps\n",
      "| epoch: 1 | step: 80 | loss: 1.82902 | grad_norm: 0.40811 | 33.813 sec / 10 steps\n",
      "| epoch: 1 | step: 90 | loss: 1.76325 | grad_norm: 0.34648 | 37.398 sec / 10 steps\n",
      "| epoch: 1 | step: 100 | loss: 1.77798 | grad_norm: 0.31312 | 29.694 sec / 10 steps\n",
      "| epoch: 1 | step: 110 | loss: 1.76574 | grad_norm: 0.32165 | 36.047 sec / 10 steps\n",
      "| epoch: 1 | step: 120 | loss: 1.72661 | grad_norm: 0.30567 | 31.410 sec / 10 steps\n",
      "| epoch: 1 | step: 130 | loss: 1.67607 | grad_norm: 0.30673 | 49.527 sec / 10 steps\n",
      "| epoch: 1 | step: 140 | loss: 1.70769 | grad_norm: 0.31891 | 44.670 sec / 10 steps\n",
      "| epoch: 1 | step: 150 | loss: 1.61693 | grad_norm: 0.34585 | 31.629 sec / 10 steps\n",
      "| epoch: 1 | step: 160 | loss: 1.63491 | grad_norm: 0.29496 | 31.029 sec / 10 steps\n",
      "| epoch: 1 | step: 170 | loss: 1.59942 | grad_norm: 0.28631 | 34.843 sec / 10 steps\n",
      "| epoch: 1 | step: 180 | loss: 1.57427 | grad_norm: 0.28250 | 32.280 sec / 10 steps\n",
      "| epoch: 1 | step: 190 | loss: 1.57243 | grad_norm: 0.28066 | 35.577 sec / 10 steps\n",
      "| epoch: 1 | step: 200 | loss: 1.53120 | grad_norm: 0.27490 | 28.831 sec / 10 steps\n",
      "| epoch: 2 | step: 210 | loss: 1.53284 | grad_norm: 0.27153 | 30.630 sec / 10 steps\n",
      "| epoch: 2 | step: 220 | loss: 1.50318 | grad_norm: 0.27090 | 54.011 sec / 10 steps\n",
      "| epoch: 2 | step: 230 | loss: 1.46474 | grad_norm: 0.27618 | 53.335 sec / 10 steps\n",
      "| epoch: 2 | step: 240 | loss: 1.48968 | grad_norm: 0.26670 | 53.137 sec / 10 steps\n",
      "| epoch: 2 | step: 250 | loss: 1.46096 | grad_norm: 0.26712 | 50.472 sec / 10 steps\n",
      "| epoch: 2 | step: 260 | loss: 1.41659 | grad_norm: 0.25602 | 53.111 sec / 10 steps\n",
      "| epoch: 2 | step: 270 | loss: 1.47670 | grad_norm: 0.32979 | 55.989 sec / 10 steps\n",
      "| epoch: 2 | step: 280 | loss: 1.34965 | grad_norm: 0.26042 | 52.620 sec / 10 steps\n",
      "| epoch: 2 | step: 290 | loss: 1.38093 | grad_norm: 0.25989 | 56.293 sec / 10 steps\n",
      "| epoch: 2 | step: 300 | loss: 1.37221 | grad_norm: 0.26453 | 54.746 sec / 10 steps\n",
      "| epoch: 2 | step: 310 | loss: 1.28950 | grad_norm: 0.26945 | 57.041 sec / 10 steps\n",
      "| epoch: 2 | step: 320 | loss: 1.26610 | grad_norm: 0.24485 | 54.461 sec / 10 steps\n",
      "| epoch: 2 | step: 330 | loss: 1.28309 | grad_norm: 0.23914 | 54.059 sec / 10 steps\n",
      "| epoch: 2 | step: 340 | loss: 1.26461 | grad_norm: 0.23692 | 55.005 sec / 10 steps\n",
      "| epoch: 2 | step: 350 | loss: 1.24108 | grad_norm: 0.23674 | 49.674 sec / 10 steps\n",
      "| epoch: 2 | step: 360 | loss: 1.21575 | grad_norm: 0.22955 | 54.258 sec / 10 steps\n",
      "| epoch: 2 | step: 370 | loss: 1.21113 | grad_norm: 0.22601 | 28.589 sec / 10 steps\n",
      "| epoch: 2 | step: 380 | loss: 1.19474 | grad_norm: 0.22304 | 24.274 sec / 10 steps\n",
      "| epoch: 2 | step: 390 | loss: 1.15076 | grad_norm: 0.22366 | 23.533 sec / 10 steps\n",
      "| epoch: 2 | step: 400 | loss: 1.11845 | grad_norm: 0.22502 | 25.026 sec / 10 steps\n",
      "| epoch: 3 | step: 410 | loss: 1.12407 | grad_norm: 0.22076 | 22.181 sec / 10 steps\n",
      "| epoch: 3 | step: 420 | loss: 1.13459 | grad_norm: 0.21111 | 24.204 sec / 10 steps\n",
      "| epoch: 3 | step: 430 | loss: 1.08750 | grad_norm: 0.21492 | 23.761 sec / 10 steps\n",
      "| epoch: 3 | step: 440 | loss: 1.11941 | grad_norm: 0.20631 | 22.285 sec / 10 steps\n",
      "| epoch: 3 | step: 450 | loss: 1.10442 | grad_norm: 0.20785 | 22.898 sec / 10 steps\n",
      "| epoch: 3 | step: 460 | loss: 1.05614 | grad_norm: 0.20360 | 21.446 sec / 10 steps\n",
      "| epoch: 3 | step: 470 | loss: 1.07215 | grad_norm: 0.20041 | 22.340 sec / 10 steps\n",
      "| epoch: 3 | step: 480 | loss: 1.00686 | grad_norm: 0.29726 | 24.007 sec / 10 steps\n",
      "| epoch: 3 | step: 490 | loss: 1.01922 | grad_norm: 0.19494 | 22.956 sec / 10 steps\n",
      "| epoch: 3 | step: 500 | loss: 1.00422 | grad_norm: 0.19377 | 25.241 sec / 10 steps\n",
      "| epoch: 3 | step: 510 | loss: 0.98233 | grad_norm: 0.19258 | 22.510 sec / 10 steps\n",
      "| epoch: 3 | step: 520 | loss: 0.98947 | grad_norm: 0.18584 | 23.677 sec / 10 steps\n",
      "| epoch: 3 | step: 530 | loss: 1.00407 | grad_norm: 0.18457 | 22.526 sec / 10 steps\n",
      "| epoch: 3 | step: 540 | loss: 0.94728 | grad_norm: 0.18358 | 22.307 sec / 10 steps\n",
      "| epoch: 3 | step: 550 | loss: 0.97182 | grad_norm: 0.17985 | 23.798 sec / 10 steps\n",
      "| epoch: 3 | step: 560 | loss: 0.98495 | grad_norm: 0.18191 | 24.237 sec / 10 steps\n",
      "| epoch: 3 | step: 570 | loss: 0.92594 | grad_norm: 0.17983 | 25.378 sec / 10 steps\n",
      "| epoch: 3 | step: 580 | loss: 0.94953 | grad_norm: 0.17532 | 23.690 sec / 10 steps\n",
      "| epoch: 3 | step: 590 | loss: 0.92210 | grad_norm: 0.17268 | 23.804 sec / 10 steps\n",
      "| epoch: 3 | step: 600 | loss: 0.90736 | grad_norm: 0.16747 | 23.995 sec / 10 steps\n",
      "| epoch: 4 | step: 610 | loss: 0.89592 | grad_norm: 0.16583 | 23.740 sec / 10 steps\n",
      "| epoch: 4 | step: 620 | loss: 0.86666 | grad_norm: 0.17437 | 23.433 sec / 10 steps\n",
      "| epoch: 4 | step: 630 | loss: 0.83157 | grad_norm: 0.17177 | 25.769 sec / 10 steps\n",
      "| epoch: 4 | step: 640 | loss: 0.87864 | grad_norm: 0.16272 | 26.616 sec / 10 steps\n",
      "| epoch: 4 | step: 650 | loss: 0.83377 | grad_norm: 0.16893 | 22.754 sec / 10 steps\n",
      "| epoch: 4 | step: 660 | loss: 0.84106 | grad_norm: 0.15392 | 24.417 sec / 10 steps\n",
      "| epoch: 4 | step: 670 | loss: 0.83196 | grad_norm: 0.15178 | 25.284 sec / 10 steps\n",
      "| epoch: 4 | step: 680 | loss: 0.82654 | grad_norm: 0.15257 | 23.901 sec / 10 steps\n",
      "| epoch: 4 | step: 690 | loss: 0.81565 | grad_norm: 0.15088 | 23.668 sec / 10 steps\n",
      "| epoch: 4 | step: 700 | loss: 0.84978 | grad_norm: 0.15974 | 24.998 sec / 10 steps\n",
      "| epoch: 4 | step: 710 | loss: 0.79335 | grad_norm: 0.14322 | 22.565 sec / 10 steps\n",
      "| epoch: 4 | step: 720 | loss: 0.80234 | grad_norm: 0.14115 | 23.498 sec / 10 steps\n",
      "| epoch: 4 | step: 730 | loss: 0.76984 | grad_norm: 0.14193 | 21.348 sec / 10 steps\n",
      "| epoch: 4 | step: 740 | loss: 0.77239 | grad_norm: 0.13750 | 24.982 sec / 10 steps\n",
      "| epoch: 4 | step: 750 | loss: 0.77095 | grad_norm: 0.13493 | 22.524 sec / 10 steps\n",
      "| epoch: 4 | step: 760 | loss: 0.76743 | grad_norm: 0.13201 | 22.164 sec / 10 steps\n",
      "| epoch: 4 | step: 770 | loss: 0.77469 | grad_norm: 0.13211 | 21.884 sec / 10 steps\n",
      "| epoch: 4 | step: 780 | loss: 0.74734 | grad_norm: 0.13033 | 23.555 sec / 10 steps\n",
      "| epoch: 4 | step: 790 | loss: 0.71030 | grad_norm: 0.12828 | 23.490 sec / 10 steps\n",
      "| epoch: 4 | step: 800 | loss: 0.75222 | grad_norm: 0.12621 | 24.378 sec / 10 steps\n",
      "| epoch: 5 | step: 810 | loss: 0.71121 | grad_norm: 0.12346 | 24.740 sec / 10 steps\n",
      "| epoch: 5 | step: 820 | loss: 0.72513 | grad_norm: 0.13655 | 22.434 sec / 10 steps\n",
      "| epoch: 5 | step: 830 | loss: 0.70533 | grad_norm: 0.11876 | 22.660 sec / 10 steps\n",
      "| epoch: 5 | step: 840 | loss: 0.73165 | grad_norm: 0.11972 | 23.574 sec / 10 steps\n",
      "| epoch: 5 | step: 850 | loss: 0.68043 | grad_norm: 0.12681 | 24.399 sec / 10 steps\n",
      "| epoch: 5 | step: 860 | loss: 0.71237 | grad_norm: 0.11406 | 21.293 sec / 10 steps\n",
      "| epoch: 5 | step: 870 | loss: 0.68841 | grad_norm: 0.11607 | 21.325 sec / 10 steps\n",
      "| epoch: 5 | step: 880 | loss: 0.63173 | grad_norm: 0.12094 | 25.842 sec / 10 steps\n",
      "| epoch: 5 | step: 890 | loss: 0.66309 | grad_norm: 0.10873 | 22.737 sec / 10 steps\n",
      "| epoch: 5 | step: 900 | loss: 0.66578 | grad_norm: 0.10623 | 20.977 sec / 10 steps\n",
      "| epoch: 5 | step: 910 | loss: 0.71909 | grad_norm: 0.12650 | 22.766 sec / 10 steps\n",
      "| epoch: 5 | step: 920 | loss: 0.68062 | grad_norm: 0.10694 | 23.996 sec / 10 steps\n",
      "| epoch: 5 | step: 930 | loss: 0.66327 | grad_norm: 0.10580 | 22.697 sec / 10 steps\n",
      "| epoch: 5 | step: 940 | loss: 0.66924 | grad_norm: 0.09975 | 22.824 sec / 10 steps\n",
      "| epoch: 5 | step: 950 | loss: 0.70927 | grad_norm: 0.10484 | 23.144 sec / 10 steps\n",
      "| epoch: 5 | step: 960 | loss: 0.63094 | grad_norm: 0.09580 | 22.173 sec / 10 steps\n",
      "| epoch: 5 | step: 970 | loss: 0.63875 | grad_norm: 0.09736 | 21.960 sec / 10 steps\n",
      "| epoch: 5 | step: 980 | loss: 0.65370 | grad_norm: 0.09357 | 23.713 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 5 | step: 990 | loss: 0.67535 | grad_norm: 0.10304 | 21.156 sec / 10 steps\n",
      "| epoch: 5 | step: 1000 | loss: 0.61594 | grad_norm: 0.08954 | 24.244 sec / 10 steps\n",
      "| epoch: 6 | step: 1010 | loss: 0.59434 | grad_norm: 0.09058 | 24.316 sec / 10 steps\n",
      "| epoch: 6 | step: 1020 | loss: 0.61787 | grad_norm: 0.08979 | 23.579 sec / 10 steps\n",
      "| epoch: 6 | step: 1030 | loss: 0.61094 | grad_norm: 0.08396 | 21.669 sec / 10 steps\n",
      "| epoch: 6 | step: 1040 | loss: 0.65682 | grad_norm: 0.09645 | 22.515 sec / 10 steps\n",
      "| epoch: 6 | step: 1050 | loss: 0.61551 | grad_norm: 0.08696 | 22.988 sec / 10 steps\n",
      "| epoch: 6 | step: 1060 | loss: 0.61816 | grad_norm: 0.08677 | 24.668 sec / 10 steps\n",
      "| epoch: 6 | step: 1070 | loss: 0.56728 | grad_norm: 0.07957 | 23.149 sec / 10 steps\n",
      "| epoch: 6 | step: 1080 | loss: 0.62159 | grad_norm: 0.09206 | 22.409 sec / 10 steps\n",
      "| epoch: 6 | step: 1090 | loss: 0.57316 | grad_norm: 0.08522 | 23.548 sec / 10 steps\n",
      "| epoch: 6 | step: 1100 | loss: 0.58501 | grad_norm: 0.07838 | 23.236 sec / 10 steps\n",
      "| epoch: 6 | step: 1110 | loss: 0.55217 | grad_norm: 0.07817 | 23.694 sec / 10 steps\n",
      "| epoch: 6 | step: 1120 | loss: 0.61093 | grad_norm: 0.09556 | 22.634 sec / 10 steps\n",
      "| epoch: 6 | step: 1130 | loss: 0.56512 | grad_norm: 0.07222 | 22.899 sec / 10 steps\n",
      "| epoch: 6 | step: 1140 | loss: 0.60328 | grad_norm: 0.07240 | 21.729 sec / 10 steps\n",
      "| epoch: 6 | step: 1150 | loss: 0.58742 | grad_norm: 0.06835 | 22.794 sec / 10 steps\n",
      "| epoch: 6 | step: 1160 | loss: 0.57402 | grad_norm: 0.06916 | 23.383 sec / 10 steps\n",
      "| epoch: 6 | step: 1170 | loss: 0.59433 | grad_norm: 0.07849 | 23.636 sec / 10 steps\n",
      "| epoch: 6 | step: 1180 | loss: 0.59603 | grad_norm: 0.06814 | 23.289 sec / 10 steps\n",
      "| epoch: 6 | step: 1190 | loss: 0.59554 | grad_norm: 0.06476 | 20.882 sec / 10 steps\n",
      "| epoch: 6 | step: 1200 | loss: 0.60163 | grad_norm: 0.06550 | 23.894 sec / 10 steps\n",
      "| epoch: 7 | step: 1210 | loss: 0.58229 | grad_norm: 0.06408 | 23.402 sec / 10 steps\n",
      "| epoch: 7 | step: 1220 | loss: 0.54888 | grad_norm: 0.06274 | 24.001 sec / 10 steps\n",
      "| epoch: 7 | step: 1230 | loss: 0.54660 | grad_norm: 0.06349 | 22.831 sec / 10 steps\n",
      "| epoch: 7 | step: 1240 | loss: 0.57450 | grad_norm: 0.06367 | 21.592 sec / 10 steps\n",
      "| epoch: 7 | step: 1250 | loss: 0.56666 | grad_norm: 0.06397 | 24.119 sec / 10 steps\n",
      "| epoch: 7 | step: 1260 | loss: 0.55869 | grad_norm: 0.05915 | 22.582 sec / 10 steps\n",
      "| epoch: 7 | step: 1270 | loss: 0.56449 | grad_norm: 0.05352 | 23.207 sec / 10 steps\n",
      "| epoch: 7 | step: 1280 | loss: 0.58011 | grad_norm: 0.05856 | 21.058 sec / 10 steps\n",
      "| epoch: 7 | step: 1290 | loss: 0.61591 | grad_norm: 0.05379 | 20.794 sec / 10 steps\n",
      "| epoch: 7 | step: 1300 | loss: 0.54337 | grad_norm: 0.05748 | 24.511 sec / 10 steps\n",
      "| epoch: 7 | step: 1310 | loss: 0.54968 | grad_norm: 0.05290 | 23.360 sec / 10 steps\n",
      "| epoch: 7 | step: 1320 | loss: 0.56270 | grad_norm: 0.05797 | 23.794 sec / 10 steps\n",
      "| epoch: 7 | step: 1330 | loss: 0.57317 | grad_norm: 0.05574 | 23.791 sec / 10 steps\n",
      "| epoch: 7 | step: 1340 | loss: 0.57436 | grad_norm: 0.04792 | 21.023 sec / 10 steps\n",
      "| epoch: 7 | step: 1350 | loss: 0.53853 | grad_norm: 0.04863 | 24.595 sec / 10 steps\n",
      "| epoch: 7 | step: 1360 | loss: 0.58058 | grad_norm: 0.04464 | 22.371 sec / 10 steps\n",
      "| epoch: 7 | step: 1370 | loss: 0.55900 | grad_norm: 0.04831 | 22.129 sec / 10 steps\n",
      "| epoch: 7 | step: 1380 | loss: 0.53865 | grad_norm: 0.04468 | 23.291 sec / 10 steps\n",
      "| epoch: 7 | step: 1390 | loss: 0.56650 | grad_norm: 0.04199 | 22.616 sec / 10 steps\n",
      "| epoch: 7 | step: 1400 | loss: 0.53637 | grad_norm: 0.04497 | 22.170 sec / 10 steps\n",
      "| epoch: 8 | step: 1410 | loss: 0.54775 | grad_norm: 0.04505 | 24.022 sec / 10 steps\n",
      "| epoch: 8 | step: 1420 | loss: 0.56346 | grad_norm: 0.04012 | 22.022 sec / 10 steps\n",
      "| epoch: 8 | step: 1430 | loss: 0.57590 | grad_norm: 0.04035 | 22.083 sec / 10 steps\n",
      "| epoch: 8 | step: 1440 | loss: 0.56142 | grad_norm: 0.04547 | 22.879 sec / 10 steps\n",
      "| epoch: 8 | step: 1450 | loss: 0.54907 | grad_norm: 0.03681 | 22.958 sec / 10 steps\n",
      "| epoch: 8 | step: 1460 | loss: 0.50143 | grad_norm: 0.04017 | 23.717 sec / 10 steps\n",
      "| epoch: 8 | step: 1470 | loss: 0.55667 | grad_norm: 0.04425 | 23.626 sec / 10 steps\n",
      "| epoch: 8 | step: 1480 | loss: 0.56319 | grad_norm: 0.03400 | 23.889 sec / 10 steps\n",
      "| epoch: 8 | step: 1490 | loss: 0.52255 | grad_norm: 0.04704 | 21.758 sec / 10 steps\n",
      "| epoch: 8 | step: 1500 | loss: 0.51152 | grad_norm: 0.03420 | 23.096 sec / 10 steps\n",
      "| epoch: 8 | step: 1510 | loss: 0.52042 | grad_norm: 0.03234 | 25.128 sec / 10 steps\n",
      "| epoch: 8 | step: 1520 | loss: 0.58447 | grad_norm: 0.03859 | 23.274 sec / 10 steps\n",
      "| epoch: 8 | step: 1530 | loss: 0.52925 | grad_norm: 0.03081 | 21.577 sec / 10 steps\n",
      "| epoch: 8 | step: 1540 | loss: 0.53988 | grad_norm: 0.03106 | 22.283 sec / 10 steps\n",
      "| epoch: 8 | step: 1550 | loss: 0.58282 | grad_norm: 0.03124 | 21.055 sec / 10 steps\n",
      "| epoch: 8 | step: 1560 | loss: 0.51570 | grad_norm: 0.03548 | 22.375 sec / 10 steps\n",
      "| epoch: 8 | step: 1570 | loss: 0.54613 | grad_norm: 0.05695 | 22.975 sec / 10 steps\n",
      "| epoch: 8 | step: 1580 | loss: 0.55162 | grad_norm: 0.03834 | 23.085 sec / 10 steps\n",
      "| epoch: 8 | step: 1590 | loss: 0.51216 | grad_norm: 0.03425 | 23.939 sec / 10 steps\n",
      "| epoch: 8 | step: 1600 | loss: 0.49617 | grad_norm: 0.02907 | 23.253 sec / 10 steps\n",
      "| epoch: 9 | step: 1610 | loss: 0.54882 | grad_norm: 0.02627 | 22.059 sec / 10 steps\n",
      "| epoch: 9 | step: 1620 | loss: 0.57298 | grad_norm: 0.02536 | 22.117 sec / 10 steps\n",
      "| epoch: 9 | step: 1630 | loss: 0.51312 | grad_norm: 0.02518 | 25.119 sec / 10 steps\n",
      "| epoch: 9 | step: 1640 | loss: 0.52586 | grad_norm: 0.02882 | 23.698 sec / 10 steps\n",
      "| epoch: 9 | step: 1650 | loss: 0.55434 | grad_norm: 0.02358 | 22.740 sec / 10 steps\n",
      "| epoch: 9 | step: 1660 | loss: 0.51204 | grad_norm: 0.02388 | 22.332 sec / 10 steps\n",
      "| epoch: 9 | step: 1670 | loss: 0.54955 | grad_norm: 0.02647 | 23.663 sec / 10 steps\n",
      "| epoch: 9 | step: 1680 | loss: 0.56403 | grad_norm: 0.02083 | 20.219 sec / 10 steps\n",
      "| epoch: 9 | step: 1690 | loss: 0.52338 | grad_norm: 0.02424 | 21.885 sec / 10 steps\n",
      "| epoch: 9 | step: 1700 | loss: 0.54676 | grad_norm: 0.02267 | 22.684 sec / 10 steps\n",
      "| epoch: 9 | step: 1710 | loss: 0.53650 | grad_norm: 0.02584 | 23.525 sec / 10 steps\n",
      "| epoch: 9 | step: 1720 | loss: 0.53350 | grad_norm: 0.02296 | 23.605 sec / 10 steps\n",
      "| epoch: 9 | step: 1730 | loss: 0.57495 | grad_norm: 0.02253 | 21.431 sec / 10 steps\n",
      "| epoch: 9 | step: 1740 | loss: 0.52483 | grad_norm: 0.02465 | 23.621 sec / 10 steps\n",
      "| epoch: 9 | step: 1750 | loss: 0.56090 | grad_norm: 0.02145 | 22.343 sec / 10 steps\n",
      "| epoch: 9 | step: 1760 | loss: 0.52187 | grad_norm: 0.01759 | 20.281 sec / 10 steps\n",
      "| epoch: 9 | step: 1770 | loss: 0.49256 | grad_norm: 0.01783 | 22.648 sec / 10 steps\n",
      "| epoch: 9 | step: 1780 | loss: 0.48932 | grad_norm: 0.02137 | 23.696 sec / 10 steps\n",
      "| epoch: 9 | step: 1790 | loss: 0.54195 | grad_norm: 0.01553 | 23.463 sec / 10 steps\n",
      "| epoch: 9 | step: 1800 | loss: 0.49933 | grad_norm: 0.02008 | 23.020 sec / 10 steps\n",
      "| epoch: 10 | step: 1810 | loss: 0.50473 | grad_norm: 0.01805 | 23.432 sec / 10 steps\n",
      "| epoch: 10 | step: 1820 | loss: 0.55213 | grad_norm: 0.02085 | 20.617 sec / 10 steps\n",
      "| epoch: 10 | step: 1830 | loss: 0.51639 | grad_norm: 0.02074 | 23.531 sec / 10 steps\n",
      "| epoch: 10 | step: 1840 | loss: 0.52826 | grad_norm: 0.01905 | 22.541 sec / 10 steps\n",
      "| epoch: 10 | step: 1850 | loss: 0.54275 | grad_norm: 0.01681 | 21.953 sec / 10 steps\n",
      "| epoch: 10 | step: 1860 | loss: 0.49010 | grad_norm: 0.02306 | 21.871 sec / 10 steps\n",
      "| epoch: 10 | step: 1870 | loss: 0.53849 | grad_norm: 0.01298 | 22.586 sec / 10 steps\n",
      "| epoch: 10 | step: 1880 | loss: 0.52533 | grad_norm: 0.01492 | 23.860 sec / 10 steps\n",
      "| epoch: 10 | step: 1890 | loss: 0.51761 | grad_norm: 0.01461 | 21.702 sec / 10 steps\n",
      "| epoch: 10 | step: 1900 | loss: 0.53098 | grad_norm: 0.03236 | 21.930 sec / 10 steps\n",
      "| epoch: 10 | step: 1910 | loss: 0.50975 | grad_norm: 0.01636 | 23.615 sec / 10 steps\n",
      "| epoch: 10 | step: 1920 | loss: 0.51858 | grad_norm: 0.01683 | 21.140 sec / 10 steps\n",
      "| epoch: 10 | step: 1930 | loss: 0.56094 | grad_norm: 0.02205 | 22.015 sec / 10 steps\n",
      "| epoch: 10 | step: 1940 | loss: 0.55659 | grad_norm: 0.01567 | 23.239 sec / 10 steps\n",
      "| epoch: 10 | step: 1950 | loss: 0.54949 | grad_norm: 0.01691 | 23.198 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 10 | step: 1960 | loss: 0.55536 | grad_norm: 0.01548 | 24.907 sec / 10 steps\n",
      "| epoch: 10 | step: 1970 | loss: 0.55350 | grad_norm: 0.01594 | 21.057 sec / 10 steps\n",
      "| epoch: 10 | step: 1980 | loss: 0.47277 | grad_norm: 0.01543 | 23.884 sec / 10 steps\n",
      "| epoch: 10 | step: 1990 | loss: 0.48830 | grad_norm: 0.01181 | 23.804 sec / 10 steps\n",
      "| epoch: 10 | step: 2000 | loss: 0.54263 | grad_norm: 0.01474 | 21.755 sec / 10 steps\n",
      "| epoch: 10 | step: 2010 | loss: 0.55376 | grad_norm: 0.01358 | 24.113 sec / 10 steps\n",
      "| epoch: 11 | step: 2020 | loss: 0.53745 | grad_norm: 0.01238 | 22.368 sec / 10 steps\n",
      "| epoch: 11 | step: 2030 | loss: 0.49920 | grad_norm: 0.03926 | 22.567 sec / 10 steps\n",
      "| epoch: 11 | step: 2040 | loss: 0.52008 | grad_norm: 0.02205 | 21.376 sec / 10 steps\n",
      "| epoch: 11 | step: 2050 | loss: 0.53413 | grad_norm: 0.01367 | 22.656 sec / 10 steps\n",
      "| epoch: 11 | step: 2060 | loss: 0.48786 | grad_norm: 0.02947 | 22.722 sec / 10 steps\n",
      "| epoch: 11 | step: 2070 | loss: 0.51888 | grad_norm: 0.01405 | 23.170 sec / 10 steps\n",
      "| epoch: 11 | step: 2080 | loss: 0.51664 | grad_norm: 0.01583 | 23.752 sec / 10 steps\n",
      "| epoch: 11 | step: 2090 | loss: 0.51573 | grad_norm: 0.01441 | 22.707 sec / 10 steps\n",
      "| epoch: 11 | step: 2100 | loss: 0.51923 | grad_norm: 0.01221 | 24.768 sec / 10 steps\n",
      "| epoch: 11 | step: 2110 | loss: 0.54828 | grad_norm: 0.01540 | 23.452 sec / 10 steps\n",
      "| epoch: 11 | step: 2120 | loss: 0.49917 | grad_norm: 0.01084 | 22.163 sec / 10 steps\n",
      "| epoch: 11 | step: 2130 | loss: 0.53359 | grad_norm: 0.01014 | 21.162 sec / 10 steps\n",
      "| epoch: 11 | step: 2140 | loss: 0.52463 | grad_norm: 0.01515 | 23.923 sec / 10 steps\n",
      "| epoch: 11 | step: 2150 | loss: 0.51115 | grad_norm: 0.01084 | 23.683 sec / 10 steps\n",
      "| epoch: 11 | step: 2160 | loss: 0.52659 | grad_norm: 0.00914 | 22.873 sec / 10 steps\n",
      "| epoch: 11 | step: 2170 | loss: 0.48245 | grad_norm: 0.01760 | 22.763 sec / 10 steps\n",
      "| epoch: 11 | step: 2180 | loss: 0.51902 | grad_norm: 0.01131 | 22.134 sec / 10 steps\n",
      "| epoch: 11 | step: 2190 | loss: 0.51935 | grad_norm: 0.01803 | 22.412 sec / 10 steps\n",
      "| epoch: 11 | step: 2200 | loss: 0.53187 | grad_norm: 0.01632 | 23.204 sec / 10 steps\n",
      "| epoch: 11 | step: 2210 | loss: 0.52559 | grad_norm: 0.01477 | 21.972 sec / 10 steps\n",
      "| epoch: 12 | step: 2220 | loss: 0.54657 | grad_norm: 0.01625 | 24.905 sec / 10 steps\n",
      "| epoch: 12 | step: 2230 | loss: 0.53117 | grad_norm: 0.05161 | 21.125 sec / 10 steps\n",
      "| epoch: 12 | step: 2240 | loss: 0.54122 | grad_norm: 0.01781 | 20.270 sec / 10 steps\n",
      "| epoch: 12 | step: 2250 | loss: 0.53276 | grad_norm: 0.01163 | 22.667 sec / 10 steps\n",
      "| epoch: 12 | step: 2260 | loss: 0.50903 | grad_norm: 0.01476 | 23.251 sec / 10 steps\n",
      "| epoch: 12 | step: 2270 | loss: 0.56705 | grad_norm: 0.01626 | 23.058 sec / 10 steps\n",
      "| epoch: 12 | step: 2280 | loss: 0.54564 | grad_norm: 0.01046 | 21.608 sec / 10 steps\n",
      "| epoch: 12 | step: 2290 | loss: 0.55475 | grad_norm: 0.00877 | 20.749 sec / 10 steps\n",
      "| epoch: 12 | step: 2300 | loss: 0.54123 | grad_norm: 0.00929 | 23.457 sec / 10 steps\n",
      "| epoch: 12 | step: 2310 | loss: 0.54466 | grad_norm: 0.01574 | 22.052 sec / 10 steps\n",
      "| epoch: 12 | step: 2320 | loss: 0.55351 | grad_norm: 0.01201 | 22.912 sec / 10 steps\n",
      "| epoch: 12 | step: 2330 | loss: 0.52738 | grad_norm: 0.01696 | 23.029 sec / 10 steps\n",
      "| epoch: 12 | step: 2340 | loss: 0.53339 | grad_norm: 0.01472 | 20.885 sec / 10 steps\n",
      "| epoch: 12 | step: 2350 | loss: 0.56740 | grad_norm: 0.00889 | 22.441 sec / 10 steps\n",
      "| epoch: 12 | step: 2360 | loss: 0.51825 | grad_norm: 0.01049 | 22.836 sec / 10 steps\n",
      "| epoch: 12 | step: 2370 | loss: 0.53248 | grad_norm: 0.00806 | 21.660 sec / 10 steps\n",
      "| epoch: 12 | step: 2380 | loss: 0.51193 | grad_norm: 0.01189 | 22.554 sec / 10 steps\n",
      "| epoch: 12 | step: 2390 | loss: 0.51525 | grad_norm: 0.01526 | 23.226 sec / 10 steps\n",
      "| epoch: 12 | step: 2400 | loss: 0.49708 | grad_norm: 0.01316 | 22.867 sec / 10 steps\n",
      "| epoch: 12 | step: 2410 | loss: 0.48177 | grad_norm: 0.01096 | 24.828 sec / 10 steps\n",
      "| epoch: 13 | step: 2420 | loss: 0.52394 | grad_norm: 0.01136 | 23.812 sec / 10 steps\n",
      "| epoch: 13 | step: 2430 | loss: 0.53746 | grad_norm: 0.01195 | 22.668 sec / 10 steps\n",
      "| epoch: 13 | step: 2440 | loss: 0.53320 | grad_norm: 0.01944 | 22.054 sec / 10 steps\n",
      "| epoch: 13 | step: 2450 | loss: 0.52456 | grad_norm: 0.01240 | 21.872 sec / 10 steps\n",
      "| epoch: 13 | step: 2460 | loss: 0.52189 | grad_norm: 0.01514 | 23.670 sec / 10 steps\n",
      "| epoch: 13 | step: 2470 | loss: 0.51242 | grad_norm: 0.02453 | 24.516 sec / 10 steps\n",
      "| epoch: 13 | step: 2480 | loss: 0.55973 | grad_norm: 0.02811 | 22.649 sec / 10 steps\n",
      "| epoch: 13 | step: 2490 | loss: 0.53069 | grad_norm: 0.01957 | 25.684 sec / 10 steps\n",
      "| epoch: 13 | step: 2500 | loss: 0.56326 | grad_norm: 0.02404 | 20.892 sec / 10 steps\n",
      "| epoch: 13 | step: 2510 | loss: 0.55684 | grad_norm: 0.01704 | 22.044 sec / 10 steps\n",
      "| epoch: 13 | step: 2520 | loss: 0.50125 | grad_norm: 0.01959 | 21.833 sec / 10 steps\n",
      "| epoch: 13 | step: 2530 | loss: 0.52075 | grad_norm: 0.02391 | 23.540 sec / 10 steps\n",
      "| epoch: 13 | step: 2540 | loss: 0.55577 | grad_norm: 0.05166 | 24.302 sec / 10 steps\n",
      "| epoch: 13 | step: 2550 | loss: 0.53871 | grad_norm: 0.01883 | 22.413 sec / 10 steps\n",
      "| epoch: 13 | step: 2560 | loss: 0.53974 | grad_norm: 0.02297 | 22.920 sec / 10 steps\n",
      "| epoch: 13 | step: 2570 | loss: 0.54207 | grad_norm: 0.01245 | 24.907 sec / 10 steps\n",
      "| epoch: 13 | step: 2580 | loss: 0.50993 | grad_norm: 0.01660 | 24.661 sec / 10 steps\n",
      "| epoch: 13 | step: 2590 | loss: 0.54408 | grad_norm: 0.01036 | 21.727 sec / 10 steps\n",
      "| epoch: 13 | step: 2600 | loss: 0.49810 | grad_norm: 0.00781 | 23.042 sec / 10 steps\n",
      "| epoch: 13 | step: 2610 | loss: 0.48567 | grad_norm: 0.00862 | 22.758 sec / 10 steps\n",
      "| epoch: 14 | step: 2620 | loss: 0.50945 | grad_norm: 0.00809 | 23.981 sec / 10 steps\n",
      "| epoch: 14 | step: 2630 | loss: 0.47703 | grad_norm: 0.00884 | 23.630 sec / 10 steps\n",
      "| epoch: 14 | step: 2640 | loss: 0.52916 | grad_norm: 0.01522 | 23.352 sec / 10 steps\n",
      "| epoch: 14 | step: 2650 | loss: 0.46510 | grad_norm: 0.02448 | 22.574 sec / 10 steps\n",
      "| epoch: 14 | step: 2660 | loss: 0.50069 | grad_norm: 0.02708 | 23.450 sec / 10 steps\n",
      "| epoch: 14 | step: 2670 | loss: 0.53726 | grad_norm: 0.00896 | 22.566 sec / 10 steps\n",
      "| epoch: 14 | step: 2680 | loss: 0.49398 | grad_norm: 0.02234 | 25.096 sec / 10 steps\n",
      "| epoch: 14 | step: 2690 | loss: 0.53895 | grad_norm: 0.00960 | 21.804 sec / 10 steps\n",
      "| epoch: 14 | step: 2700 | loss: 0.50600 | grad_norm: 0.01445 | 22.091 sec / 10 steps\n",
      "| epoch: 14 | step: 2710 | loss: 0.51619 | grad_norm: 0.01478 | 21.903 sec / 10 steps\n",
      "| epoch: 14 | step: 2720 | loss: 0.56770 | grad_norm: 0.01707 | 23.353 sec / 10 steps\n",
      "| epoch: 14 | step: 2730 | loss: 0.48710 | grad_norm: 0.01247 | 21.909 sec / 10 steps\n",
      "| epoch: 14 | step: 2740 | loss: 0.52790 | grad_norm: 0.00931 | 22.905 sec / 10 steps\n",
      "| epoch: 14 | step: 2750 | loss: 0.48020 | grad_norm: 0.01293 | 23.762 sec / 10 steps\n",
      "| epoch: 14 | step: 2760 | loss: 0.54503 | grad_norm: 0.01583 | 22.343 sec / 10 steps\n",
      "| epoch: 14 | step: 2770 | loss: 0.48088 | grad_norm: 0.00903 | 24.034 sec / 10 steps\n",
      "| epoch: 14 | step: 2780 | loss: 0.50928 | grad_norm: 0.02005 | 22.846 sec / 10 steps\n",
      "| epoch: 14 | step: 2790 | loss: 0.52326 | grad_norm: 0.04075 | 24.433 sec / 10 steps\n",
      "| epoch: 14 | step: 2800 | loss: 0.53794 | grad_norm: 0.01402 | 21.485 sec / 10 steps\n",
      "| epoch: 14 | step: 2810 | loss: 0.55990 | grad_norm: 0.01954 | 22.773 sec / 10 steps\n",
      "| epoch: 15 | step: 2820 | loss: 0.51911 | grad_norm: 0.01119 | 22.346 sec / 10 steps\n",
      "| epoch: 15 | step: 2830 | loss: 0.52104 | grad_norm: 0.00814 | 24.206 sec / 10 steps\n",
      "| epoch: 15 | step: 2840 | loss: 0.50285 | grad_norm: 0.01470 | 24.421 sec / 10 steps\n",
      "| epoch: 15 | step: 2850 | loss: 0.55141 | grad_norm: 0.02094 | 21.811 sec / 10 steps\n",
      "| epoch: 15 | step: 2860 | loss: 0.54029 | grad_norm: 0.01377 | 22.749 sec / 10 steps\n",
      "| epoch: 15 | step: 2870 | loss: 0.54454 | grad_norm: 0.01251 | 22.155 sec / 10 steps\n",
      "| epoch: 15 | step: 2880 | loss: 0.54137 | grad_norm: 0.05758 | 21.725 sec / 10 steps\n",
      "| epoch: 15 | step: 2890 | loss: 0.51582 | grad_norm: 0.01642 | 23.164 sec / 10 steps\n",
      "| epoch: 15 | step: 2900 | loss: 0.52116 | grad_norm: 0.01512 | 22.859 sec / 10 steps\n",
      "| epoch: 15 | step: 2910 | loss: 0.52191 | grad_norm: 0.01098 | 22.284 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 15 | step: 2920 | loss: 0.52232 | grad_norm: 0.01565 | 23.692 sec / 10 steps\n",
      "| epoch: 15 | step: 2930 | loss: 0.56413 | grad_norm: 0.01240 | 22.561 sec / 10 steps\n",
      "| epoch: 15 | step: 2940 | loss: 0.55803 | grad_norm: 0.01205 | 22.502 sec / 10 steps\n",
      "| epoch: 15 | step: 2950 | loss: 0.49209 | grad_norm: 0.02219 | 23.024 sec / 10 steps\n",
      "| epoch: 15 | step: 2960 | loss: 0.54749 | grad_norm: 0.01575 | 21.424 sec / 10 steps\n",
      "| epoch: 15 | step: 2970 | loss: 0.52542 | grad_norm: 0.01077 | 20.262 sec / 10 steps\n",
      "| epoch: 15 | step: 2980 | loss: 0.50672 | grad_norm: 0.02829 | 22.948 sec / 10 steps\n",
      "| epoch: 15 | step: 2990 | loss: 0.53617 | grad_norm: 0.01204 | 21.838 sec / 10 steps\n",
      "| epoch: 15 | step: 3000 | loss: 0.53428 | grad_norm: 0.01450 | 22.195 sec / 10 steps\n",
      "| epoch: 15 | step: 3010 | loss: 0.51497 | grad_norm: 0.01795 | 23.233 sec / 10 steps\n",
      "| epoch: 16 | step: 3020 | loss: 0.54122 | grad_norm: 0.01519 | 23.392 sec / 10 steps\n",
      "| epoch: 16 | step: 3030 | loss: 0.51039 | grad_norm: 0.02166 | 22.559 sec / 10 steps\n",
      "| epoch: 16 | step: 3040 | loss: 0.51017 | grad_norm: 0.00810 | 24.602 sec / 10 steps\n",
      "| epoch: 16 | step: 3050 | loss: 0.52215 | grad_norm: 0.01610 | 23.497 sec / 10 steps\n",
      "| epoch: 16 | step: 3060 | loss: 0.52613 | grad_norm: 0.01428 | 24.270 sec / 10 steps\n",
      "| epoch: 16 | step: 3070 | loss: 0.54739 | grad_norm: 0.02869 | 23.119 sec / 10 steps\n",
      "| epoch: 16 | step: 3080 | loss: 0.52585 | grad_norm: 0.00914 | 21.713 sec / 10 steps\n",
      "| epoch: 16 | step: 3090 | loss: 0.49566 | grad_norm: 0.01117 | 23.715 sec / 10 steps\n",
      "| epoch: 16 | step: 3100 | loss: 0.55602 | grad_norm: 0.01880 | 22.199 sec / 10 steps\n",
      "| epoch: 16 | step: 3110 | loss: 0.52702 | grad_norm: 0.02422 | 23.021 sec / 10 steps\n",
      "| epoch: 16 | step: 3120 | loss: 0.56129 | grad_norm: 0.01777 | 23.074 sec / 10 steps\n",
      "| epoch: 16 | step: 3130 | loss: 0.53957 | grad_norm: 0.01864 | 22.805 sec / 10 steps\n",
      "| epoch: 16 | step: 3140 | loss: 0.51233 | grad_norm: 0.01140 | 22.552 sec / 10 steps\n",
      "| epoch: 16 | step: 3150 | loss: 0.54921 | grad_norm: 0.01001 | 22.530 sec / 10 steps\n",
      "| epoch: 16 | step: 3160 | loss: 0.54748 | grad_norm: 0.01816 | 21.918 sec / 10 steps\n",
      "| epoch: 16 | step: 3170 | loss: 0.51660 | grad_norm: 0.01917 | 22.668 sec / 10 steps\n",
      "| epoch: 16 | step: 3180 | loss: 0.53565 | grad_norm: 0.00982 | 23.251 sec / 10 steps\n",
      "| epoch: 16 | step: 3190 | loss: 0.54816 | grad_norm: 0.01179 | 23.096 sec / 10 steps\n",
      "| epoch: 16 | step: 3200 | loss: 0.49653 | grad_norm: 0.01049 | 22.402 sec / 10 steps\n",
      "| epoch: 16 | step: 3210 | loss: 0.52901 | grad_norm: 0.01329 | 24.783 sec / 10 steps\n",
      "| epoch: 17 | step: 3220 | loss: 0.53290 | grad_norm: 0.01583 | 22.767 sec / 10 steps\n",
      "| epoch: 17 | step: 3230 | loss: 0.55093 | grad_norm: 0.01078 | 23.198 sec / 10 steps\n",
      "| epoch: 17 | step: 3240 | loss: 0.52711 | grad_norm: 0.01378 | 23.080 sec / 10 steps\n",
      "| epoch: 17 | step: 3250 | loss: 0.52114 | grad_norm: 0.00790 | 22.914 sec / 10 steps\n",
      "| epoch: 17 | step: 3260 | loss: 0.54581 | grad_norm: 0.01749 | 21.899 sec / 10 steps\n",
      "| epoch: 17 | step: 3270 | loss: 0.55889 | grad_norm: 0.01780 | 21.340 sec / 10 steps\n",
      "| epoch: 17 | step: 3280 | loss: 0.52406 | grad_norm: 0.01881 | 22.964 sec / 10 steps\n",
      "| epoch: 17 | step: 3290 | loss: 0.55816 | grad_norm: 0.01761 | 22.048 sec / 10 steps\n",
      "| epoch: 17 | step: 3300 | loss: 0.48738 | grad_norm: 0.01004 | 23.172 sec / 10 steps\n",
      "| epoch: 17 | step: 3310 | loss: 0.51775 | grad_norm: 0.01421 | 22.200 sec / 10 steps\n",
      "| epoch: 17 | step: 3320 | loss: 0.53998 | grad_norm: 0.01143 | 22.779 sec / 10 steps\n",
      "| epoch: 17 | step: 3330 | loss: 0.52290 | grad_norm: 0.01226 | 22.332 sec / 10 steps\n",
      "| epoch: 17 | step: 3340 | loss: 0.52402 | grad_norm: 0.00842 | 22.356 sec / 10 steps\n",
      "| epoch: 17 | step: 3350 | loss: 0.57189 | grad_norm: 0.01818 | 23.260 sec / 10 steps\n",
      "| epoch: 17 | step: 3360 | loss: 0.53926 | grad_norm: 0.01888 | 23.724 sec / 10 steps\n",
      "| epoch: 17 | step: 3370 | loss: 0.53897 | grad_norm: 0.02087 | 23.016 sec / 10 steps\n",
      "| epoch: 17 | step: 3380 | loss: 0.51696 | grad_norm: 0.00827 | 24.006 sec / 10 steps\n",
      "| epoch: 17 | step: 3390 | loss: 0.48241 | grad_norm: 0.00996 | 23.531 sec / 10 steps\n",
      "| epoch: 17 | step: 3400 | loss: 0.49754 | grad_norm: 0.01852 | 23.668 sec / 10 steps\n",
      "| epoch: 17 | step: 3410 | loss: 0.51915 | grad_norm: 0.01120 | 24.183 sec / 10 steps\n",
      "| epoch: 18 | step: 3420 | loss: 0.51698 | grad_norm: 0.01060 | 23.879 sec / 10 steps\n",
      "| epoch: 18 | step: 3430 | loss: 0.51445 | grad_norm: 0.01304 | 21.588 sec / 10 steps\n",
      "| epoch: 18 | step: 3440 | loss: 0.55719 | grad_norm: 0.04939 | 21.966 sec / 10 steps\n",
      "| epoch: 18 | step: 3450 | loss: 0.51082 | grad_norm: 0.01026 | 22.322 sec / 10 steps\n",
      "| epoch: 18 | step: 3460 | loss: 0.53827 | grad_norm: 0.01445 | 23.685 sec / 10 steps\n",
      "| epoch: 18 | step: 3470 | loss: 0.54192 | grad_norm: 0.01408 | 22.414 sec / 10 steps\n",
      "| epoch: 18 | step: 3480 | loss: 0.54555 | grad_norm: 0.01105 | 23.903 sec / 10 steps\n",
      "| epoch: 18 | step: 3490 | loss: 0.54941 | grad_norm: 0.01708 | 22.423 sec / 10 steps\n",
      "| epoch: 18 | step: 3500 | loss: 0.48872 | grad_norm: 0.00803 | 21.603 sec / 10 steps\n",
      "| epoch: 18 | step: 3510 | loss: 0.55183 | grad_norm: 0.01156 | 23.654 sec / 10 steps\n",
      "| epoch: 18 | step: 3520 | loss: 0.51790 | grad_norm: 0.00895 | 25.230 sec / 10 steps\n",
      "| epoch: 18 | step: 3530 | loss: 0.53536 | grad_norm: 0.01517 | 23.444 sec / 10 steps\n",
      "| epoch: 18 | step: 3540 | loss: 0.50693 | grad_norm: 0.01121 | 21.465 sec / 10 steps\n",
      "| epoch: 18 | step: 3550 | loss: 0.47687 | grad_norm: 0.00832 | 23.876 sec / 10 steps\n",
      "| epoch: 18 | step: 3560 | loss: 0.49205 | grad_norm: 0.01159 | 24.293 sec / 10 steps\n",
      "| epoch: 18 | step: 3570 | loss: 0.52932 | grad_norm: 0.00920 | 23.431 sec / 10 steps\n",
      "| epoch: 18 | step: 3580 | loss: 0.53877 | grad_norm: 0.01034 | 21.473 sec / 10 steps\n",
      "| epoch: 18 | step: 3590 | loss: 0.49539 | grad_norm: 0.01037 | 21.572 sec / 10 steps\n",
      "| epoch: 18 | step: 3600 | loss: 0.53284 | grad_norm: 0.02493 | 20.219 sec / 10 steps\n",
      "| epoch: 18 | step: 3610 | loss: 0.51928 | grad_norm: 0.01030 | 24.626 sec / 10 steps\n",
      "| epoch: 19 | step: 3620 | loss: 0.49913 | grad_norm: 0.00956 | 21.906 sec / 10 steps\n",
      "| epoch: 19 | step: 3630 | loss: 0.53792 | grad_norm: 0.01064 | 22.193 sec / 10 steps\n",
      "| epoch: 19 | step: 3640 | loss: 0.53755 | grad_norm: 0.01423 | 23.954 sec / 10 steps\n",
      "| epoch: 19 | step: 3650 | loss: 0.49537 | grad_norm: 0.00844 | 22.576 sec / 10 steps\n",
      "| epoch: 19 | step: 3660 | loss: 0.53858 | grad_norm: 0.01083 | 22.644 sec / 10 steps\n",
      "| epoch: 19 | step: 3670 | loss: 0.50432 | grad_norm: 0.01356 | 23.639 sec / 10 steps\n",
      "| epoch: 19 | step: 3680 | loss: 0.50701 | grad_norm: 0.02194 | 23.148 sec / 10 steps\n",
      "| epoch: 19 | step: 3690 | loss: 0.55118 | grad_norm: 0.01102 | 22.904 sec / 10 steps\n",
      "| epoch: 19 | step: 3700 | loss: 0.53001 | grad_norm: 0.00799 | 20.889 sec / 10 steps\n",
      "| epoch: 19 | step: 3710 | loss: 0.53743 | grad_norm: 0.00913 | 22.015 sec / 10 steps\n",
      "| epoch: 19 | step: 3720 | loss: 0.48450 | grad_norm: 0.00790 | 25.169 sec / 10 steps\n",
      "| epoch: 19 | step: 3730 | loss: 0.52864 | grad_norm: 0.01724 | 22.871 sec / 10 steps\n",
      "| epoch: 19 | step: 3740 | loss: 0.50265 | grad_norm: 0.01506 | 20.977 sec / 10 steps\n",
      "| epoch: 19 | step: 3750 | loss: 0.52953 | grad_norm: 0.01534 | 23.744 sec / 10 steps\n",
      "| epoch: 19 | step: 3760 | loss: 0.53934 | grad_norm: 0.01010 | 21.558 sec / 10 steps\n",
      "| epoch: 19 | step: 3770 | loss: 0.51458 | grad_norm: 0.00891 | 23.278 sec / 10 steps\n",
      "| epoch: 19 | step: 3780 | loss: 0.51727 | grad_norm: 0.00672 | 21.349 sec / 10 steps\n",
      "| epoch: 19 | step: 3790 | loss: 0.54119 | grad_norm: 0.01692 | 23.001 sec / 10 steps\n",
      "| epoch: 19 | step: 3800 | loss: 0.50317 | grad_norm: 0.01216 | 23.578 sec / 10 steps\n",
      "| epoch: 19 | step: 3810 | loss: 0.53891 | grad_norm: 0.01816 | 22.317 sec / 10 steps\n",
      "| epoch: 20 | step: 3820 | loss: 0.53018 | grad_norm: 0.01643 | 23.664 sec / 10 steps\n",
      "| epoch: 20 | step: 3830 | loss: 0.57492 | grad_norm: 0.01387 | 22.149 sec / 10 steps\n",
      "| epoch: 20 | step: 3840 | loss: 0.54535 | grad_norm: 0.01201 | 23.404 sec / 10 steps\n",
      "| epoch: 20 | step: 3850 | loss: 0.51841 | grad_norm: 0.01008 | 22.118 sec / 10 steps\n",
      "| epoch: 20 | step: 3860 | loss: 0.54730 | grad_norm: 0.02678 | 20.964 sec / 10 steps\n",
      "| epoch: 20 | step: 3870 | loss: 0.54928 | grad_norm: 0.01156 | 22.052 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 20 | step: 3880 | loss: 0.51438 | grad_norm: 0.01031 | 22.398 sec / 10 steps\n",
      "| epoch: 20 | step: 3890 | loss: 0.47764 | grad_norm: 0.01018 | 22.558 sec / 10 steps\n",
      "| epoch: 20 | step: 3900 | loss: 0.51928 | grad_norm: 0.00914 | 22.974 sec / 10 steps\n",
      "| epoch: 20 | step: 3910 | loss: 0.53172 | grad_norm: 0.01366 | 22.425 sec / 10 steps\n",
      "| epoch: 20 | step: 3920 | loss: 0.55355 | grad_norm: 0.01669 | 21.195 sec / 10 steps\n",
      "| epoch: 20 | step: 3930 | loss: 0.52487 | grad_norm: 0.01051 | 23.140 sec / 10 steps\n",
      "| epoch: 20 | step: 3940 | loss: 0.52032 | grad_norm: 0.00948 | 23.351 sec / 10 steps\n",
      "| epoch: 20 | step: 3950 | loss: 0.51706 | grad_norm: 0.01832 | 23.490 sec / 10 steps\n",
      "| epoch: 20 | step: 3960 | loss: 0.49939 | grad_norm: 0.01057 | 22.852 sec / 10 steps\n",
      "| epoch: 20 | step: 3970 | loss: 0.52859 | grad_norm: 0.01370 | 22.538 sec / 10 steps\n",
      "| epoch: 20 | step: 3980 | loss: 0.55959 | grad_norm: 0.01401 | 22.694 sec / 10 steps\n",
      "| epoch: 20 | step: 3990 | loss: 0.57496 | grad_norm: 0.01824 | 24.086 sec / 10 steps\n",
      "| epoch: 20 | step: 4000 | loss: 0.54353 | grad_norm: 0.00888 | 22.721 sec / 10 steps\n",
      "| epoch: 20 | step: 4010 | loss: 0.52592 | grad_norm: 0.01135 | 22.611 sec / 10 steps\n",
      "| epoch: 20 | step: 4020 | loss: 0.55977 | grad_norm: 0.01160 | 22.292 sec / 10 steps\n",
      "| epoch: 21 | step: 4030 | loss: 0.54281 | grad_norm: 0.01562 | 21.735 sec / 10 steps\n",
      "| epoch: 21 | step: 4040 | loss: 0.52752 | grad_norm: 0.01173 | 22.857 sec / 10 steps\n",
      "| epoch: 21 | step: 4050 | loss: 0.57523 | grad_norm: 0.01847 | 21.637 sec / 10 steps\n",
      "| epoch: 21 | step: 4060 | loss: 0.51366 | grad_norm: 0.00803 | 23.103 sec / 10 steps\n",
      "| epoch: 21 | step: 4070 | loss: 0.51966 | grad_norm: 0.01176 | 21.150 sec / 10 steps\n",
      "| epoch: 21 | step: 4080 | loss: 0.52996 | grad_norm: 0.00903 | 20.146 sec / 10 steps\n",
      "| epoch: 21 | step: 4090 | loss: 0.52560 | grad_norm: 0.00883 | 23.552 sec / 10 steps\n",
      "| epoch: 21 | step: 4100 | loss: 0.55981 | grad_norm: 0.00856 | 22.905 sec / 10 steps\n",
      "| epoch: 21 | step: 4110 | loss: 0.56077 | grad_norm: 0.01190 | 21.364 sec / 10 steps\n",
      "| epoch: 21 | step: 4120 | loss: 0.49434 | grad_norm: 0.01028 | 22.267 sec / 10 steps\n",
      "| epoch: 21 | step: 4130 | loss: 0.56132 | grad_norm: 0.00715 | 23.327 sec / 10 steps\n",
      "| epoch: 21 | step: 4140 | loss: 0.54566 | grad_norm: 0.00940 | 22.546 sec / 10 steps\n",
      "| epoch: 21 | step: 4150 | loss: 0.54494 | grad_norm: 0.01818 | 22.051 sec / 10 steps\n",
      "| epoch: 21 | step: 4160 | loss: 0.52737 | grad_norm: 0.01085 | 21.670 sec / 10 steps\n",
      "| epoch: 21 | step: 4170 | loss: 0.54943 | grad_norm: 0.00971 | 23.402 sec / 10 steps\n",
      "| epoch: 21 | step: 4180 | loss: 0.50951 | grad_norm: 0.01132 | 23.777 sec / 10 steps\n",
      "| epoch: 21 | step: 4190 | loss: 0.55119 | grad_norm: 0.01161 | 22.230 sec / 10 steps\n",
      "| epoch: 21 | step: 4200 | loss: 0.55433 | grad_norm: 0.01982 | 23.746 sec / 10 steps\n",
      "| epoch: 21 | step: 4210 | loss: 0.55378 | grad_norm: 0.00788 | 24.255 sec / 10 steps\n",
      "| epoch: 21 | step: 4220 | loss: 0.54004 | grad_norm: 0.01425 | 22.291 sec / 10 steps\n",
      "| epoch: 22 | step: 4230 | loss: 0.53380 | grad_norm: 0.00859 | 21.957 sec / 10 steps\n",
      "| epoch: 22 | step: 4240 | loss: 0.53636 | grad_norm: 0.01187 | 22.444 sec / 10 steps\n",
      "| epoch: 22 | step: 4250 | loss: 0.51199 | grad_norm: 0.01627 | 25.221 sec / 10 steps\n",
      "| epoch: 22 | step: 4260 | loss: 0.53915 | grad_norm: 0.02589 | 21.969 sec / 10 steps\n",
      "| epoch: 22 | step: 4270 | loss: 0.51367 | grad_norm: 0.00944 | 21.566 sec / 10 steps\n",
      "| epoch: 22 | step: 4280 | loss: 0.51337 | grad_norm: 0.01019 | 22.705 sec / 10 steps\n",
      "| epoch: 22 | step: 4290 | loss: 0.53085 | grad_norm: 0.01070 | 22.926 sec / 10 steps\n",
      "| epoch: 22 | step: 4300 | loss: 0.52393 | grad_norm: 0.00808 | 23.929 sec / 10 steps\n",
      "| epoch: 22 | step: 4310 | loss: 0.51839 | grad_norm: 0.01186 | 23.310 sec / 10 steps\n",
      "| epoch: 22 | step: 4320 | loss: 0.49967 | grad_norm: 0.01004 | 23.591 sec / 10 steps\n",
      "| epoch: 22 | step: 4330 | loss: 0.52004 | grad_norm: 0.01020 | 22.244 sec / 10 steps\n",
      "| epoch: 22 | step: 4340 | loss: 0.53785 | grad_norm: 0.01179 | 22.630 sec / 10 steps\n",
      "| epoch: 22 | step: 4350 | loss: 0.52896 | grad_norm: 0.01838 | 24.437 sec / 10 steps\n",
      "| epoch: 22 | step: 4360 | loss: 0.47405 | grad_norm: 0.00915 | 24.229 sec / 10 steps\n",
      "| epoch: 22 | step: 4370 | loss: 0.52557 | grad_norm: 0.02451 | 22.027 sec / 10 steps\n",
      "| epoch: 22 | step: 4380 | loss: 0.48356 | grad_norm: 0.00690 | 24.231 sec / 10 steps\n",
      "| epoch: 22 | step: 4390 | loss: 0.50540 | grad_norm: 0.01178 | 24.072 sec / 10 steps\n",
      "| epoch: 22 | step: 4400 | loss: 0.46939 | grad_norm: 0.00657 | 23.187 sec / 10 steps\n",
      "| epoch: 22 | step: 4410 | loss: 0.52092 | grad_norm: 0.00811 | 22.489 sec / 10 steps\n",
      "| epoch: 22 | step: 4420 | loss: 0.53303 | grad_norm: 0.02036 | 22.224 sec / 10 steps\n",
      "| epoch: 23 | step: 4430 | loss: 0.52068 | grad_norm: 0.00949 | 23.237 sec / 10 steps\n",
      "| epoch: 23 | step: 4440 | loss: 0.48134 | grad_norm: 0.01098 | 22.883 sec / 10 steps\n",
      "| epoch: 23 | step: 4450 | loss: 0.52426 | grad_norm: 0.01019 | 22.514 sec / 10 steps\n",
      "| epoch: 23 | step: 4460 | loss: 0.54265 | grad_norm: 0.00977 | 23.449 sec / 10 steps\n",
      "| epoch: 23 | step: 4470 | loss: 0.54818 | grad_norm: 0.01000 | 22.306 sec / 10 steps\n",
      "| epoch: 23 | step: 4480 | loss: 0.49448 | grad_norm: 0.01654 | 24.478 sec / 10 steps\n",
      "| epoch: 23 | step: 4490 | loss: 0.56006 | grad_norm: 0.00988 | 23.387 sec / 10 steps\n",
      "| epoch: 23 | step: 4500 | loss: 0.56138 | grad_norm: 0.01820 | 23.566 sec / 10 steps\n",
      "| epoch: 23 | step: 4510 | loss: 0.54366 | grad_norm: 0.00749 | 22.325 sec / 10 steps\n",
      "| epoch: 23 | step: 4520 | loss: 0.52164 | grad_norm: 0.01025 | 23.624 sec / 10 steps\n",
      "| epoch: 23 | step: 4530 | loss: 0.51084 | grad_norm: 0.01481 | 23.803 sec / 10 steps\n",
      "| epoch: 23 | step: 4540 | loss: 0.46502 | grad_norm: 0.01041 | 25.076 sec / 10 steps\n",
      "| epoch: 23 | step: 4550 | loss: 0.54751 | grad_norm: 0.01810 | 20.772 sec / 10 steps\n",
      "| epoch: 23 | step: 4560 | loss: 0.54241 | grad_norm: 0.00973 | 23.150 sec / 10 steps\n",
      "| epoch: 23 | step: 4570 | loss: 0.51452 | grad_norm: 0.00991 | 22.927 sec / 10 steps\n",
      "| epoch: 23 | step: 4580 | loss: 0.54429 | grad_norm: 0.02152 | 23.798 sec / 10 steps\n",
      "| epoch: 23 | step: 4590 | loss: 0.53477 | grad_norm: 0.02806 | 23.024 sec / 10 steps\n",
      "| epoch: 23 | step: 4600 | loss: 0.53831 | grad_norm: 0.01029 | 21.963 sec / 10 steps\n",
      "| epoch: 23 | step: 4610 | loss: 0.53524 | grad_norm: 0.00931 | 21.778 sec / 10 steps\n",
      "| epoch: 23 | step: 4620 | loss: 0.50532 | grad_norm: 0.00840 | 23.183 sec / 10 steps\n",
      "| epoch: 24 | step: 4630 | loss: 0.49252 | grad_norm: 0.00617 | 24.785 sec / 10 steps\n",
      "| epoch: 24 | step: 4640 | loss: 0.52309 | grad_norm: 0.00620 | 23.129 sec / 10 steps\n",
      "| epoch: 24 | step: 4650 | loss: 0.53060 | grad_norm: 0.00672 | 23.527 sec / 10 steps\n",
      "| epoch: 24 | step: 4660 | loss: 0.54168 | grad_norm: 0.01759 | 21.907 sec / 10 steps\n",
      "| epoch: 24 | step: 4670 | loss: 0.54913 | grad_norm: 0.01867 | 23.135 sec / 10 steps\n",
      "| epoch: 24 | step: 4680 | loss: 0.53459 | grad_norm: 0.01618 | 22.514 sec / 10 steps\n",
      "| epoch: 24 | step: 4690 | loss: 0.54537 | grad_norm: 0.01032 | 22.685 sec / 10 steps\n",
      "| epoch: 24 | step: 4700 | loss: 0.53323 | grad_norm: 0.01061 | 21.507 sec / 10 steps\n",
      "| epoch: 24 | step: 4710 | loss: 0.55074 | grad_norm: 0.01409 | 25.426 sec / 10 steps\n",
      "| epoch: 24 | step: 4720 | loss: 0.52902 | grad_norm: 0.01486 | 23.045 sec / 10 steps\n",
      "| epoch: 24 | step: 4730 | loss: 0.55457 | grad_norm: 0.00802 | 22.806 sec / 10 steps\n",
      "| epoch: 24 | step: 4740 | loss: 0.50723 | grad_norm: 0.01140 | 22.578 sec / 10 steps\n",
      "| epoch: 24 | step: 4750 | loss: 0.53348 | grad_norm: 0.00944 | 23.920 sec / 10 steps\n",
      "| epoch: 24 | step: 4760 | loss: 0.50618 | grad_norm: 0.00881 | 22.668 sec / 10 steps\n",
      "| epoch: 24 | step: 4770 | loss: 0.50041 | grad_norm: 0.01105 | 21.636 sec / 10 steps\n",
      "| epoch: 24 | step: 4780 | loss: 0.53939 | grad_norm: 0.01506 | 23.354 sec / 10 steps\n",
      "| epoch: 24 | step: 4790 | loss: 0.53742 | grad_norm: 0.00825 | 20.865 sec / 10 steps\n",
      "| epoch: 24 | step: 4800 | loss: 0.52287 | grad_norm: 0.01547 | 23.003 sec / 10 steps\n",
      "| epoch: 24 | step: 4810 | loss: 0.54619 | grad_norm: 0.01372 | 22.752 sec / 10 steps\n",
      "| epoch: 24 | step: 4820 | loss: 0.52955 | grad_norm: 0.00761 | 21.688 sec / 10 steps\n",
      "| epoch: 25 | step: 4830 | loss: 0.53047 | grad_norm: 0.01062 | 22.558 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 25 | step: 4840 | loss: 0.56195 | grad_norm: 0.01332 | 23.688 sec / 10 steps\n",
      "| epoch: 25 | step: 4850 | loss: 0.55750 | grad_norm: 0.01143 | 20.193 sec / 10 steps\n",
      "| epoch: 25 | step: 4860 | loss: 0.52369 | grad_norm: 0.00880 | 20.959 sec / 10 steps\n",
      "| epoch: 25 | step: 4870 | loss: 0.51193 | grad_norm: 0.00789 | 21.483 sec / 10 steps\n",
      "| epoch: 25 | step: 4880 | loss: 0.48941 | grad_norm: 0.00639 | 23.509 sec / 10 steps\n",
      "| epoch: 25 | step: 4890 | loss: 0.48568 | grad_norm: 0.04321 | 22.974 sec / 10 steps\n",
      "| epoch: 25 | step: 4900 | loss: 0.52817 | grad_norm: 0.01644 | 22.565 sec / 10 steps\n",
      "| epoch: 25 | step: 4910 | loss: 0.53542 | grad_norm: 0.01349 | 22.786 sec / 10 steps\n",
      "| epoch: 25 | step: 4920 | loss: 0.50799 | grad_norm: 0.00925 | 21.134 sec / 10 steps\n",
      "| epoch: 25 | step: 4930 | loss: 0.54353 | grad_norm: 0.01019 | 23.785 sec / 10 steps\n",
      "| epoch: 25 | step: 4940 | loss: 0.56830 | grad_norm: 0.01974 | 22.677 sec / 10 steps\n",
      "| epoch: 25 | step: 4950 | loss: 0.54913 | grad_norm: 0.01525 | 22.837 sec / 10 steps\n",
      "| epoch: 25 | step: 4960 | loss: 0.48771 | grad_norm: 0.00814 | 22.770 sec / 10 steps\n",
      "| epoch: 25 | step: 4970 | loss: 0.53581 | grad_norm: 0.03151 | 21.408 sec / 10 steps\n",
      "| epoch: 25 | step: 4980 | loss: 0.48808 | grad_norm: 0.00749 | 23.630 sec / 10 steps\n",
      "| epoch: 25 | step: 4990 | loss: 0.55017 | grad_norm: 0.01107 | 23.308 sec / 10 steps\n",
      "| epoch: 25 | step: 5000 | loss: 0.52351 | grad_norm: 0.00850 | 23.268 sec / 10 steps\n",
      "| epoch: 25 | step: 5010 | loss: 0.49021 | grad_norm: 0.00666 | 21.992 sec / 10 steps\n",
      "| epoch: 25 | step: 5020 | loss: 0.50805 | grad_norm: 0.00560 | 21.889 sec / 10 steps\n",
      "| epoch: 26 | step: 5030 | loss: 0.53200 | grad_norm: 0.01218 | 21.909 sec / 10 steps\n",
      "| epoch: 26 | step: 5040 | loss: 0.52190 | grad_norm: 0.01180 | 23.551 sec / 10 steps\n",
      "| epoch: 26 | step: 5050 | loss: 0.50061 | grad_norm: 0.00778 | 23.191 sec / 10 steps\n",
      "| epoch: 26 | step: 5060 | loss: 0.53662 | grad_norm: 0.00685 | 22.257 sec / 10 steps\n",
      "| epoch: 26 | step: 5070 | loss: 0.52680 | grad_norm: 0.00825 | 23.054 sec / 10 steps\n",
      "| epoch: 26 | step: 5080 | loss: 0.52184 | grad_norm: 0.01106 | 22.115 sec / 10 steps\n",
      "| epoch: 26 | step: 5090 | loss: 0.52830 | grad_norm: 0.00735 | 22.944 sec / 10 steps\n",
      "| epoch: 26 | step: 5100 | loss: 0.52208 | grad_norm: 0.01143 | 22.667 sec / 10 steps\n",
      "| epoch: 26 | step: 5110 | loss: 0.51615 | grad_norm: 0.02135 | 22.819 sec / 10 steps\n",
      "| epoch: 26 | step: 5120 | loss: 0.55985 | grad_norm: 0.01098 | 21.863 sec / 10 steps\n",
      "| epoch: 26 | step: 5130 | loss: 0.52152 | grad_norm: 0.00974 | 21.497 sec / 10 steps\n",
      "| epoch: 26 | step: 5140 | loss: 0.52144 | grad_norm: 0.00783 | 23.461 sec / 10 steps\n",
      "| epoch: 26 | step: 5150 | loss: 0.53516 | grad_norm: 0.01339 | 21.327 sec / 10 steps\n",
      "| epoch: 26 | step: 5160 | loss: 0.56232 | grad_norm: 0.00989 | 23.364 sec / 10 steps\n",
      "| epoch: 26 | step: 5170 | loss: 0.52814 | grad_norm: 0.00972 | 23.085 sec / 10 steps\n",
      "| epoch: 26 | step: 5180 | loss: 0.55655 | grad_norm: 0.01266 | 20.898 sec / 10 steps\n",
      "| epoch: 26 | step: 5190 | loss: 0.53320 | grad_norm: 0.02143 | 20.877 sec / 10 steps\n",
      "| epoch: 26 | step: 5200 | loss: 0.55559 | grad_norm: 0.01099 | 22.894 sec / 10 steps\n",
      "| epoch: 26 | step: 5210 | loss: 0.49423 | grad_norm: 0.01087 | 24.848 sec / 10 steps\n",
      "| epoch: 26 | step: 5220 | loss: 0.52117 | grad_norm: 0.01595 | 23.674 sec / 10 steps\n",
      "| epoch: 27 | step: 5230 | loss: 0.52900 | grad_norm: 0.00881 | 22.445 sec / 10 steps\n",
      "| epoch: 27 | step: 5240 | loss: 0.54269 | grad_norm: 0.01481 | 22.524 sec / 10 steps\n",
      "| epoch: 27 | step: 5250 | loss: 0.52680 | grad_norm: 0.01099 | 22.869 sec / 10 steps\n",
      "| epoch: 27 | step: 5260 | loss: 0.52632 | grad_norm: 0.01217 | 21.824 sec / 10 steps\n",
      "| epoch: 27 | step: 5270 | loss: 0.57067 | grad_norm: 0.00976 | 23.179 sec / 10 steps\n",
      "| epoch: 27 | step: 5280 | loss: 0.52624 | grad_norm: 0.01003 | 22.347 sec / 10 steps\n",
      "| epoch: 27 | step: 5290 | loss: 0.53432 | grad_norm: 0.01056 | 21.997 sec / 10 steps\n",
      "| epoch: 27 | step: 5300 | loss: 0.50035 | grad_norm: 0.01252 | 22.228 sec / 10 steps\n",
      "| epoch: 27 | step: 5310 | loss: 0.53469 | grad_norm: 0.00877 | 23.638 sec / 10 steps\n",
      "| epoch: 27 | step: 5320 | loss: 0.54197 | grad_norm: 0.01709 | 22.561 sec / 10 steps\n",
      "| epoch: 27 | step: 5330 | loss: 0.48979 | grad_norm: 0.01010 | 23.277 sec / 10 steps\n",
      "| epoch: 27 | step: 5340 | loss: 0.52026 | grad_norm: 0.00880 | 21.043 sec / 10 steps\n",
      "| epoch: 27 | step: 5350 | loss: 0.54043 | grad_norm: 0.01776 | 22.887 sec / 10 steps\n",
      "| epoch: 27 | step: 5360 | loss: 0.50916 | grad_norm: 0.01150 | 22.385 sec / 10 steps\n",
      "| epoch: 27 | step: 5370 | loss: 0.49466 | grad_norm: 0.01023 | 21.446 sec / 10 steps\n",
      "| epoch: 27 | step: 5380 | loss: 0.52113 | grad_norm: 0.01482 | 21.341 sec / 10 steps\n",
      "| epoch: 27 | step: 5390 | loss: 0.47962 | grad_norm: 0.01121 | 22.287 sec / 10 steps\n",
      "| epoch: 27 | step: 5400 | loss: 0.53826 | grad_norm: 0.00934 | 23.637 sec / 10 steps\n",
      "| epoch: 27 | step: 5410 | loss: 0.48337 | grad_norm: 0.01192 | 25.035 sec / 10 steps\n",
      "| epoch: 27 | step: 5420 | loss: 0.50960 | grad_norm: 0.00706 | 22.596 sec / 10 steps\n",
      "| epoch: 28 | step: 5430 | loss: 0.50544 | grad_norm: 0.00525 | 23.445 sec / 10 steps\n",
      "| epoch: 28 | step: 5440 | loss: 0.49682 | grad_norm: 0.00858 | 23.288 sec / 10 steps\n",
      "| epoch: 28 | step: 5450 | loss: 0.56034 | grad_norm: 0.02029 | 21.098 sec / 10 steps\n",
      "| epoch: 28 | step: 5460 | loss: 0.54865 | grad_norm: 0.01537 | 22.703 sec / 10 steps\n",
      "| epoch: 28 | step: 5470 | loss: 0.51759 | grad_norm: 0.00663 | 24.082 sec / 10 steps\n",
      "| epoch: 28 | step: 5480 | loss: 0.52095 | grad_norm: 0.00860 | 22.091 sec / 10 steps\n",
      "| epoch: 28 | step: 5490 | loss: 0.50804 | grad_norm: 0.00678 | 22.752 sec / 10 steps\n",
      "| epoch: 28 | step: 5500 | loss: 0.51940 | grad_norm: 0.00756 | 20.383 sec / 10 steps\n",
      "| epoch: 28 | step: 5510 | loss: 0.54011 | grad_norm: 0.00873 | 22.793 sec / 10 steps\n",
      "| epoch: 28 | step: 5520 | loss: 0.52949 | grad_norm: 0.01031 | 25.708 sec / 10 steps\n",
      "| epoch: 28 | step: 5530 | loss: 0.50414 | grad_norm: 0.00787 | 22.150 sec / 10 steps\n",
      "| epoch: 28 | step: 5540 | loss: 0.54618 | grad_norm: 0.01690 | 22.900 sec / 10 steps\n",
      "| epoch: 28 | step: 5550 | loss: 0.50969 | grad_norm: 0.01293 | 21.478 sec / 10 steps\n",
      "| epoch: 28 | step: 5560 | loss: 0.51630 | grad_norm: 0.00910 | 23.563 sec / 10 steps\n",
      "| epoch: 28 | step: 5570 | loss: 0.52882 | grad_norm: 0.01046 | 22.960 sec / 10 steps\n",
      "| epoch: 28 | step: 5580 | loss: 0.56029 | grad_norm: 0.00945 | 23.246 sec / 10 steps\n",
      "| epoch: 28 | step: 5590 | loss: 0.51754 | grad_norm: 0.01431 | 24.333 sec / 10 steps\n",
      "| epoch: 28 | step: 5600 | loss: 0.50986 | grad_norm: 0.01776 | 21.993 sec / 10 steps\n",
      "| epoch: 28 | step: 5610 | loss: 0.53210 | grad_norm: 0.01405 | 24.618 sec / 10 steps\n",
      "| epoch: 28 | step: 5620 | loss: 0.52507 | grad_norm: 0.01343 | 22.331 sec / 10 steps\n",
      "| epoch: 29 | step: 5630 | loss: 0.46713 | grad_norm: 0.00939 | 23.894 sec / 10 steps\n",
      "| epoch: 29 | step: 5640 | loss: 0.47633 | grad_norm: 0.00746 | 23.662 sec / 10 steps\n",
      "| epoch: 29 | step: 5650 | loss: 0.56145 | grad_norm: 0.00741 | 22.508 sec / 10 steps\n",
      "| epoch: 29 | step: 5660 | loss: 0.50183 | grad_norm: 0.01012 | 21.372 sec / 10 steps\n",
      "| epoch: 29 | step: 5670 | loss: 0.51872 | grad_norm: 0.00910 | 22.672 sec / 10 steps\n",
      "| epoch: 29 | step: 5680 | loss: 0.52032 | grad_norm: 0.00592 | 21.432 sec / 10 steps\n",
      "| epoch: 29 | step: 5690 | loss: 0.52681 | grad_norm: 0.01178 | 21.911 sec / 10 steps\n",
      "| epoch: 29 | step: 5700 | loss: 0.55416 | grad_norm: 0.00726 | 23.871 sec / 10 steps\n",
      "| epoch: 29 | step: 5710 | loss: 0.49195 | grad_norm: 0.00625 | 22.501 sec / 10 steps\n",
      "| epoch: 29 | step: 5720 | loss: 0.51397 | grad_norm: 0.00869 | 22.930 sec / 10 steps\n",
      "| epoch: 29 | step: 5730 | loss: 0.50150 | grad_norm: 0.01127 | 23.804 sec / 10 steps\n",
      "| epoch: 29 | step: 5740 | loss: 0.52945 | grad_norm: 0.00795 | 23.175 sec / 10 steps\n",
      "| epoch: 29 | step: 5750 | loss: 0.56721 | grad_norm: 0.00896 | 23.188 sec / 10 steps\n",
      "| epoch: 29 | step: 5760 | loss: 0.52432 | grad_norm: 0.00969 | 21.912 sec / 10 steps\n",
      "| epoch: 29 | step: 5770 | loss: 0.53695 | grad_norm: 0.00813 | 21.307 sec / 10 steps\n",
      "| epoch: 29 | step: 5780 | loss: 0.49430 | grad_norm: 0.00956 | 23.319 sec / 10 steps\n",
      "| epoch: 29 | step: 5790 | loss: 0.48740 | grad_norm: 0.01014 | 25.115 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 29 | step: 5800 | loss: 0.53664 | grad_norm: 0.00805 | 24.877 sec / 10 steps\n",
      "| epoch: 29 | step: 5810 | loss: 0.52553 | grad_norm: 0.01134 | 22.743 sec / 10 steps\n",
      "| epoch: 29 | step: 5820 | loss: 0.52190 | grad_norm: 0.00991 | 21.707 sec / 10 steps\n",
      "| epoch: 30 | step: 5830 | loss: 0.52290 | grad_norm: 0.01574 | 23.221 sec / 10 steps\n",
      "| epoch: 30 | step: 5840 | loss: 0.48436 | grad_norm: 0.01157 | 21.573 sec / 10 steps\n",
      "| epoch: 30 | step: 5850 | loss: 0.51676 | grad_norm: 0.01667 | 23.108 sec / 10 steps\n",
      "| epoch: 30 | step: 5860 | loss: 0.53110 | grad_norm: 0.00928 | 22.195 sec / 10 steps\n",
      "| epoch: 30 | step: 5870 | loss: 0.52114 | grad_norm: 0.00906 | 23.905 sec / 10 steps\n",
      "| epoch: 30 | step: 5880 | loss: 0.52654 | grad_norm: 0.00762 | 22.009 sec / 10 steps\n",
      "| epoch: 30 | step: 5890 | loss: 0.51737 | grad_norm: 0.01019 | 24.567 sec / 10 steps\n",
      "| epoch: 30 | step: 5900 | loss: 0.53797 | grad_norm: 0.01012 | 24.062 sec / 10 steps\n",
      "| epoch: 30 | step: 5910 | loss: 0.47709 | grad_norm: 0.01575 | 24.694 sec / 10 steps\n",
      "| epoch: 30 | step: 5920 | loss: 0.55026 | grad_norm: 0.01403 | 21.737 sec / 10 steps\n",
      "| epoch: 30 | step: 5930 | loss: 0.55746 | grad_norm: 0.02067 | 22.017 sec / 10 steps\n",
      "| epoch: 30 | step: 5940 | loss: 0.55514 | grad_norm: 0.01146 | 22.997 sec / 10 steps\n",
      "| epoch: 30 | step: 5950 | loss: 0.47405 | grad_norm: 0.01074 | 23.821 sec / 10 steps\n",
      "| epoch: 30 | step: 5960 | loss: 0.56142 | grad_norm: 0.01271 | 21.643 sec / 10 steps\n",
      "| epoch: 30 | step: 5970 | loss: 0.52817 | grad_norm: 0.00798 | 21.710 sec / 10 steps\n",
      "| epoch: 30 | step: 5980 | loss: 0.53924 | grad_norm: 0.01171 | 24.260 sec / 10 steps\n",
      "| epoch: 30 | step: 5990 | loss: 0.55643 | grad_norm: 0.01426 | 22.432 sec / 10 steps\n",
      "| epoch: 30 | step: 6000 | loss: 0.52794 | grad_norm: 0.00961 | 21.636 sec / 10 steps\n",
      "| epoch: 30 | step: 6010 | loss: 0.47238 | grad_norm: 0.00777 | 23.065 sec / 10 steps\n",
      "| epoch: 30 | step: 6020 | loss: 0.50856 | grad_norm: 0.00732 | 22.187 sec / 10 steps\n",
      "| epoch: 30 | step: 6030 | loss: 0.54825 | grad_norm: 0.00821 | 21.894 sec / 10 steps\n",
      "| epoch: 31 | step: 6040 | loss: 0.50125 | grad_norm: 0.00849 | 21.796 sec / 10 steps\n",
      "| epoch: 31 | step: 6050 | loss: 0.52908 | grad_norm: 0.00887 | 22.732 sec / 10 steps\n",
      "| epoch: 31 | step: 6060 | loss: 0.51992 | grad_norm: 0.00727 | 23.281 sec / 10 steps\n",
      "| epoch: 31 | step: 6070 | loss: 0.56743 | grad_norm: 0.01407 | 22.349 sec / 10 steps\n",
      "| epoch: 31 | step: 6080 | loss: 0.54756 | grad_norm: 0.00747 | 20.333 sec / 10 steps\n",
      "| epoch: 31 | step: 6090 | loss: 0.49082 | grad_norm: 0.01595 | 22.764 sec / 10 steps\n",
      "| epoch: 31 | step: 6100 | loss: 0.49791 | grad_norm: 0.00898 | 23.811 sec / 10 steps\n",
      "| epoch: 31 | step: 6110 | loss: 0.52805 | grad_norm: 0.00993 | 24.148 sec / 10 steps\n",
      "| epoch: 31 | step: 6120 | loss: 0.50736 | grad_norm: 0.00758 | 21.725 sec / 10 steps\n",
      "| epoch: 31 | step: 6130 | loss: 0.50553 | grad_norm: 0.00717 | 21.911 sec / 10 steps\n",
      "| epoch: 31 | step: 6140 | loss: 0.56681 | grad_norm: 0.01401 | 21.064 sec / 10 steps\n",
      "| epoch: 31 | step: 6150 | loss: 0.52651 | grad_norm: 0.00869 | 23.230 sec / 10 steps\n",
      "| epoch: 31 | step: 6160 | loss: 0.49786 | grad_norm: 0.00672 | 24.070 sec / 10 steps\n",
      "| epoch: 31 | step: 6170 | loss: 0.48481 | grad_norm: 0.00576 | 23.439 sec / 10 steps\n",
      "| epoch: 31 | step: 6180 | loss: 0.53848 | grad_norm: 0.01896 | 22.241 sec / 10 steps\n",
      "| epoch: 31 | step: 6190 | loss: 0.50039 | grad_norm: 0.00849 | 23.452 sec / 10 steps\n",
      "| epoch: 31 | step: 6200 | loss: 0.54958 | grad_norm: 0.01107 | 23.146 sec / 10 steps\n",
      "| epoch: 31 | step: 6210 | loss: 0.53056 | grad_norm: 0.01134 | 22.251 sec / 10 steps\n",
      "| epoch: 31 | step: 6220 | loss: 0.54421 | grad_norm: 0.00648 | 23.130 sec / 10 steps\n",
      "| epoch: 31 | step: 6230 | loss: 0.55623 | grad_norm: 0.01172 | 22.813 sec / 10 steps\n",
      "| epoch: 32 | step: 6240 | loss: 0.52612 | grad_norm: 0.00974 | 22.697 sec / 10 steps\n",
      "| epoch: 32 | step: 6250 | loss: 0.55721 | grad_norm: 0.01307 | 21.502 sec / 10 steps\n",
      "| epoch: 32 | step: 6260 | loss: 0.54778 | grad_norm: 0.00700 | 22.317 sec / 10 steps\n",
      "| epoch: 32 | step: 6270 | loss: 0.51065 | grad_norm: 0.00855 | 22.858 sec / 10 steps\n",
      "| epoch: 32 | step: 6280 | loss: 0.55672 | grad_norm: 0.01055 | 22.458 sec / 10 steps\n",
      "| epoch: 32 | step: 6290 | loss: 0.52391 | grad_norm: 0.04157 | 20.982 sec / 10 steps\n",
      "| epoch: 32 | step: 6300 | loss: 0.52369 | grad_norm: 0.01371 | 23.073 sec / 10 steps\n",
      "| epoch: 32 | step: 6310 | loss: 0.53076 | grad_norm: 0.00917 | 23.678 sec / 10 steps\n",
      "| epoch: 32 | step: 6320 | loss: 0.53602 | grad_norm: 0.00769 | 22.620 sec / 10 steps\n",
      "| epoch: 32 | step: 6330 | loss: 0.52272 | grad_norm: 0.01017 | 22.641 sec / 10 steps\n",
      "| epoch: 32 | step: 6340 | loss: 0.55866 | grad_norm: 0.00840 | 21.348 sec / 10 steps\n",
      "| epoch: 32 | step: 6350 | loss: 0.53840 | grad_norm: 0.01607 | 21.278 sec / 10 steps\n",
      "| epoch: 32 | step: 6360 | loss: 0.47642 | grad_norm: 0.01050 | 25.190 sec / 10 steps\n",
      "| epoch: 32 | step: 6370 | loss: 0.50985 | grad_norm: 0.01037 | 23.720 sec / 10 steps\n",
      "| epoch: 32 | step: 6380 | loss: 0.54628 | grad_norm: 0.00799 | 23.756 sec / 10 steps\n",
      "| epoch: 32 | step: 6390 | loss: 0.51887 | grad_norm: 0.01349 | 23.718 sec / 10 steps\n",
      "| epoch: 32 | step: 6400 | loss: 0.52925 | grad_norm: 0.00931 | 23.215 sec / 10 steps\n",
      "| epoch: 32 | step: 6410 | loss: 0.53578 | grad_norm: 0.00879 | 23.058 sec / 10 steps\n",
      "| epoch: 32 | step: 6420 | loss: 0.51942 | grad_norm: 0.01045 | 23.366 sec / 10 steps\n",
      "| epoch: 32 | step: 6430 | loss: 0.56356 | grad_norm: 0.01051 | 21.860 sec / 10 steps\n",
      "| epoch: 33 | step: 6440 | loss: 0.55852 | grad_norm: 0.00731 | 22.266 sec / 10 steps\n",
      "| epoch: 33 | step: 6450 | loss: 0.53166 | grad_norm: 0.00678 | 20.693 sec / 10 steps\n",
      "| epoch: 33 | step: 6460 | loss: 0.52882 | grad_norm: 0.02711 | 23.756 sec / 10 steps\n",
      "| epoch: 33 | step: 6470 | loss: 0.53894 | grad_norm: 0.00898 | 23.316 sec / 10 steps\n",
      "| epoch: 33 | step: 6480 | loss: 0.50654 | grad_norm: 0.00983 | 23.074 sec / 10 steps\n",
      "| epoch: 33 | step: 6490 | loss: 0.53391 | grad_norm: 0.01187 | 23.358 sec / 10 steps\n",
      "| epoch: 33 | step: 6500 | loss: 0.50484 | grad_norm: 0.01225 | 22.749 sec / 10 steps\n",
      "| epoch: 33 | step: 6510 | loss: 0.55288 | grad_norm: 0.00886 | 23.454 sec / 10 steps\n",
      "| epoch: 33 | step: 6520 | loss: 0.51366 | grad_norm: 0.00778 | 22.193 sec / 10 steps\n",
      "| epoch: 33 | step: 6530 | loss: 0.53498 | grad_norm: 0.01217 | 21.465 sec / 10 steps\n",
      "| epoch: 33 | step: 6540 | loss: 0.51482 | grad_norm: 0.01449 | 23.901 sec / 10 steps\n",
      "| epoch: 33 | step: 6550 | loss: 0.54481 | grad_norm: 0.00832 | 23.810 sec / 10 steps\n",
      "| epoch: 33 | step: 6560 | loss: 0.55239 | grad_norm: 0.01635 | 22.748 sec / 10 steps\n",
      "| epoch: 33 | step: 6570 | loss: 0.50367 | grad_norm: 0.01641 | 22.692 sec / 10 steps\n",
      "| epoch: 33 | step: 6580 | loss: 0.54553 | grad_norm: 0.01357 | 22.750 sec / 10 steps\n",
      "| epoch: 33 | step: 6590 | loss: 0.52450 | grad_norm: 0.01045 | 22.451 sec / 10 steps\n",
      "| epoch: 33 | step: 6600 | loss: 0.52036 | grad_norm: 0.00863 | 23.608 sec / 10 steps\n",
      "| epoch: 33 | step: 6610 | loss: 0.52377 | grad_norm: 0.00694 | 21.392 sec / 10 steps\n",
      "| epoch: 33 | step: 6620 | loss: 0.55181 | grad_norm: 0.02220 | 21.347 sec / 10 steps\n",
      "| epoch: 33 | step: 6630 | loss: 0.55091 | grad_norm: 0.01352 | 21.030 sec / 10 steps\n",
      "| epoch: 34 | step: 6640 | loss: 0.51899 | grad_norm: 0.01277 | 21.483 sec / 10 steps\n",
      "| epoch: 34 | step: 6650 | loss: 0.51239 | grad_norm: 0.00721 | 22.456 sec / 10 steps\n",
      "| epoch: 34 | step: 6660 | loss: 0.48832 | grad_norm: 0.01452 | 23.240 sec / 10 steps\n",
      "| epoch: 34 | step: 6670 | loss: 0.53116 | grad_norm: 0.00800 | 22.650 sec / 10 steps\n",
      "| epoch: 34 | step: 6680 | loss: 0.54052 | grad_norm: 0.00615 | 23.404 sec / 10 steps\n",
      "| epoch: 34 | step: 6690 | loss: 0.51889 | grad_norm: 0.00976 | 21.816 sec / 10 steps\n",
      "| epoch: 34 | step: 6700 | loss: 0.50103 | grad_norm: 0.01118 | 24.286 sec / 10 steps\n",
      "| epoch: 34 | step: 6710 | loss: 0.54131 | grad_norm: 0.00919 | 22.604 sec / 10 steps\n",
      "| epoch: 34 | step: 6720 | loss: 0.54152 | grad_norm: 0.00879 | 22.148 sec / 10 steps\n",
      "| epoch: 34 | step: 6730 | loss: 0.57012 | grad_norm: 0.00839 | 23.512 sec / 10 steps\n",
      "| epoch: 34 | step: 6740 | loss: 0.52498 | grad_norm: 0.00586 | 22.861 sec / 10 steps\n",
      "| epoch: 34 | step: 6750 | loss: 0.50395 | grad_norm: 0.01191 | 22.769 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 34 | step: 6760 | loss: 0.50389 | grad_norm: 0.01761 | 21.905 sec / 10 steps\n",
      "| epoch: 34 | step: 6770 | loss: 0.53783 | grad_norm: 0.01044 | 21.990 sec / 10 steps\n",
      "| epoch: 34 | step: 6780 | loss: 0.52445 | grad_norm: 0.01181 | 22.826 sec / 10 steps\n",
      "| epoch: 34 | step: 6790 | loss: 0.53315 | grad_norm: 0.04068 | 22.888 sec / 10 steps\n",
      "| epoch: 34 | step: 6800 | loss: 0.51843 | grad_norm: 0.01003 | 23.190 sec / 10 steps\n",
      "| epoch: 34 | step: 6810 | loss: 0.50592 | grad_norm: 0.01101 | 22.936 sec / 10 steps\n",
      "| epoch: 34 | step: 6820 | loss: 0.55361 | grad_norm: 0.01347 | 21.959 sec / 10 steps\n",
      "| epoch: 34 | step: 6830 | loss: 0.54149 | grad_norm: 0.01543 | 22.558 sec / 10 steps\n",
      "| epoch: 35 | step: 6840 | loss: 0.54720 | grad_norm: 0.01185 | 22.378 sec / 10 steps\n",
      "| epoch: 35 | step: 6850 | loss: 0.51714 | grad_norm: 0.00823 | 23.561 sec / 10 steps\n",
      "| epoch: 35 | step: 6860 | loss: 0.51192 | grad_norm: 0.00775 | 24.606 sec / 10 steps\n",
      "| epoch: 35 | step: 6870 | loss: 0.49848 | grad_norm: 0.01419 | 21.124 sec / 10 steps\n",
      "| epoch: 35 | step: 6880 | loss: 0.53345 | grad_norm: 0.01044 | 22.779 sec / 10 steps\n",
      "| epoch: 35 | step: 6890 | loss: 0.52586 | grad_norm: 0.02658 | 21.460 sec / 10 steps\n",
      "| epoch: 35 | step: 6900 | loss: 0.48456 | grad_norm: 0.01002 | 23.467 sec / 10 steps\n",
      "| epoch: 35 | step: 6910 | loss: 0.53739 | grad_norm: 0.00759 | 22.396 sec / 10 steps\n",
      "| epoch: 35 | step: 6920 | loss: 0.52850 | grad_norm: 0.00670 | 21.297 sec / 10 steps\n",
      "| epoch: 35 | step: 6930 | loss: 0.56310 | grad_norm: 0.01217 | 21.001 sec / 10 steps\n",
      "| epoch: 35 | step: 6940 | loss: 0.52287 | grad_norm: 0.00868 | 23.696 sec / 10 steps\n",
      "| epoch: 35 | step: 6950 | loss: 0.48098 | grad_norm: 0.01156 | 22.587 sec / 10 steps\n",
      "| epoch: 35 | step: 6960 | loss: 0.56516 | grad_norm: 0.01109 | 21.783 sec / 10 steps\n",
      "| epoch: 35 | step: 6970 | loss: 0.48311 | grad_norm: 0.01000 | 23.646 sec / 10 steps\n",
      "| epoch: 35 | step: 6980 | loss: 0.49003 | grad_norm: 0.01231 | 21.875 sec / 10 steps\n",
      "| epoch: 35 | step: 6990 | loss: 0.52323 | grad_norm: 0.00969 | 22.202 sec / 10 steps\n",
      "| epoch: 35 | step: 7000 | loss: 0.47163 | grad_norm: 0.00778 | 23.289 sec / 10 steps\n",
      "| epoch: 35 | step: 7010 | loss: 0.52223 | grad_norm: 0.00774 | 24.764 sec / 10 steps\n",
      "| epoch: 35 | step: 7020 | loss: 0.50664 | grad_norm: 0.00676 | 23.866 sec / 10 steps\n",
      "| epoch: 35 | step: 7030 | loss: 0.45804 | grad_norm: 0.00708 | 23.622 sec / 10 steps\n",
      "| epoch: 36 | step: 7040 | loss: 0.55793 | grad_norm: 0.01101 | 22.226 sec / 10 steps\n",
      "| epoch: 36 | step: 7050 | loss: 0.54919 | grad_norm: 0.00906 | 23.531 sec / 10 steps\n",
      "| epoch: 36 | step: 7060 | loss: 0.52604 | grad_norm: 0.00599 | 23.116 sec / 10 steps\n",
      "| epoch: 36 | step: 7070 | loss: 0.50328 | grad_norm: 0.00628 | 22.928 sec / 10 steps\n",
      "| epoch: 36 | step: 7080 | loss: 0.53726 | grad_norm: 0.00771 | 22.127 sec / 10 steps\n",
      "| epoch: 36 | step: 7090 | loss: 0.51805 | grad_norm: 0.00609 | 22.712 sec / 10 steps\n",
      "| epoch: 36 | step: 7100 | loss: 0.45696 | grad_norm: 0.00668 | 22.697 sec / 10 steps\n",
      "| epoch: 36 | step: 7110 | loss: 0.52032 | grad_norm: 0.00697 | 22.526 sec / 10 steps\n",
      "| epoch: 36 | step: 7120 | loss: 0.56625 | grad_norm: 0.00901 | 21.676 sec / 10 steps\n",
      "| epoch: 36 | step: 7130 | loss: 0.53874 | grad_norm: 0.00837 | 23.287 sec / 10 steps\n",
      "| epoch: 36 | step: 7140 | loss: 0.54637 | grad_norm: 0.01402 | 20.464 sec / 10 steps\n",
      "| epoch: 36 | step: 7150 | loss: 0.53869 | grad_norm: 0.01007 | 22.841 sec / 10 steps\n",
      "| epoch: 36 | step: 7160 | loss: 0.53966 | grad_norm: 0.01071 | 21.108 sec / 10 steps\n",
      "| epoch: 36 | step: 7170 | loss: 0.53880 | grad_norm: 0.01379 | 24.277 sec / 10 steps\n",
      "| epoch: 36 | step: 7180 | loss: 0.52241 | grad_norm: 0.01044 | 22.162 sec / 10 steps\n",
      "| epoch: 36 | step: 7190 | loss: 0.55686 | grad_norm: 0.00786 | 20.478 sec / 10 steps\n",
      "| epoch: 36 | step: 7200 | loss: 0.53426 | grad_norm: 0.01484 | 21.275 sec / 10 steps\n",
      "| epoch: 36 | step: 7210 | loss: 0.55901 | grad_norm: 0.00690 | 22.449 sec / 10 steps\n",
      "| epoch: 36 | step: 7220 | loss: 0.54969 | grad_norm: 0.01561 | 22.851 sec / 10 steps\n",
      "| epoch: 36 | step: 7230 | loss: 0.53976 | grad_norm: 0.00907 | 23.352 sec / 10 steps\n",
      "| epoch: 37 | step: 7240 | loss: 0.54268 | grad_norm: 0.00729 | 22.749 sec / 10 steps\n",
      "| epoch: 37 | step: 7250 | loss: 0.56288 | grad_norm: 0.00744 | 21.151 sec / 10 steps\n",
      "| epoch: 37 | step: 7260 | loss: 0.52202 | grad_norm: 0.01827 | 24.159 sec / 10 steps\n",
      "| epoch: 37 | step: 7270 | loss: 0.56507 | grad_norm: 0.01188 | 22.538 sec / 10 steps\n",
      "| epoch: 37 | step: 7280 | loss: 0.47929 | grad_norm: 0.00798 | 23.323 sec / 10 steps\n",
      "| epoch: 37 | step: 7290 | loss: 0.53697 | grad_norm: 0.00823 | 21.716 sec / 10 steps\n",
      "| epoch: 37 | step: 7300 | loss: 0.49905 | grad_norm: 0.00737 | 23.180 sec / 10 steps\n",
      "| epoch: 37 | step: 7310 | loss: 0.55128 | grad_norm: 0.00845 | 23.197 sec / 10 steps\n",
      "| epoch: 37 | step: 7320 | loss: 0.56146 | grad_norm: 0.01129 | 23.192 sec / 10 steps\n",
      "| epoch: 37 | step: 7330 | loss: 0.54131 | grad_norm: 0.01047 | 22.294 sec / 10 steps\n",
      "| epoch: 37 | step: 7340 | loss: 0.52377 | grad_norm: 0.01184 | 22.543 sec / 10 steps\n",
      "| epoch: 37 | step: 7350 | loss: 0.50939 | grad_norm: 0.00912 | 21.273 sec / 10 steps\n",
      "| epoch: 37 | step: 7360 | loss: 0.48419 | grad_norm: 0.00509 | 22.605 sec / 10 steps\n",
      "| epoch: 37 | step: 7370 | loss: 0.52677 | grad_norm: 0.00816 | 22.041 sec / 10 steps\n",
      "| epoch: 37 | step: 7380 | loss: 0.55086 | grad_norm: 0.00639 | 22.959 sec / 10 steps\n",
      "| epoch: 37 | step: 7390 | loss: 0.54755 | grad_norm: 0.00720 | 23.701 sec / 10 steps\n",
      "| epoch: 37 | step: 7400 | loss: 0.50168 | grad_norm: 0.00643 | 22.276 sec / 10 steps\n",
      "| epoch: 37 | step: 7410 | loss: 0.51257 | grad_norm: 0.01577 | 22.501 sec / 10 steps\n",
      "| epoch: 37 | step: 7420 | loss: 0.54514 | grad_norm: 0.01082 | 22.648 sec / 10 steps\n",
      "| epoch: 37 | step: 7430 | loss: 0.54334 | grad_norm: 0.01153 | 21.858 sec / 10 steps\n",
      "| epoch: 38 | step: 7440 | loss: 0.53912 | grad_norm: 0.00765 | 23.387 sec / 10 steps\n",
      "| epoch: 38 | step: 7450 | loss: 0.48523 | grad_norm: 0.00469 | 22.543 sec / 10 steps\n",
      "| epoch: 38 | step: 7460 | loss: 0.50718 | grad_norm: 0.01078 | 21.585 sec / 10 steps\n",
      "| epoch: 38 | step: 7470 | loss: 0.50674 | grad_norm: 0.00555 | 22.830 sec / 10 steps\n",
      "| epoch: 38 | step: 7480 | loss: 0.54228 | grad_norm: 0.01383 | 22.027 sec / 10 steps\n",
      "| epoch: 38 | step: 7490 | loss: 0.53197 | grad_norm: 0.01694 | 23.340 sec / 10 steps\n",
      "| epoch: 38 | step: 7500 | loss: 0.54677 | grad_norm: 0.01191 | 20.784 sec / 10 steps\n",
      "| epoch: 38 | step: 7510 | loss: 0.53952 | grad_norm: 0.01836 | 21.135 sec / 10 steps\n",
      "| epoch: 38 | step: 7520 | loss: 0.54271 | grad_norm: 0.01461 | 22.755 sec / 10 steps\n",
      "| epoch: 38 | step: 7530 | loss: 0.48308 | grad_norm: 0.00616 | 23.942 sec / 10 steps\n",
      "| epoch: 38 | step: 7540 | loss: 0.52223 | grad_norm: 0.00774 | 22.580 sec / 10 steps\n",
      "| epoch: 38 | step: 7550 | loss: 0.47866 | grad_norm: 0.01104 | 24.362 sec / 10 steps\n",
      "| epoch: 38 | step: 7560 | loss: 0.48952 | grad_norm: 0.00625 | 22.375 sec / 10 steps\n",
      "| epoch: 38 | step: 7570 | loss: 0.52702 | grad_norm: 0.00840 | 23.270 sec / 10 steps\n",
      "| epoch: 38 | step: 7580 | loss: 0.54698 | grad_norm: 0.00709 | 22.691 sec / 10 steps\n",
      "| epoch: 38 | step: 7590 | loss: 0.53067 | grad_norm: 0.00887 | 23.540 sec / 10 steps\n",
      "| epoch: 38 | step: 7600 | loss: 0.55420 | grad_norm: 0.01818 | 22.114 sec / 10 steps\n",
      "| epoch: 38 | step: 7610 | loss: 0.53968 | grad_norm: 0.00849 | 22.334 sec / 10 steps\n",
      "| epoch: 38 | step: 7620 | loss: 0.51318 | grad_norm: 0.01118 | 21.999 sec / 10 steps\n",
      "| epoch: 38 | step: 7630 | loss: 0.51983 | grad_norm: 0.00838 | 22.408 sec / 10 steps\n",
      "| epoch: 39 | step: 7640 | loss: 0.48715 | grad_norm: 0.00469 | 22.647 sec / 10 steps\n",
      "| epoch: 39 | step: 7650 | loss: 0.50133 | grad_norm: 0.01604 | 25.072 sec / 10 steps\n",
      "| epoch: 39 | step: 7660 | loss: 0.50440 | grad_norm: 0.00699 | 21.507 sec / 10 steps\n",
      "| epoch: 39 | step: 7670 | loss: 0.51037 | grad_norm: 0.00624 | 21.871 sec / 10 steps\n",
      "| epoch: 39 | step: 7680 | loss: 0.52423 | grad_norm: 0.00789 | 21.880 sec / 10 steps\n",
      "| epoch: 39 | step: 7690 | loss: 0.53196 | grad_norm: 0.00748 | 22.184 sec / 10 steps\n",
      "| epoch: 39 | step: 7700 | loss: 0.52957 | grad_norm: 0.01337 | 22.715 sec / 10 steps\n",
      "| epoch: 39 | step: 7710 | loss: 0.52955 | grad_norm: 0.00946 | 22.796 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 39 | step: 7720 | loss: 0.54526 | grad_norm: 0.00654 | 20.615 sec / 10 steps\n",
      "| epoch: 39 | step: 7730 | loss: 0.50727 | grad_norm: 0.00675 | 23.024 sec / 10 steps\n",
      "| epoch: 39 | step: 7740 | loss: 0.55211 | grad_norm: 0.00747 | 22.727 sec / 10 steps\n",
      "| epoch: 39 | step: 7750 | loss: 0.53446 | grad_norm: 0.00839 | 21.319 sec / 10 steps\n",
      "| epoch: 39 | step: 7760 | loss: 0.55758 | grad_norm: 0.00889 | 22.563 sec / 10 steps\n",
      "| epoch: 39 | step: 7770 | loss: 0.53756 | grad_norm: 0.01492 | 22.446 sec / 10 steps\n",
      "| epoch: 39 | step: 7780 | loss: 0.53298 | grad_norm: 0.00565 | 21.593 sec / 10 steps\n",
      "| epoch: 39 | step: 7790 | loss: 0.52643 | grad_norm: 0.02102 | 21.810 sec / 10 steps\n",
      "| epoch: 39 | step: 7800 | loss: 0.52782 | grad_norm: 0.00672 | 23.767 sec / 10 steps\n",
      "| epoch: 39 | step: 7810 | loss: 0.52069 | grad_norm: 0.00538 | 22.925 sec / 10 steps\n",
      "| epoch: 39 | step: 7820 | loss: 0.53332 | grad_norm: 0.01525 | 23.205 sec / 10 steps\n",
      "| epoch: 39 | step: 7830 | loss: 0.48401 | grad_norm: 0.00741 | 23.053 sec / 10 steps\n",
      "| epoch: 40 | step: 7840 | loss: 0.52838 | grad_norm: 0.00759 | 22.879 sec / 10 steps\n",
      "| epoch: 40 | step: 7850 | loss: 0.54104 | grad_norm: 0.00631 | 23.268 sec / 10 steps\n",
      "| epoch: 40 | step: 7860 | loss: 0.51732 | grad_norm: 0.00768 | 23.599 sec / 10 steps\n",
      "| epoch: 40 | step: 7870 | loss: 0.48800 | grad_norm: 0.00740 | 21.506 sec / 10 steps\n",
      "| epoch: 40 | step: 7880 | loss: 0.53074 | grad_norm: 0.00605 | 22.852 sec / 10 steps\n",
      "| epoch: 40 | step: 7890 | loss: 0.51747 | grad_norm: 0.00554 | 23.315 sec / 10 steps\n",
      "| epoch: 40 | step: 7900 | loss: 0.52848 | grad_norm: 0.01080 | 23.221 sec / 10 steps\n",
      "| epoch: 40 | step: 7910 | loss: 0.51506 | grad_norm: 0.00890 | 22.328 sec / 10 steps\n",
      "| epoch: 40 | step: 7920 | loss: 0.51829 | grad_norm: 0.00944 | 24.259 sec / 10 steps\n",
      "| epoch: 40 | step: 7930 | loss: 0.49771 | grad_norm: 0.00803 | 22.879 sec / 10 steps\n",
      "| epoch: 40 | step: 7940 | loss: 0.50661 | grad_norm: 0.00568 | 24.866 sec / 10 steps\n",
      "| epoch: 40 | step: 7950 | loss: 0.54366 | grad_norm: 0.01521 | 22.682 sec / 10 steps\n",
      "| epoch: 40 | step: 7960 | loss: 0.53583 | grad_norm: 0.00710 | 22.460 sec / 10 steps\n",
      "| epoch: 40 | step: 7970 | loss: 0.55617 | grad_norm: 0.00876 | 22.789 sec / 10 steps\n",
      "| epoch: 40 | step: 7980 | loss: 0.53706 | grad_norm: 0.01299 | 21.872 sec / 10 steps\n",
      "| epoch: 40 | step: 7990 | loss: 0.49314 | grad_norm: 0.00867 | 22.796 sec / 10 steps\n",
      "| epoch: 40 | step: 8000 | loss: 0.53430 | grad_norm: 0.01249 | 22.360 sec / 10 steps\n",
      "| epoch: 40 | step: 8010 | loss: 0.52747 | grad_norm: 0.00823 | 22.512 sec / 10 steps\n",
      "| epoch: 40 | step: 8020 | loss: 0.54727 | grad_norm: 0.00711 | 24.171 sec / 10 steps\n",
      "| epoch: 40 | step: 8030 | loss: 0.53534 | grad_norm: 0.00976 | 22.657 sec / 10 steps\n",
      "| epoch: 40 | step: 8040 | loss: 0.54962 | grad_norm: 0.00960 | 20.122 sec / 10 steps\n",
      "| epoch: 41 | step: 8050 | loss: 0.50913 | grad_norm: 0.00963 | 22.050 sec / 10 steps\n",
      "| epoch: 41 | step: 8060 | loss: 0.48798 | grad_norm: 0.01000 | 23.238 sec / 10 steps\n",
      "| epoch: 41 | step: 8070 | loss: 0.55992 | grad_norm: 0.01127 | 22.418 sec / 10 steps\n",
      "| epoch: 41 | step: 8080 | loss: 0.55066 | grad_norm: 0.00962 | 22.974 sec / 10 steps\n",
      "| epoch: 41 | step: 8090 | loss: 0.54002 | grad_norm: 0.00724 | 23.353 sec / 10 steps\n",
      "| epoch: 41 | step: 8100 | loss: 0.54972 | grad_norm: 0.01217 | 22.503 sec / 10 steps\n",
      "| epoch: 41 | step: 8110 | loss: 0.48978 | grad_norm: 0.00756 | 22.699 sec / 10 steps\n",
      "| epoch: 41 | step: 8120 | loss: 0.53465 | grad_norm: 0.00601 | 23.316 sec / 10 steps\n",
      "| epoch: 41 | step: 8130 | loss: 0.53106 | grad_norm: 0.00723 | 24.923 sec / 10 steps\n",
      "| epoch: 41 | step: 8140 | loss: 0.54636 | grad_norm: 0.00937 | 22.805 sec / 10 steps\n",
      "| epoch: 41 | step: 8150 | loss: 0.49799 | grad_norm: 0.01148 | 24.411 sec / 10 steps\n",
      "| epoch: 41 | step: 8160 | loss: 0.52375 | grad_norm: 0.01323 | 24.079 sec / 10 steps\n",
      "| epoch: 41 | step: 8170 | loss: 0.50901 | grad_norm: 0.00774 | 23.802 sec / 10 steps\n",
      "| epoch: 41 | step: 8180 | loss: 0.51633 | grad_norm: 0.01154 | 23.648 sec / 10 steps\n",
      "| epoch: 41 | step: 8190 | loss: 0.53706 | grad_norm: 0.00934 | 21.035 sec / 10 steps\n",
      "| epoch: 41 | step: 8200 | loss: 0.50290 | grad_norm: 0.00661 | 21.800 sec / 10 steps\n",
      "| epoch: 41 | step: 8210 | loss: 0.49747 | grad_norm: 0.00598 | 23.811 sec / 10 steps\n",
      "| epoch: 41 | step: 8220 | loss: 0.52554 | grad_norm: 0.00537 | 24.617 sec / 10 steps\n",
      "| epoch: 41 | step: 8230 | loss: 0.54674 | grad_norm: 0.01430 | 23.489 sec / 10 steps\n",
      "| epoch: 41 | step: 8240 | loss: 0.53216 | grad_norm: 0.01266 | 23.005 sec / 10 steps\n",
      "| epoch: 42 | step: 8250 | loss: 0.48378 | grad_norm: 0.00647 | 23.713 sec / 10 steps\n",
      "| epoch: 42 | step: 8260 | loss: 0.51414 | grad_norm: 0.00789 | 24.218 sec / 10 steps\n",
      "| epoch: 42 | step: 8270 | loss: 0.50105 | grad_norm: 0.00841 | 23.105 sec / 10 steps\n",
      "| epoch: 42 | step: 8280 | loss: 0.55034 | grad_norm: 0.00803 | 23.172 sec / 10 steps\n",
      "| epoch: 42 | step: 8290 | loss: 0.54450 | grad_norm: 0.01069 | 22.894 sec / 10 steps\n",
      "| epoch: 42 | step: 8300 | loss: 0.53348 | grad_norm: 0.01386 | 24.567 sec / 10 steps\n",
      "| epoch: 42 | step: 8310 | loss: 0.52389 | grad_norm: 0.01209 | 23.762 sec / 10 steps\n",
      "| epoch: 42 | step: 8320 | loss: 0.56663 | grad_norm: 0.01623 | 22.639 sec / 10 steps\n",
      "| epoch: 42 | step: 8330 | loss: 0.54287 | grad_norm: 0.01881 | 23.637 sec / 10 steps\n",
      "| epoch: 42 | step: 8340 | loss: 0.52244 | grad_norm: 0.00895 | 22.263 sec / 10 steps\n",
      "| epoch: 42 | step: 8350 | loss: 0.49736 | grad_norm: 0.00616 | 21.089 sec / 10 steps\n",
      "| epoch: 42 | step: 8360 | loss: 0.52946 | grad_norm: 0.01076 | 23.455 sec / 10 steps\n",
      "| epoch: 42 | step: 8370 | loss: 0.54605 | grad_norm: 0.01002 | 22.567 sec / 10 steps\n",
      "| epoch: 42 | step: 8380 | loss: 0.53883 | grad_norm: 0.00688 | 21.767 sec / 10 steps\n",
      "| epoch: 42 | step: 8390 | loss: 0.47776 | grad_norm: 0.00555 | 23.122 sec / 10 steps\n",
      "| epoch: 42 | step: 8400 | loss: 0.54037 | grad_norm: 0.00712 | 23.328 sec / 10 steps\n",
      "| epoch: 42 | step: 8410 | loss: 0.47299 | grad_norm: 0.00468 | 23.943 sec / 10 steps\n",
      "| epoch: 42 | step: 8420 | loss: 0.50629 | grad_norm: 0.00715 | 21.971 sec / 10 steps\n",
      "| epoch: 42 | step: 8430 | loss: 0.54979 | grad_norm: 0.00812 | 23.374 sec / 10 steps\n",
      "| epoch: 42 | step: 8440 | loss: 0.51182 | grad_norm: 0.00665 | 22.212 sec / 10 steps\n",
      "| epoch: 43 | step: 8450 | loss: 0.48314 | grad_norm: 0.00632 | 23.832 sec / 10 steps\n",
      "| epoch: 43 | step: 8460 | loss: 0.49930 | grad_norm: 0.01036 | 21.567 sec / 10 steps\n",
      "| epoch: 43 | step: 8470 | loss: 0.55696 | grad_norm: 0.00730 | 22.439 sec / 10 steps\n",
      "| epoch: 43 | step: 8480 | loss: 0.54570 | grad_norm: 0.01132 | 22.185 sec / 10 steps\n",
      "| epoch: 43 | step: 8490 | loss: 0.55490 | grad_norm: 0.02423 | 23.288 sec / 10 steps\n",
      "| epoch: 43 | step: 8500 | loss: 0.54934 | grad_norm: 0.01649 | 22.705 sec / 10 steps\n",
      "| epoch: 43 | step: 8510 | loss: 0.52560 | grad_norm: 0.01075 | 22.721 sec / 10 steps\n",
      "| epoch: 43 | step: 8520 | loss: 0.47325 | grad_norm: 0.00615 | 24.716 sec / 10 steps\n",
      "| epoch: 43 | step: 8530 | loss: 0.50373 | grad_norm: 0.00754 | 21.819 sec / 10 steps\n",
      "| epoch: 43 | step: 8540 | loss: 0.52011 | grad_norm: 0.01302 | 22.161 sec / 10 steps\n",
      "| epoch: 43 | step: 8550 | loss: 0.54322 | grad_norm: 0.00839 | 23.489 sec / 10 steps\n",
      "| epoch: 43 | step: 8560 | loss: 0.49058 | grad_norm: 0.00859 | 21.965 sec / 10 steps\n",
      "| epoch: 43 | step: 8570 | loss: 0.54147 | grad_norm: 0.00777 | 22.113 sec / 10 steps\n",
      "| epoch: 43 | step: 8580 | loss: 0.55858 | grad_norm: 0.00786 | 22.402 sec / 10 steps\n",
      "| epoch: 43 | step: 8590 | loss: 0.47254 | grad_norm: 0.00639 | 23.853 sec / 10 steps\n",
      "| epoch: 43 | step: 8600 | loss: 0.53674 | grad_norm: 0.00833 | 23.875 sec / 10 steps\n",
      "| epoch: 43 | step: 8610 | loss: 0.56391 | grad_norm: 0.00704 | 23.326 sec / 10 steps\n",
      "| epoch: 43 | step: 8620 | loss: 0.53573 | grad_norm: 0.00728 | 22.517 sec / 10 steps\n",
      "| epoch: 43 | step: 8630 | loss: 0.52057 | grad_norm: 0.00634 | 23.835 sec / 10 steps\n",
      "| epoch: 43 | step: 8640 | loss: 0.51867 | grad_norm: 0.00760 | 22.768 sec / 10 steps\n",
      "| epoch: 44 | step: 8650 | loss: 0.52224 | grad_norm: 0.01411 | 22.129 sec / 10 steps\n",
      "| epoch: 44 | step: 8660 | loss: 0.54321 | grad_norm: 0.01073 | 21.740 sec / 10 steps\n",
      "| epoch: 44 | step: 8670 | loss: 0.50015 | grad_norm: 0.01066 | 22.361 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 44 | step: 8680 | loss: 0.48831 | grad_norm: 0.00618 | 22.542 sec / 10 steps\n",
      "| epoch: 44 | step: 8690 | loss: 0.53513 | grad_norm: 0.01005 | 22.491 sec / 10 steps\n",
      "| epoch: 44 | step: 8700 | loss: 0.53265 | grad_norm: 0.01211 | 22.267 sec / 10 steps\n",
      "| epoch: 44 | step: 8710 | loss: 0.52196 | grad_norm: 0.00736 | 22.682 sec / 10 steps\n",
      "| epoch: 44 | step: 8720 | loss: 0.49571 | grad_norm: 0.00752 | 21.619 sec / 10 steps\n",
      "| epoch: 44 | step: 8730 | loss: 0.47846 | grad_norm: 0.01010 | 24.251 sec / 10 steps\n",
      "| epoch: 44 | step: 8740 | loss: 0.53399 | grad_norm: 0.01132 | 22.525 sec / 10 steps\n",
      "| epoch: 44 | step: 8750 | loss: 0.56100 | grad_norm: 0.01183 | 22.589 sec / 10 steps\n",
      "| epoch: 44 | step: 8760 | loss: 0.50377 | grad_norm: 0.00803 | 23.860 sec / 10 steps\n",
      "| epoch: 44 | step: 8770 | loss: 0.50662 | grad_norm: 0.00828 | 21.499 sec / 10 steps\n",
      "| epoch: 44 | step: 8780 | loss: 0.50683 | grad_norm: 0.01144 | 23.084 sec / 10 steps\n",
      "| epoch: 44 | step: 8790 | loss: 0.48668 | grad_norm: 0.02444 | 24.059 sec / 10 steps\n",
      "| epoch: 44 | step: 8800 | loss: 0.51848 | grad_norm: 0.01063 | 24.059 sec / 10 steps\n",
      "| epoch: 44 | step: 8810 | loss: 0.54901 | grad_norm: 0.00828 | 22.562 sec / 10 steps\n",
      "| epoch: 44 | step: 8820 | loss: 0.53360 | grad_norm: 0.00995 | 22.800 sec / 10 steps\n",
      "| epoch: 44 | step: 8830 | loss: 0.54234 | grad_norm: 0.01137 | 23.424 sec / 10 steps\n",
      "| epoch: 44 | step: 8840 | loss: 0.51348 | grad_norm: 0.00608 | 23.322 sec / 10 steps\n",
      "| epoch: 45 | step: 8850 | loss: 0.56705 | grad_norm: 0.00727 | 21.733 sec / 10 steps\n",
      "| epoch: 45 | step: 8860 | loss: 0.56611 | grad_norm: 0.00714 | 23.508 sec / 10 steps\n",
      "| epoch: 45 | step: 8870 | loss: 0.55579 | grad_norm: 0.00813 | 21.955 sec / 10 steps\n",
      "| epoch: 45 | step: 8880 | loss: 0.54592 | grad_norm: 0.00669 | 21.391 sec / 10 steps\n",
      "| epoch: 45 | step: 8890 | loss: 0.55493 | grad_norm: 0.01783 | 22.431 sec / 10 steps\n",
      "| epoch: 45 | step: 8900 | loss: 0.52129 | grad_norm: 0.00546 | 24.460 sec / 10 steps\n",
      "| epoch: 45 | step: 8910 | loss: 0.51971 | grad_norm: 0.00605 | 23.559 sec / 10 steps\n",
      "| epoch: 45 | step: 8920 | loss: 0.53402 | grad_norm: 0.00570 | 22.151 sec / 10 steps\n",
      "| epoch: 45 | step: 8930 | loss: 0.53522 | grad_norm: 0.01700 | 21.834 sec / 10 steps\n",
      "| epoch: 45 | step: 8940 | loss: 0.48880 | grad_norm: 0.00818 | 23.076 sec / 10 steps\n",
      "| epoch: 45 | step: 8950 | loss: 0.53464 | grad_norm: 0.01206 | 23.364 sec / 10 steps\n",
      "| epoch: 45 | step: 8960 | loss: 0.52252 | grad_norm: 0.01443 | 24.001 sec / 10 steps\n",
      "| epoch: 45 | step: 8970 | loss: 0.52448 | grad_norm: 0.01040 | 22.629 sec / 10 steps\n",
      "| epoch: 45 | step: 8980 | loss: 0.53965 | grad_norm: 0.01101 | 22.287 sec / 10 steps\n",
      "| epoch: 45 | step: 8990 | loss: 0.53109 | grad_norm: 0.00926 | 22.680 sec / 10 steps\n",
      "| epoch: 45 | step: 9000 | loss: 0.54459 | grad_norm: 0.01131 | 26.827 sec / 10 steps\n",
      "| epoch: 45 | step: 9010 | loss: 0.49615 | grad_norm: 0.00520 | 25.154 sec / 10 steps\n",
      "| epoch: 45 | step: 9020 | loss: 0.48548 | grad_norm: 0.00630 | 22.423 sec / 10 steps\n",
      "| epoch: 45 | step: 9030 | loss: 0.48635 | grad_norm: 0.00914 | 23.013 sec / 10 steps\n",
      "| epoch: 45 | step: 9040 | loss: 0.57408 | grad_norm: 0.00880 | 23.830 sec / 10 steps\n",
      "| epoch: 46 | step: 9050 | loss: 0.52203 | grad_norm: 0.00640 | 23.226 sec / 10 steps\n",
      "| epoch: 46 | step: 9060 | loss: 0.52585 | grad_norm: 0.01001 | 23.185 sec / 10 steps\n",
      "| epoch: 46 | step: 9070 | loss: 0.55080 | grad_norm: 0.00632 | 22.971 sec / 10 steps\n",
      "| epoch: 46 | step: 9080 | loss: 0.53615 | grad_norm: 0.01477 | 23.198 sec / 10 steps\n",
      "| epoch: 46 | step: 9090 | loss: 0.53913 | grad_norm: 0.00855 | 22.581 sec / 10 steps\n",
      "| epoch: 46 | step: 9100 | loss: 0.50700 | grad_norm: 0.00769 | 22.995 sec / 10 steps\n",
      "| epoch: 46 | step: 9110 | loss: 0.49306 | grad_norm: 0.00965 | 23.204 sec / 10 steps\n",
      "| epoch: 46 | step: 9120 | loss: 0.46565 | grad_norm: 0.00666 | 25.428 sec / 10 steps\n",
      "| epoch: 46 | step: 9130 | loss: 0.52984 | grad_norm: 0.01472 | 21.987 sec / 10 steps\n",
      "| epoch: 46 | step: 9140 | loss: 0.53903 | grad_norm: 0.00907 | 22.873 sec / 10 steps\n",
      "| epoch: 46 | step: 9150 | loss: 0.52315 | grad_norm: 0.00907 | 22.657 sec / 10 steps\n",
      "| epoch: 46 | step: 9160 | loss: 0.52282 | grad_norm: 0.00596 | 23.209 sec / 10 steps\n",
      "| epoch: 46 | step: 9170 | loss: 0.52526 | grad_norm: 0.00712 | 25.001 sec / 10 steps\n",
      "| epoch: 46 | step: 9180 | loss: 0.48622 | grad_norm: 0.00542 | 26.782 sec / 10 steps\n",
      "| epoch: 46 | step: 9190 | loss: 0.49543 | grad_norm: 0.00622 | 21.549 sec / 10 steps\n",
      "| epoch: 46 | step: 9200 | loss: 0.52454 | grad_norm: 0.01140 | 23.636 sec / 10 steps\n",
      "| epoch: 46 | step: 9210 | loss: 0.55818 | grad_norm: 0.00781 | 23.241 sec / 10 steps\n",
      "| epoch: 46 | step: 9220 | loss: 0.53263 | grad_norm: 0.01162 | 21.376 sec / 10 steps\n",
      "| epoch: 46 | step: 9230 | loss: 0.55406 | grad_norm: 0.02442 | 21.577 sec / 10 steps\n",
      "| epoch: 46 | step: 9240 | loss: 0.54941 | grad_norm: 0.00820 | 21.187 sec / 10 steps\n",
      "| epoch: 47 | step: 9250 | loss: 0.54619 | grad_norm: 0.01347 | 22.511 sec / 10 steps\n",
      "| epoch: 47 | step: 9260 | loss: 0.47236 | grad_norm: 0.00716 | 23.842 sec / 10 steps\n",
      "| epoch: 47 | step: 9270 | loss: 0.52243 | grad_norm: 0.00666 | 22.674 sec / 10 steps\n",
      "| epoch: 47 | step: 9280 | loss: 0.53907 | grad_norm: 0.00768 | 23.737 sec / 10 steps\n",
      "| epoch: 47 | step: 9290 | loss: 0.55295 | grad_norm: 0.00924 | 22.694 sec / 10 steps\n",
      "| epoch: 47 | step: 9300 | loss: 0.48301 | grad_norm: 0.00686 | 22.466 sec / 10 steps\n",
      "| epoch: 47 | step: 9310 | loss: 0.51861 | grad_norm: 0.00764 | 22.141 sec / 10 steps\n",
      "| epoch: 47 | step: 9320 | loss: 0.53133 | grad_norm: 0.00990 | 22.949 sec / 10 steps\n",
      "| epoch: 47 | step: 9330 | loss: 0.53140 | grad_norm: 0.00962 | 22.023 sec / 10 steps\n",
      "| epoch: 47 | step: 9340 | loss: 0.52706 | grad_norm: 0.00798 | 21.585 sec / 10 steps\n",
      "| epoch: 47 | step: 9350 | loss: 0.50423 | grad_norm: 0.00635 | 23.268 sec / 10 steps\n",
      "| epoch: 47 | step: 9360 | loss: 0.52609 | grad_norm: 0.00617 | 21.781 sec / 10 steps\n",
      "| epoch: 47 | step: 9370 | loss: 0.52304 | grad_norm: 0.00935 | 24.154 sec / 10 steps\n",
      "| epoch: 47 | step: 9380 | loss: 0.54640 | grad_norm: 0.00703 | 23.536 sec / 10 steps\n",
      "| epoch: 47 | step: 9390 | loss: 0.55348 | grad_norm: 0.01099 | 24.045 sec / 10 steps\n",
      "| epoch: 47 | step: 9400 | loss: 0.50531 | grad_norm: 0.00581 | 22.443 sec / 10 steps\n",
      "| epoch: 47 | step: 9410 | loss: 0.46972 | grad_norm: 0.00735 | 22.788 sec / 10 steps\n",
      "| epoch: 47 | step: 9420 | loss: 0.54629 | grad_norm: 0.00979 | 22.954 sec / 10 steps\n",
      "| epoch: 47 | step: 9430 | loss: 0.52791 | grad_norm: 0.00660 | 23.151 sec / 10 steps\n",
      "| epoch: 47 | step: 9440 | loss: 0.54916 | grad_norm: 0.00725 | 23.278 sec / 10 steps\n",
      "| epoch: 48 | step: 9450 | loss: 0.52027 | grad_norm: 0.00654 | 21.100 sec / 10 steps\n",
      "| epoch: 48 | step: 9460 | loss: 0.52094 | grad_norm: 0.00831 | 23.100 sec / 10 steps\n",
      "| epoch: 48 | step: 9470 | loss: 0.46653 | grad_norm: 0.00763 | 24.079 sec / 10 steps\n",
      "| epoch: 48 | step: 9480 | loss: 0.52219 | grad_norm: 0.00697 | 23.525 sec / 10 steps\n",
      "| epoch: 48 | step: 9490 | loss: 0.52416 | grad_norm: 0.01186 | 22.622 sec / 10 steps\n",
      "| epoch: 48 | step: 9500 | loss: 0.51328 | grad_norm: 0.00659 | 22.801 sec / 10 steps\n",
      "| epoch: 48 | step: 9510 | loss: 0.51208 | grad_norm: 0.01828 | 22.940 sec / 10 steps\n",
      "| epoch: 48 | step: 9520 | loss: 0.47179 | grad_norm: 0.00615 | 24.202 sec / 10 steps\n",
      "| epoch: 48 | step: 9530 | loss: 0.52755 | grad_norm: 0.00889 | 22.680 sec / 10 steps\n",
      "| epoch: 48 | step: 9540 | loss: 0.51947 | grad_norm: 0.00890 | 23.039 sec / 10 steps\n",
      "| epoch: 48 | step: 9550 | loss: 0.52386 | grad_norm: 0.00993 | 23.020 sec / 10 steps\n",
      "| epoch: 48 | step: 9560 | loss: 0.50983 | grad_norm: 0.00589 | 22.042 sec / 10 steps\n",
      "| epoch: 48 | step: 9570 | loss: 0.48599 | grad_norm: 0.00757 | 22.894 sec / 10 steps\n",
      "| epoch: 48 | step: 9580 | loss: 0.50778 | grad_norm: 0.00644 | 21.561 sec / 10 steps\n",
      "| epoch: 48 | step: 9590 | loss: 0.53433 | grad_norm: 0.00629 | 22.521 sec / 10 steps\n",
      "| epoch: 48 | step: 9600 | loss: 0.54603 | grad_norm: 0.00902 | 20.638 sec / 10 steps\n",
      "| epoch: 48 | step: 9610 | loss: 0.52041 | grad_norm: 0.00666 | 21.616 sec / 10 steps\n",
      "| epoch: 48 | step: 9620 | loss: 0.50631 | grad_norm: 0.00707 | 24.256 sec / 10 steps\n",
      "| epoch: 48 | step: 9630 | loss: 0.50449 | grad_norm: 0.00635 | 22.833 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 48 | step: 9640 | loss: 0.55695 | grad_norm: 0.01686 | 22.623 sec / 10 steps\n",
      "| epoch: 49 | step: 9650 | loss: 0.53402 | grad_norm: 0.02004 | 21.341 sec / 10 steps\n",
      "| epoch: 49 | step: 9660 | loss: 0.54497 | grad_norm: 0.01021 | 21.329 sec / 10 steps\n",
      "| epoch: 49 | step: 9670 | loss: 0.52710 | grad_norm: 0.00737 | 22.805 sec / 10 steps\n",
      "| epoch: 49 | step: 9680 | loss: 0.50448 | grad_norm: 0.00707 | 22.671 sec / 10 steps\n",
      "| epoch: 49 | step: 9690 | loss: 0.53663 | grad_norm: 0.00784 | 20.754 sec / 10 steps\n",
      "| epoch: 49 | step: 9700 | loss: 0.49623 | grad_norm: 0.01164 | 22.800 sec / 10 steps\n",
      "| epoch: 49 | step: 9710 | loss: 0.56086 | grad_norm: 0.00898 | 21.892 sec / 10 steps\n",
      "| epoch: 49 | step: 9720 | loss: 0.51194 | grad_norm: 0.00791 | 21.871 sec / 10 steps\n",
      "| epoch: 49 | step: 9730 | loss: 0.53719 | grad_norm: 0.01338 | 22.655 sec / 10 steps\n",
      "| epoch: 49 | step: 9740 | loss: 0.54031 | grad_norm: 0.00609 | 22.740 sec / 10 steps\n",
      "| epoch: 49 | step: 9750 | loss: 0.54762 | grad_norm: 0.00680 | 21.482 sec / 10 steps\n",
      "| epoch: 49 | step: 9760 | loss: 0.55439 | grad_norm: 0.00691 | 22.113 sec / 10 steps\n",
      "| epoch: 49 | step: 9770 | loss: 0.56783 | grad_norm: 0.01263 | 20.224 sec / 10 steps\n",
      "| epoch: 49 | step: 9780 | loss: 0.50514 | grad_norm: 0.01055 | 22.114 sec / 10 steps\n",
      "| epoch: 49 | step: 9790 | loss: 0.52103 | grad_norm: 0.00982 | 22.448 sec / 10 steps\n",
      "| epoch: 49 | step: 9800 | loss: 0.53509 | grad_norm: 0.00935 | 20.788 sec / 10 steps\n",
      "| epoch: 49 | step: 9810 | loss: 0.55480 | grad_norm: 0.00499 | 23.192 sec / 10 steps\n",
      "| epoch: 49 | step: 9820 | loss: 0.49815 | grad_norm: 0.00703 | 21.294 sec / 10 steps\n",
      "| epoch: 49 | step: 9830 | loss: 0.47936 | grad_norm: 0.01017 | 22.498 sec / 10 steps\n",
      "| epoch: 49 | step: 9840 | loss: 0.54077 | grad_norm: 0.01717 | 21.964 sec / 10 steps\n",
      "| epoch: 50 | step: 9850 | loss: 0.55640 | grad_norm: 0.01756 | 21.880 sec / 10 steps\n",
      "| epoch: 50 | step: 9860 | loss: 0.51892 | grad_norm: 0.00557 | 23.637 sec / 10 steps\n",
      "| epoch: 50 | step: 9870 | loss: 0.53570 | grad_norm: 0.00685 | 22.439 sec / 10 steps\n",
      "| epoch: 50 | step: 9880 | loss: 0.50629 | grad_norm: 0.00732 | 22.069 sec / 10 steps\n",
      "| epoch: 50 | step: 9890 | loss: 0.51973 | grad_norm: 0.00798 | 22.156 sec / 10 steps\n",
      "| epoch: 50 | step: 9900 | loss: 0.52860 | grad_norm: 0.01163 | 21.431 sec / 10 steps\n",
      "| epoch: 50 | step: 9910 | loss: 0.50953 | grad_norm: 0.01035 | 24.180 sec / 10 steps\n",
      "| epoch: 50 | step: 9920 | loss: 0.54907 | grad_norm: 0.00977 | 22.054 sec / 10 steps\n",
      "| epoch: 50 | step: 9930 | loss: 0.50579 | grad_norm: 0.00594 | 21.298 sec / 10 steps\n",
      "| epoch: 50 | step: 9940 | loss: 0.49913 | grad_norm: 0.00793 | 23.706 sec / 10 steps\n",
      "| epoch: 50 | step: 9950 | loss: 0.54225 | grad_norm: 0.01611 | 22.278 sec / 10 steps\n",
      "| epoch: 50 | step: 9960 | loss: 0.54689 | grad_norm: 0.00996 | 20.013 sec / 10 steps\n",
      "| epoch: 50 | step: 9970 | loss: 0.53736 | grad_norm: 0.01195 | 22.263 sec / 10 steps\n",
      "| epoch: 50 | step: 9980 | loss: 0.50517 | grad_norm: 0.01237 | 21.987 sec / 10 steps\n",
      "| epoch: 50 | step: 9990 | loss: 0.50267 | grad_norm: 0.00981 | 22.346 sec / 10 steps\n",
      "| epoch: 50 | step: 10000 | loss: 0.54333 | grad_norm: 0.00784 | 23.056 sec / 10 steps\n",
      "| epoch: 50 | step: 10010 | loss: 0.54950 | grad_norm: 0.00758 | 23.142 sec / 10 steps\n",
      "| epoch: 50 | step: 10020 | loss: 0.49654 | grad_norm: 0.00941 | 21.984 sec / 10 steps\n",
      "| epoch: 50 | step: 10030 | loss: 0.50636 | grad_norm: 0.00866 | 22.136 sec / 10 steps\n",
      "| epoch: 50 | step: 10040 | loss: 0.55565 | grad_norm: 0.00956 | 21.288 sec / 10 steps\n",
      "| epoch: 50 | step: 10050 | loss: 0.51678 | grad_norm: 0.00490 | 21.807 sec / 10 steps\n",
      "| epoch: 51 | step: 10060 | loss: 0.54414 | grad_norm: 0.00825 | 22.143 sec / 10 steps\n",
      "| epoch: 51 | step: 10070 | loss: 0.53327 | grad_norm: 0.00734 | 22.480 sec / 10 steps\n",
      "| epoch: 51 | step: 10080 | loss: 0.52624 | grad_norm: 0.01370 | 23.384 sec / 10 steps\n",
      "| epoch: 51 | step: 10090 | loss: 0.51954 | grad_norm: 0.00980 | 22.347 sec / 10 steps\n",
      "| epoch: 51 | step: 10100 | loss: 0.54396 | grad_norm: 0.00777 | 22.461 sec / 10 steps\n",
      "| epoch: 51 | step: 10110 | loss: 0.50865 | grad_norm: 0.00683 | 21.822 sec / 10 steps\n",
      "| epoch: 51 | step: 10120 | loss: 0.53775 | grad_norm: 0.01381 | 21.959 sec / 10 steps\n",
      "| epoch: 51 | step: 10130 | loss: 0.50406 | grad_norm: 0.03381 | 24.507 sec / 10 steps\n",
      "| epoch: 51 | step: 10140 | loss: 0.46316 | grad_norm: 0.00806 | 24.067 sec / 10 steps\n",
      "| epoch: 51 | step: 10150 | loss: 0.54491 | grad_norm: 0.01093 | 21.808 sec / 10 steps\n",
      "| epoch: 51 | step: 10160 | loss: 0.50523 | grad_norm: 0.00836 | 24.476 sec / 10 steps\n",
      "| epoch: 51 | step: 10170 | loss: 0.52246 | grad_norm: 0.00747 | 22.658 sec / 10 steps\n",
      "| epoch: 51 | step: 10180 | loss: 0.52486 | grad_norm: 0.00820 | 21.851 sec / 10 steps\n",
      "| epoch: 51 | step: 10190 | loss: 0.49538 | grad_norm: 0.00697 | 21.689 sec / 10 steps\n",
      "| epoch: 51 | step: 10200 | loss: 0.52298 | grad_norm: 0.00469 | 22.363 sec / 10 steps\n",
      "| epoch: 51 | step: 10210 | loss: 0.50770 | grad_norm: 0.00596 | 23.869 sec / 10 steps\n",
      "| epoch: 51 | step: 10220 | loss: 0.48805 | grad_norm: 0.00667 | 24.300 sec / 10 steps\n",
      "| epoch: 51 | step: 10230 | loss: 0.48825 | grad_norm: 0.00811 | 23.194 sec / 10 steps\n",
      "| epoch: 51 | step: 10240 | loss: 0.54679 | grad_norm: 0.00872 | 21.802 sec / 10 steps\n",
      "| epoch: 51 | step: 10250 | loss: 0.54350 | grad_norm: 0.00951 | 21.661 sec / 10 steps\n",
      "| epoch: 52 | step: 10260 | loss: 0.49441 | grad_norm: 0.00503 | 23.689 sec / 10 steps\n",
      "| epoch: 52 | step: 10270 | loss: 0.53025 | grad_norm: 0.00874 | 22.671 sec / 10 steps\n",
      "| epoch: 52 | step: 10280 | loss: 0.54723 | grad_norm: 0.01052 | 21.704 sec / 10 steps\n",
      "| epoch: 52 | step: 10290 | loss: 0.55129 | grad_norm: 0.01090 | 22.397 sec / 10 steps\n",
      "| epoch: 52 | step: 10300 | loss: 0.49981 | grad_norm: 0.00754 | 21.895 sec / 10 steps\n",
      "| epoch: 52 | step: 10310 | loss: 0.52221 | grad_norm: 0.00743 | 23.544 sec / 10 steps\n",
      "| epoch: 52 | step: 10320 | loss: 0.53026 | grad_norm: 0.00696 | 22.768 sec / 10 steps\n",
      "| epoch: 52 | step: 10330 | loss: 0.52632 | grad_norm: 0.00644 | 23.280 sec / 10 steps\n",
      "| epoch: 52 | step: 10340 | loss: 0.48872 | grad_norm: 0.00661 | 25.015 sec / 10 steps\n",
      "| epoch: 52 | step: 10350 | loss: 0.52327 | grad_norm: 0.00644 | 21.822 sec / 10 steps\n",
      "| epoch: 52 | step: 10360 | loss: 0.55259 | grad_norm: 0.00617 | 22.681 sec / 10 steps\n",
      "| epoch: 52 | step: 10370 | loss: 0.49967 | grad_norm: 0.00865 | 21.894 sec / 10 steps\n",
      "| epoch: 52 | step: 10380 | loss: 0.51086 | grad_norm: 0.00770 | 23.146 sec / 10 steps\n",
      "| epoch: 52 | step: 10390 | loss: 0.51778 | grad_norm: 0.02434 | 22.272 sec / 10 steps\n",
      "| epoch: 52 | step: 10400 | loss: 0.52656 | grad_norm: 0.00727 | 23.614 sec / 10 steps\n",
      "| epoch: 52 | step: 10410 | loss: 0.53796 | grad_norm: 0.01890 | 23.167 sec / 10 steps\n",
      "| epoch: 52 | step: 10420 | loss: 0.54004 | grad_norm: 0.00896 | 22.078 sec / 10 steps\n",
      "| epoch: 52 | step: 10430 | loss: 0.51222 | grad_norm: 0.00780 | 22.119 sec / 10 steps\n",
      "| epoch: 52 | step: 10440 | loss: 0.47991 | grad_norm: 0.00625 | 22.537 sec / 10 steps\n",
      "| epoch: 52 | step: 10450 | loss: 0.52727 | grad_norm: 0.00975 | 22.221 sec / 10 steps\n",
      "| epoch: 53 | step: 10460 | loss: 0.53099 | grad_norm: 0.00723 | 21.540 sec / 10 steps\n",
      "| epoch: 53 | step: 10470 | loss: 0.48792 | grad_norm: 0.00585 | 22.778 sec / 10 steps\n",
      "| epoch: 53 | step: 10480 | loss: 0.53767 | grad_norm: 0.00834 | 22.715 sec / 10 steps\n",
      "| epoch: 53 | step: 10490 | loss: 0.51234 | grad_norm: 0.00921 | 22.856 sec / 10 steps\n",
      "| epoch: 53 | step: 10500 | loss: 0.53020 | grad_norm: 0.00643 | 22.437 sec / 10 steps\n",
      "| epoch: 53 | step: 10510 | loss: 0.54432 | grad_norm: 0.00782 | 22.854 sec / 10 steps\n",
      "| epoch: 53 | step: 10520 | loss: 0.51232 | grad_norm: 0.00832 | 21.774 sec / 10 steps\n",
      "| epoch: 53 | step: 10530 | loss: 0.51972 | grad_norm: 0.00653 | 21.920 sec / 10 steps\n",
      "| epoch: 53 | step: 10540 | loss: 0.51755 | grad_norm: 0.00504 | 22.721 sec / 10 steps\n",
      "| epoch: 53 | step: 10550 | loss: 0.55810 | grad_norm: 0.00723 | 21.365 sec / 10 steps\n",
      "| epoch: 53 | step: 10560 | loss: 0.49653 | grad_norm: 0.00535 | 23.212 sec / 10 steps\n",
      "| epoch: 53 | step: 10570 | loss: 0.54950 | grad_norm: 0.00842 | 20.871 sec / 10 steps\n",
      "| epoch: 53 | step: 10580 | loss: 0.53317 | grad_norm: 0.00792 | 23.510 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 53 | step: 10590 | loss: 0.53268 | grad_norm: 0.00677 | 21.246 sec / 10 steps\n",
      "| epoch: 53 | step: 10600 | loss: 0.48694 | grad_norm: 0.00803 | 23.231 sec / 10 steps\n",
      "| epoch: 53 | step: 10610 | loss: 0.54540 | grad_norm: 0.00620 | 24.258 sec / 10 steps\n",
      "| epoch: 53 | step: 10620 | loss: 0.52807 | grad_norm: 0.01014 | 21.232 sec / 10 steps\n",
      "| epoch: 53 | step: 10630 | loss: 0.54936 | grad_norm: 0.00461 | 21.534 sec / 10 steps\n",
      "| epoch: 53 | step: 10640 | loss: 0.50171 | grad_norm: 0.00798 | 22.870 sec / 10 steps\n",
      "| epoch: 53 | step: 10650 | loss: 0.53957 | grad_norm: 0.00885 | 23.117 sec / 10 steps\n",
      "| epoch: 54 | step: 10660 | loss: 0.52709 | grad_norm: 0.00772 | 22.770 sec / 10 steps\n",
      "| epoch: 54 | step: 10670 | loss: 0.51363 | grad_norm: 0.01071 | 23.020 sec / 10 steps\n",
      "| epoch: 54 | step: 10680 | loss: 0.49120 | grad_norm: 0.00813 | 23.558 sec / 10 steps\n",
      "| epoch: 54 | step: 10690 | loss: 0.50341 | grad_norm: 0.01053 | 23.222 sec / 10 steps\n",
      "| epoch: 54 | step: 10700 | loss: 0.52927 | grad_norm: 0.00834 | 22.636 sec / 10 steps\n",
      "| epoch: 54 | step: 10710 | loss: 0.56046 | grad_norm: 0.01093 | 22.793 sec / 10 steps\n",
      "| epoch: 54 | step: 10720 | loss: 0.53730 | grad_norm: 0.00902 | 22.234 sec / 10 steps\n",
      "| epoch: 54 | step: 10730 | loss: 0.52211 | grad_norm: 0.00566 | 21.707 sec / 10 steps\n",
      "| epoch: 54 | step: 10740 | loss: 0.52778 | grad_norm: 0.00544 | 22.994 sec / 10 steps\n",
      "| epoch: 54 | step: 10750 | loss: 0.57134 | grad_norm: 0.00787 | 23.499 sec / 10 steps\n",
      "| epoch: 54 | step: 10760 | loss: 0.50959 | grad_norm: 0.00511 | 21.522 sec / 10 steps\n",
      "| epoch: 54 | step: 10770 | loss: 0.52431 | grad_norm: 0.00758 | 21.757 sec / 10 steps\n",
      "| epoch: 54 | step: 10780 | loss: 0.53395 | grad_norm: 0.01166 | 21.471 sec / 10 steps\n",
      "| epoch: 54 | step: 10790 | loss: 0.52742 | grad_norm: 0.00823 | 22.447 sec / 10 steps\n",
      "| epoch: 54 | step: 10800 | loss: 0.56333 | grad_norm: 0.00887 | 22.595 sec / 10 steps\n",
      "| epoch: 54 | step: 10810 | loss: 0.49123 | grad_norm: 0.00731 | 25.199 sec / 10 steps\n",
      "| epoch: 54 | step: 10820 | loss: 0.53063 | grad_norm: 0.01117 | 23.868 sec / 10 steps\n",
      "| epoch: 54 | step: 10830 | loss: 0.50315 | grad_norm: 0.00484 | 23.650 sec / 10 steps\n",
      "| epoch: 54 | step: 10840 | loss: 0.49430 | grad_norm: 0.00720 | 23.192 sec / 10 steps\n",
      "| epoch: 54 | step: 10850 | loss: 0.50645 | grad_norm: 0.05792 | 22.281 sec / 10 steps\n",
      "| epoch: 55 | step: 10860 | loss: 0.53539 | grad_norm: 0.00871 | 22.352 sec / 10 steps\n",
      "| epoch: 55 | step: 10870 | loss: 0.54654 | grad_norm: 0.00954 | 22.045 sec / 10 steps\n",
      "| epoch: 55 | step: 10880 | loss: 0.52384 | grad_norm: 0.00947 | 23.541 sec / 10 steps\n",
      "| epoch: 55 | step: 10890 | loss: 0.51903 | grad_norm: 0.00651 | 20.747 sec / 10 steps\n",
      "| epoch: 55 | step: 10900 | loss: 0.53874 | grad_norm: 0.01008 | 23.626 sec / 10 steps\n",
      "| epoch: 55 | step: 10910 | loss: 0.53064 | grad_norm: 0.00824 | 24.587 sec / 10 steps\n",
      "| epoch: 55 | step: 10920 | loss: 0.50196 | grad_norm: 0.00883 | 21.605 sec / 10 steps\n",
      "| epoch: 55 | step: 10930 | loss: 0.50752 | grad_norm: 0.00927 | 23.109 sec / 10 steps\n",
      "| epoch: 55 | step: 10940 | loss: 0.52540 | grad_norm: 0.00666 | 21.538 sec / 10 steps\n",
      "| epoch: 55 | step: 10950 | loss: 0.52651 | grad_norm: 0.00661 | 23.444 sec / 10 steps\n",
      "| epoch: 55 | step: 10960 | loss: 0.51391 | grad_norm: 0.00459 | 25.912 sec / 10 steps\n",
      "| epoch: 55 | step: 10970 | loss: 0.54339 | grad_norm: 0.00562 | 23.609 sec / 10 steps\n",
      "| epoch: 55 | step: 10980 | loss: 0.52624 | grad_norm: 0.00614 | 22.926 sec / 10 steps\n",
      "| epoch: 55 | step: 10990 | loss: 0.53425 | grad_norm: 0.00759 | 22.935 sec / 10 steps\n",
      "| epoch: 55 | step: 11000 | loss: 0.55584 | grad_norm: 0.00882 | 23.516 sec / 10 steps\n",
      "| epoch: 55 | step: 11010 | loss: 0.52487 | grad_norm: 0.00652 | 23.738 sec / 10 steps\n",
      "| epoch: 55 | step: 11020 | loss: 0.54069 | grad_norm: 0.00639 | 22.640 sec / 10 steps\n",
      "| epoch: 55 | step: 11030 | loss: 0.48409 | grad_norm: 0.00595 | 23.201 sec / 10 steps\n",
      "| epoch: 55 | step: 11040 | loss: 0.47164 | grad_norm: 0.00626 | 22.868 sec / 10 steps\n",
      "| epoch: 55 | step: 11050 | loss: 0.48549 | grad_norm: 0.00671 | 23.085 sec / 10 steps\n",
      "| epoch: 56 | step: 11060 | loss: 0.47567 | grad_norm: 0.00755 | 23.995 sec / 10 steps\n",
      "| epoch: 56 | step: 11070 | loss: 0.52472 | grad_norm: 0.00994 | 21.798 sec / 10 steps\n",
      "| epoch: 56 | step: 11080 | loss: 0.51375 | grad_norm: 0.00722 | 22.290 sec / 10 steps\n",
      "| epoch: 56 | step: 11090 | loss: 0.52265 | grad_norm: 0.00846 | 21.955 sec / 10 steps\n",
      "| epoch: 56 | step: 11100 | loss: 0.46028 | grad_norm: 0.01152 | 22.300 sec / 10 steps\n",
      "| epoch: 56 | step: 11110 | loss: 0.51924 | grad_norm: 0.00658 | 23.299 sec / 10 steps\n",
      "| epoch: 56 | step: 11120 | loss: 0.52517 | grad_norm: 0.00668 | 22.320 sec / 10 steps\n",
      "| epoch: 56 | step: 11130 | loss: 0.49965 | grad_norm: 0.01037 | 23.424 sec / 10 steps\n",
      "| epoch: 56 | step: 11140 | loss: 0.52885 | grad_norm: 0.00578 | 23.145 sec / 10 steps\n",
      "| epoch: 56 | step: 11150 | loss: 0.53630 | grad_norm: 0.00785 | 21.323 sec / 10 steps\n",
      "| epoch: 56 | step: 11160 | loss: 0.49518 | grad_norm: 0.00500 | 22.892 sec / 10 steps\n",
      "| epoch: 56 | step: 11170 | loss: 0.54579 | grad_norm: 0.00983 | 21.800 sec / 10 steps\n",
      "| epoch: 56 | step: 11180 | loss: 0.47042 | grad_norm: 0.00631 | 23.110 sec / 10 steps\n",
      "| epoch: 56 | step: 11190 | loss: 0.56913 | grad_norm: 0.01295 | 22.094 sec / 10 steps\n",
      "| epoch: 56 | step: 11200 | loss: 0.51428 | grad_norm: 0.00439 | 22.726 sec / 10 steps\n",
      "| epoch: 56 | step: 11210 | loss: 0.52358 | grad_norm: 0.00697 | 23.024 sec / 10 steps\n",
      "| epoch: 56 | step: 11220 | loss: 0.53914 | grad_norm: 0.00582 | 24.223 sec / 10 steps\n",
      "| epoch: 56 | step: 11230 | loss: 0.52695 | grad_norm: 0.00790 | 24.054 sec / 10 steps\n",
      "| epoch: 56 | step: 11240 | loss: 0.48542 | grad_norm: 0.00549 | 22.649 sec / 10 steps\n",
      "| epoch: 56 | step: 11250 | loss: 0.50773 | grad_norm: 0.00598 | 21.774 sec / 10 steps\n",
      "| epoch: 57 | step: 11260 | loss: 0.53099 | grad_norm: 0.02312 | 22.300 sec / 10 steps\n",
      "| epoch: 57 | step: 11270 | loss: 0.52039 | grad_norm: 0.00780 | 22.426 sec / 10 steps\n",
      "| epoch: 57 | step: 11280 | loss: 0.46539 | grad_norm: 0.00671 | 23.307 sec / 10 steps\n",
      "| epoch: 57 | step: 11290 | loss: 0.51476 | grad_norm: 0.00787 | 23.300 sec / 10 steps\n",
      "| epoch: 57 | step: 11300 | loss: 0.53215 | grad_norm: 0.00889 | 22.357 sec / 10 steps\n",
      "| epoch: 57 | step: 11310 | loss: 0.53444 | grad_norm: 0.07823 | 23.337 sec / 10 steps\n",
      "| epoch: 57 | step: 11320 | loss: 0.54735 | grad_norm: 0.00982 | 21.539 sec / 10 steps\n",
      "| epoch: 57 | step: 11330 | loss: 0.50634 | grad_norm: 0.00589 | 24.628 sec / 10 steps\n",
      "| epoch: 57 | step: 11340 | loss: 0.48240 | grad_norm: 0.01246 | 24.553 sec / 10 steps\n",
      "| epoch: 57 | step: 11350 | loss: 0.51934 | grad_norm: 0.01276 | 21.754 sec / 10 steps\n",
      "| epoch: 57 | step: 11360 | loss: 0.50949 | grad_norm: 0.00734 | 20.542 sec / 10 steps\n",
      "| epoch: 57 | step: 11370 | loss: 0.51677 | grad_norm: 0.01103 | 24.690 sec / 10 steps\n",
      "| epoch: 57 | step: 11380 | loss: 0.54091 | grad_norm: 0.00627 | 25.252 sec / 10 steps\n",
      "| epoch: 57 | step: 11390 | loss: 0.50667 | grad_norm: 0.01000 | 23.670 sec / 10 steps\n",
      "| epoch: 57 | step: 11400 | loss: 0.48321 | grad_norm: 0.00685 | 23.557 sec / 10 steps\n",
      "| epoch: 57 | step: 11410 | loss: 0.50531 | grad_norm: 0.01193 | 23.448 sec / 10 steps\n",
      "| epoch: 57 | step: 11420 | loss: 0.53761 | grad_norm: 0.00678 | 23.007 sec / 10 steps\n",
      "| epoch: 57 | step: 11430 | loss: 0.47557 | grad_norm: 0.00533 | 22.595 sec / 10 steps\n",
      "| epoch: 57 | step: 11440 | loss: 0.53456 | grad_norm: 0.00704 | 22.490 sec / 10 steps\n",
      "| epoch: 57 | step: 11450 | loss: 0.55336 | grad_norm: 0.00668 | 23.741 sec / 10 steps\n",
      "| epoch: 58 | step: 11460 | loss: 0.51420 | grad_norm: 0.00702 | 23.056 sec / 10 steps\n",
      "| epoch: 58 | step: 11470 | loss: 0.48770 | grad_norm: 0.00658 | 22.990 sec / 10 steps\n",
      "| epoch: 58 | step: 11480 | loss: 0.51403 | grad_norm: 0.00530 | 21.812 sec / 10 steps\n",
      "| epoch: 58 | step: 11490 | loss: 0.51432 | grad_norm: 0.00533 | 23.502 sec / 10 steps\n",
      "| epoch: 58 | step: 11500 | loss: 0.54281 | grad_norm: 0.01023 | 22.080 sec / 10 steps\n",
      "| epoch: 58 | step: 11510 | loss: 0.53616 | grad_norm: 0.00637 | 24.024 sec / 10 steps\n",
      "| epoch: 58 | step: 11520 | loss: 0.53086 | grad_norm: 0.01025 | 22.459 sec / 10 steps\n",
      "| epoch: 58 | step: 11530 | loss: 0.53376 | grad_norm: 0.01012 | 22.646 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 58 | step: 11540 | loss: 0.50124 | grad_norm: 0.00617 | 22.763 sec / 10 steps\n",
      "| epoch: 58 | step: 11550 | loss: 0.55292 | grad_norm: 0.00742 | 23.522 sec / 10 steps\n",
      "| epoch: 58 | step: 11560 | loss: 0.52078 | grad_norm: 0.00545 | 22.455 sec / 10 steps\n",
      "| epoch: 58 | step: 11570 | loss: 0.51732 | grad_norm: 0.00552 | 22.938 sec / 10 steps\n",
      "| epoch: 58 | step: 11580 | loss: 0.55266 | grad_norm: 0.00945 | 23.773 sec / 10 steps\n",
      "| epoch: 58 | step: 11590 | loss: 0.52649 | grad_norm: 0.01191 | 23.891 sec / 10 steps\n",
      "| epoch: 58 | step: 11600 | loss: 0.55534 | grad_norm: 0.01040 | 22.519 sec / 10 steps\n",
      "| epoch: 58 | step: 11610 | loss: 0.52314 | grad_norm: 0.00815 | 24.196 sec / 10 steps\n",
      "| epoch: 58 | step: 11620 | loss: 0.51512 | grad_norm: 0.00794 | 20.853 sec / 10 steps\n",
      "| epoch: 58 | step: 11630 | loss: 0.48904 | grad_norm: 0.00673 | 22.522 sec / 10 steps\n",
      "| epoch: 58 | step: 11640 | loss: 0.53396 | grad_norm: 0.00673 | 23.255 sec / 10 steps\n",
      "| epoch: 58 | step: 11650 | loss: 0.54070 | grad_norm: 0.00945 | 21.989 sec / 10 steps\n",
      "| epoch: 59 | step: 11660 | loss: 0.53539 | grad_norm: 0.01007 | 22.772 sec / 10 steps\n",
      "| epoch: 59 | step: 11670 | loss: 0.50306 | grad_norm: 0.00619 | 22.191 sec / 10 steps\n",
      "| epoch: 59 | step: 11680 | loss: 0.53549 | grad_norm: 0.00526 | 23.766 sec / 10 steps\n",
      "| epoch: 59 | step: 11690 | loss: 0.50558 | grad_norm: 0.01027 | 23.742 sec / 10 steps\n",
      "| epoch: 59 | step: 11700 | loss: 0.52973 | grad_norm: 0.00481 | 23.423 sec / 10 steps\n",
      "| epoch: 59 | step: 11710 | loss: 0.51303 | grad_norm: 0.00623 | 24.047 sec / 10 steps\n",
      "| epoch: 59 | step: 11720 | loss: 0.53944 | grad_norm: 0.00728 | 22.108 sec / 10 steps\n",
      "| epoch: 59 | step: 11730 | loss: 0.58231 | grad_norm: 0.00772 | 21.558 sec / 10 steps\n",
      "| epoch: 59 | step: 11740 | loss: 0.52229 | grad_norm: 0.01011 | 24.965 sec / 10 steps\n",
      "| epoch: 59 | step: 11750 | loss: 0.50088 | grad_norm: 0.00699 | 24.836 sec / 10 steps\n",
      "| epoch: 59 | step: 11760 | loss: 0.51237 | grad_norm: 0.00774 | 23.097 sec / 10 steps\n",
      "| epoch: 59 | step: 11770 | loss: 0.46856 | grad_norm: 0.00446 | 23.080 sec / 10 steps\n",
      "| epoch: 59 | step: 11780 | loss: 0.53047 | grad_norm: 0.00666 | 23.191 sec / 10 steps\n",
      "| epoch: 59 | step: 11790 | loss: 0.52510 | grad_norm: 0.00999 | 22.642 sec / 10 steps\n",
      "| epoch: 59 | step: 11800 | loss: 0.55913 | grad_norm: 0.00734 | 22.638 sec / 10 steps\n",
      "| epoch: 59 | step: 11810 | loss: 0.46843 | grad_norm: 0.00784 | 24.025 sec / 10 steps\n",
      "| epoch: 59 | step: 11820 | loss: 0.53967 | grad_norm: 0.00791 | 21.862 sec / 10 steps\n",
      "| epoch: 59 | step: 11830 | loss: 0.50001 | grad_norm: 0.00785 | 23.432 sec / 10 steps\n",
      "| epoch: 59 | step: 11840 | loss: 0.52942 | grad_norm: 0.01673 | 21.728 sec / 10 steps\n",
      "| epoch: 59 | step: 11850 | loss: 0.51693 | grad_norm: 0.00790 | 23.012 sec / 10 steps\n",
      "| epoch: 60 | step: 11860 | loss: 0.54954 | grad_norm: 0.00690 | 22.120 sec / 10 steps\n",
      "| epoch: 60 | step: 11870 | loss: 0.52024 | grad_norm: 0.00676 | 22.398 sec / 10 steps\n",
      "| epoch: 60 | step: 11880 | loss: 0.53121 | grad_norm: 0.01155 | 21.503 sec / 10 steps\n",
      "| epoch: 60 | step: 11890 | loss: 0.53053 | grad_norm: 0.00701 | 23.740 sec / 10 steps\n",
      "| epoch: 60 | step: 11900 | loss: 0.51575 | grad_norm: 0.00707 | 22.561 sec / 10 steps\n",
      "| epoch: 60 | step: 11910 | loss: 0.51781 | grad_norm: 0.00912 | 22.411 sec / 10 steps\n",
      "| epoch: 60 | step: 11920 | loss: 0.51683 | grad_norm: 0.00718 | 23.459 sec / 10 steps\n",
      "| epoch: 60 | step: 11930 | loss: 0.54642 | grad_norm: 0.00663 | 23.184 sec / 10 steps\n",
      "| epoch: 60 | step: 11940 | loss: 0.47611 | grad_norm: 0.00641 | 22.510 sec / 10 steps\n",
      "| epoch: 60 | step: 11950 | loss: 0.53677 | grad_norm: 0.00531 | 22.819 sec / 10 steps\n",
      "| epoch: 60 | step: 11960 | loss: 0.48033 | grad_norm: 0.00986 | 25.441 sec / 10 steps\n",
      "| epoch: 60 | step: 11970 | loss: 0.53208 | grad_norm: 0.00849 | 22.338 sec / 10 steps\n",
      "| epoch: 60 | step: 11980 | loss: 0.52714 | grad_norm: 0.01918 | 20.766 sec / 10 steps\n",
      "| epoch: 60 | step: 11990 | loss: 0.48705 | grad_norm: 0.00495 | 23.070 sec / 10 steps\n",
      "| epoch: 60 | step: 12000 | loss: 0.49346 | grad_norm: 0.00499 | 24.084 sec / 10 steps\n",
      "| epoch: 60 | step: 12010 | loss: 0.53324 | grad_norm: 0.00735 | 22.732 sec / 10 steps\n",
      "| epoch: 60 | step: 12020 | loss: 0.50912 | grad_norm: 0.00812 | 24.139 sec / 10 steps\n",
      "| epoch: 60 | step: 12030 | loss: 0.53170 | grad_norm: 0.00627 | 22.899 sec / 10 steps\n",
      "| epoch: 60 | step: 12040 | loss: 0.51636 | grad_norm: 0.00637 | 22.773 sec / 10 steps\n",
      "| epoch: 60 | step: 12050 | loss: 0.56115 | grad_norm: 0.00636 | 24.086 sec / 10 steps\n",
      "| epoch: 60 | step: 12060 | loss: 0.53868 | grad_norm: 0.00773 | 22.590 sec / 10 steps\n",
      "| epoch: 61 | step: 12070 | loss: 0.53304 | grad_norm: 0.00809 | 22.726 sec / 10 steps\n",
      "| epoch: 61 | step: 12080 | loss: 0.51497 | grad_norm: 0.00861 | 24.459 sec / 10 steps\n",
      "| epoch: 61 | step: 12090 | loss: 0.54329 | grad_norm: 0.00902 | 20.996 sec / 10 steps\n",
      "| epoch: 61 | step: 12100 | loss: 0.51153 | grad_norm: 0.00739 | 22.463 sec / 10 steps\n",
      "| epoch: 61 | step: 12110 | loss: 0.52304 | grad_norm: 0.00830 | 24.670 sec / 10 steps\n",
      "| epoch: 61 | step: 12120 | loss: 0.55422 | grad_norm: 0.00982 | 22.252 sec / 10 steps\n",
      "| epoch: 61 | step: 12130 | loss: 0.50659 | grad_norm: 0.00574 | 22.855 sec / 10 steps\n",
      "| epoch: 61 | step: 12140 | loss: 0.52553 | grad_norm: 0.02078 | 20.667 sec / 10 steps\n",
      "| epoch: 61 | step: 12150 | loss: 0.50523 | grad_norm: 0.00673 | 21.941 sec / 10 steps\n",
      "| epoch: 61 | step: 12160 | loss: 0.52097 | grad_norm: 0.00679 | 21.588 sec / 10 steps\n",
      "| epoch: 61 | step: 12170 | loss: 0.54972 | grad_norm: 0.00821 | 22.204 sec / 10 steps\n",
      "| epoch: 61 | step: 12180 | loss: 0.53616 | grad_norm: 0.01137 | 24.286 sec / 10 steps\n",
      "| epoch: 61 | step: 12190 | loss: 0.53355 | grad_norm: 0.00918 | 23.027 sec / 10 steps\n",
      "| epoch: 61 | step: 12200 | loss: 0.54095 | grad_norm: 0.00783 | 22.753 sec / 10 steps\n",
      "| epoch: 61 | step: 12210 | loss: 0.53901 | grad_norm: 0.00900 | 24.929 sec / 10 steps\n",
      "| epoch: 61 | step: 12220 | loss: 0.53824 | grad_norm: 0.01047 | 24.256 sec / 10 steps\n",
      "| epoch: 61 | step: 12230 | loss: 0.48654 | grad_norm: 0.00509 | 24.183 sec / 10 steps\n",
      "| epoch: 61 | step: 12240 | loss: 0.50094 | grad_norm: 0.00575 | 22.798 sec / 10 steps\n",
      "| epoch: 61 | step: 12250 | loss: 0.49287 | grad_norm: 0.00468 | 22.858 sec / 10 steps\n",
      "| epoch: 61 | step: 12260 | loss: 0.50075 | grad_norm: 0.00644 | 26.020 sec / 10 steps\n",
      "| epoch: 62 | step: 12270 | loss: 0.53669 | grad_norm: 0.00615 | 23.266 sec / 10 steps\n",
      "| epoch: 62 | step: 12280 | loss: 0.54241 | grad_norm: 0.00942 | 24.768 sec / 10 steps\n",
      "| epoch: 62 | step: 12290 | loss: 0.49116 | grad_norm: 0.00679 | 23.646 sec / 10 steps\n",
      "| epoch: 62 | step: 12300 | loss: 0.51206 | grad_norm: 0.00678 | 21.758 sec / 10 steps\n",
      "| epoch: 62 | step: 12310 | loss: 0.54926 | grad_norm: 0.00754 | 23.659 sec / 10 steps\n",
      "| epoch: 62 | step: 12320 | loss: 0.53191 | grad_norm: 0.00733 | 22.628 sec / 10 steps\n",
      "| epoch: 62 | step: 12330 | loss: 0.50653 | grad_norm: 0.00482 | 24.896 sec / 10 steps\n",
      "| epoch: 62 | step: 12340 | loss: 0.52375 | grad_norm: 0.01114 | 22.569 sec / 10 steps\n",
      "| epoch: 62 | step: 12350 | loss: 0.48860 | grad_norm: 0.00524 | 22.129 sec / 10 steps\n",
      "| epoch: 62 | step: 12360 | loss: 0.51825 | grad_norm: 0.00744 | 25.848 sec / 10 steps\n",
      "| epoch: 62 | step: 12370 | loss: 0.50877 | grad_norm: 0.00514 | 22.324 sec / 10 steps\n",
      "| epoch: 62 | step: 12380 | loss: 0.51533 | grad_norm: 0.00632 | 24.597 sec / 10 steps\n",
      "| epoch: 62 | step: 12390 | loss: 0.47921 | grad_norm: 0.00644 | 26.609 sec / 10 steps\n",
      "| epoch: 62 | step: 12400 | loss: 0.53024 | grad_norm: 0.00944 | 21.757 sec / 10 steps\n",
      "| epoch: 62 | step: 12410 | loss: 0.52600 | grad_norm: 0.00816 | 23.533 sec / 10 steps\n",
      "| epoch: 62 | step: 12420 | loss: 0.54393 | grad_norm: 0.01115 | 22.435 sec / 10 steps\n",
      "| epoch: 62 | step: 12430 | loss: 0.53450 | grad_norm: 0.00612 | 23.558 sec / 10 steps\n",
      "| epoch: 62 | step: 12440 | loss: 0.54340 | grad_norm: 0.01077 | 22.706 sec / 10 steps\n",
      "| epoch: 62 | step: 12450 | loss: 0.54599 | grad_norm: 0.00902 | 24.174 sec / 10 steps\n",
      "| epoch: 62 | step: 12460 | loss: 0.46259 | grad_norm: 0.00418 | 24.496 sec / 10 steps\n",
      "| epoch: 63 | step: 12470 | loss: 0.51395 | grad_norm: 0.00781 | 22.314 sec / 10 steps\n",
      "| epoch: 63 | step: 12480 | loss: 0.49932 | grad_norm: 0.00465 | 24.119 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 63 | step: 12490 | loss: 0.49519 | grad_norm: 0.00507 | 25.887 sec / 10 steps\n",
      "| epoch: 63 | step: 12500 | loss: 0.52451 | grad_norm: 0.00865 | 23.388 sec / 10 steps\n",
      "| epoch: 63 | step: 12510 | loss: 0.55305 | grad_norm: 0.00644 | 22.947 sec / 10 steps\n",
      "| epoch: 63 | step: 12520 | loss: 0.49716 | grad_norm: 0.00757 | 23.005 sec / 10 steps\n",
      "| epoch: 63 | step: 12530 | loss: 0.53088 | grad_norm: 0.00814 | 23.423 sec / 10 steps\n",
      "| epoch: 63 | step: 12540 | loss: 0.54298 | grad_norm: 0.00923 | 24.497 sec / 10 steps\n",
      "| epoch: 63 | step: 12550 | loss: 0.48551 | grad_norm: 0.00970 | 21.525 sec / 10 steps\n",
      "| epoch: 63 | step: 12560 | loss: 0.51341 | grad_norm: 0.00557 | 22.409 sec / 10 steps\n",
      "| epoch: 63 | step: 12570 | loss: 0.52248 | grad_norm: 0.00759 | 21.502 sec / 10 steps\n",
      "| epoch: 63 | step: 12580 | loss: 0.50263 | grad_norm: 0.00461 | 22.474 sec / 10 steps\n",
      "| epoch: 63 | step: 12590 | loss: 0.56218 | grad_norm: 0.00547 | 21.212 sec / 10 steps\n",
      "| epoch: 63 | step: 12600 | loss: 0.54133 | grad_norm: 0.00956 | 21.693 sec / 10 steps\n",
      "| epoch: 63 | step: 12610 | loss: 0.53054 | grad_norm: 0.00679 | 23.534 sec / 10 steps\n",
      "| epoch: 63 | step: 12620 | loss: 0.48491 | grad_norm: 0.00476 | 24.075 sec / 10 steps\n",
      "| epoch: 63 | step: 12630 | loss: 0.56099 | grad_norm: 0.01252 | 20.278 sec / 10 steps\n",
      "| epoch: 63 | step: 12640 | loss: 0.50899 | grad_norm: 0.00494 | 23.601 sec / 10 steps\n",
      "| epoch: 63 | step: 12650 | loss: 0.58013 | grad_norm: 0.01028 | 21.944 sec / 10 steps\n",
      "| epoch: 63 | step: 12660 | loss: 0.51412 | grad_norm: 0.00531 | 21.682 sec / 10 steps\n",
      "| epoch: 64 | step: 12670 | loss: 0.55543 | grad_norm: 0.00751 | 21.487 sec / 10 steps\n",
      "| epoch: 64 | step: 12680 | loss: 0.48841 | grad_norm: 0.00568 | 23.146 sec / 10 steps\n",
      "| epoch: 64 | step: 12690 | loss: 0.49620 | grad_norm: 0.00568 | 24.425 sec / 10 steps\n",
      "| epoch: 64 | step: 12700 | loss: 0.53015 | grad_norm: 0.00701 | 22.553 sec / 10 steps\n",
      "| epoch: 64 | step: 12710 | loss: 0.54338 | grad_norm: 0.00547 | 21.930 sec / 10 steps\n",
      "| epoch: 64 | step: 12720 | loss: 0.52735 | grad_norm: 0.00891 | 21.855 sec / 10 steps\n",
      "| epoch: 64 | step: 12730 | loss: 0.55656 | grad_norm: 0.00896 | 23.756 sec / 10 steps\n",
      "| epoch: 64 | step: 12740 | loss: 0.46920 | grad_norm: 0.00806 | 23.497 sec / 10 steps\n",
      "| epoch: 64 | step: 12750 | loss: 0.52829 | grad_norm: 0.16387 | 22.573 sec / 10 steps\n",
      "| epoch: 64 | step: 12760 | loss: 0.53211 | grad_norm: 0.00836 | 20.792 sec / 10 steps\n",
      "| epoch: 64 | step: 12770 | loss: 0.55836 | grad_norm: 0.01176 | 22.448 sec / 10 steps\n",
      "| epoch: 64 | step: 12780 | loss: 0.53063 | grad_norm: 0.01447 | 22.438 sec / 10 steps\n",
      "| epoch: 64 | step: 12790 | loss: 0.51260 | grad_norm: 0.00646 | 21.457 sec / 10 steps\n",
      "| epoch: 64 | step: 12800 | loss: 0.47789 | grad_norm: 0.00758 | 22.406 sec / 10 steps\n",
      "| epoch: 64 | step: 12810 | loss: 0.51223 | grad_norm: 0.00538 | 23.947 sec / 10 steps\n",
      "| epoch: 64 | step: 12820 | loss: 0.49060 | grad_norm: 0.00602 | 22.565 sec / 10 steps\n",
      "| epoch: 64 | step: 12830 | loss: 0.47430 | grad_norm: 0.00718 | 23.261 sec / 10 steps\n",
      "| epoch: 64 | step: 12840 | loss: 0.49058 | grad_norm: 0.00798 | 22.753 sec / 10 steps\n",
      "| epoch: 64 | step: 12850 | loss: 0.54093 | grad_norm: 0.00705 | 22.277 sec / 10 steps\n",
      "| epoch: 64 | step: 12860 | loss: 0.56265 | grad_norm: 0.00890 | 22.536 sec / 10 steps\n",
      "| epoch: 65 | step: 12870 | loss: 0.53952 | grad_norm: 0.01010 | 22.819 sec / 10 steps\n",
      "| epoch: 65 | step: 12880 | loss: 0.47669 | grad_norm: 0.00436 | 23.913 sec / 10 steps\n",
      "| epoch: 65 | step: 12890 | loss: 0.49669 | grad_norm: 0.00714 | 23.015 sec / 10 steps\n",
      "| epoch: 65 | step: 12900 | loss: 0.54907 | grad_norm: 0.00629 | 22.364 sec / 10 steps\n",
      "| epoch: 65 | step: 12910 | loss: 0.53716 | grad_norm: 0.00936 | 22.798 sec / 10 steps\n",
      "| epoch: 65 | step: 12920 | loss: 0.53981 | grad_norm: 0.00762 | 22.013 sec / 10 steps\n",
      "| epoch: 65 | step: 12930 | loss: 0.52525 | grad_norm: 0.01031 | 22.444 sec / 10 steps\n",
      "| epoch: 65 | step: 12940 | loss: 0.52265 | grad_norm: 0.00700 | 26.030 sec / 10 steps\n",
      "| epoch: 65 | step: 12950 | loss: 0.51885 | grad_norm: 0.01003 | 22.297 sec / 10 steps\n",
      "| epoch: 65 | step: 12960 | loss: 0.53609 | grad_norm: 0.00601 | 23.839 sec / 10 steps\n",
      "| epoch: 65 | step: 12970 | loss: 0.48713 | grad_norm: 0.00725 | 23.117 sec / 10 steps\n",
      "| epoch: 65 | step: 12980 | loss: 0.51966 | grad_norm: 0.00566 | 21.294 sec / 10 steps\n",
      "| epoch: 65 | step: 12990 | loss: 0.51237 | grad_norm: 0.00561 | 21.798 sec / 10 steps\n",
      "| epoch: 65 | step: 13000 | loss: 0.54896 | grad_norm: 0.01028 | 21.474 sec / 10 steps\n",
      "| epoch: 65 | step: 13010 | loss: 0.56434 | grad_norm: 0.00587 | 21.603 sec / 10 steps\n",
      "| epoch: 65 | step: 13020 | loss: 0.56297 | grad_norm: 0.00919 | 22.215 sec / 10 steps\n",
      "| epoch: 65 | step: 13030 | loss: 0.49727 | grad_norm: 0.00724 | 21.903 sec / 10 steps\n",
      "| epoch: 65 | step: 13040 | loss: 0.51969 | grad_norm: 0.00478 | 21.682 sec / 10 steps\n",
      "| epoch: 65 | step: 13050 | loss: 0.51295 | grad_norm: 0.00422 | 23.204 sec / 10 steps\n",
      "| epoch: 65 | step: 13060 | loss: 0.51646 | grad_norm: 0.00639 | 21.062 sec / 10 steps\n",
      "| epoch: 66 | step: 13070 | loss: 0.55453 | grad_norm: 0.01081 | 22.660 sec / 10 steps\n",
      "| epoch: 66 | step: 13080 | loss: 0.55880 | grad_norm: 0.00610 | 20.019 sec / 10 steps\n",
      "| epoch: 66 | step: 13090 | loss: 0.54944 | grad_norm: 0.01117 | 23.102 sec / 10 steps\n",
      "| epoch: 66 | step: 13100 | loss: 0.49862 | grad_norm: 0.00848 | 23.580 sec / 10 steps\n",
      "| epoch: 66 | step: 13110 | loss: 0.52688 | grad_norm: 0.00959 | 22.762 sec / 10 steps\n",
      "| epoch: 66 | step: 13120 | loss: 0.51204 | grad_norm: 0.00859 | 21.720 sec / 10 steps\n",
      "| epoch: 66 | step: 13130 | loss: 0.53962 | grad_norm: 0.00639 | 21.425 sec / 10 steps\n",
      "| epoch: 66 | step: 13140 | loss: 0.52326 | grad_norm: 0.00597 | 22.794 sec / 10 steps\n",
      "| epoch: 66 | step: 13150 | loss: 0.55314 | grad_norm: 0.00843 | 22.046 sec / 10 steps\n",
      "| epoch: 66 | step: 13160 | loss: 0.53596 | grad_norm: 0.01308 | 23.725 sec / 10 steps\n",
      "| epoch: 66 | step: 13170 | loss: 0.52029 | grad_norm: 0.00530 | 23.494 sec / 10 steps\n",
      "| epoch: 66 | step: 13180 | loss: 0.48783 | grad_norm: 0.00614 | 23.586 sec / 10 steps\n",
      "| epoch: 66 | step: 13190 | loss: 0.51178 | grad_norm: 0.00668 | 22.578 sec / 10 steps\n",
      "| epoch: 66 | step: 13200 | loss: 0.50414 | grad_norm: 0.00651 | 23.067 sec / 10 steps\n",
      "| epoch: 66 | step: 13210 | loss: 0.50493 | grad_norm: 0.00490 | 23.544 sec / 10 steps\n",
      "| epoch: 66 | step: 13220 | loss: 0.50879 | grad_norm: 0.00554 | 22.691 sec / 10 steps\n",
      "| epoch: 66 | step: 13230 | loss: 0.53953 | grad_norm: 0.00750 | 22.920 sec / 10 steps\n",
      "| epoch: 66 | step: 13240 | loss: 0.49455 | grad_norm: 0.00625 | 22.300 sec / 10 steps\n",
      "| epoch: 66 | step: 13250 | loss: 0.52487 | grad_norm: 0.00863 | 21.940 sec / 10 steps\n",
      "| epoch: 66 | step: 13260 | loss: 0.53071 | grad_norm: 0.01126 | 23.218 sec / 10 steps\n",
      "| epoch: 67 | step: 13270 | loss: 0.50831 | grad_norm: 0.00474 | 22.324 sec / 10 steps\n",
      "| epoch: 67 | step: 13280 | loss: 0.54176 | grad_norm: 0.00728 | 22.051 sec / 10 steps\n",
      "| epoch: 67 | step: 13290 | loss: 0.52533 | grad_norm: 0.00732 | 21.797 sec / 10 steps\n",
      "| epoch: 67 | step: 13300 | loss: 0.51780 | grad_norm: 0.00555 | 22.047 sec / 10 steps\n",
      "| epoch: 67 | step: 13310 | loss: 0.53661 | grad_norm: 0.01764 | 22.174 sec / 10 steps\n",
      "| epoch: 67 | step: 13320 | loss: 0.54155 | grad_norm: 0.01050 | 22.123 sec / 10 steps\n",
      "| epoch: 67 | step: 13330 | loss: 0.52190 | grad_norm: 0.00415 | 23.310 sec / 10 steps\n",
      "| epoch: 67 | step: 13340 | loss: 0.51077 | grad_norm: 0.00561 | 22.990 sec / 10 steps\n",
      "| epoch: 67 | step: 13350 | loss: 0.56657 | grad_norm: 0.00557 | 21.870 sec / 10 steps\n",
      "| epoch: 67 | step: 13360 | loss: 0.51430 | grad_norm: 0.00647 | 22.601 sec / 10 steps\n",
      "| epoch: 67 | step: 13370 | loss: 0.55209 | grad_norm: 0.00928 | 21.993 sec / 10 steps\n",
      "| epoch: 67 | step: 13380 | loss: 0.52737 | grad_norm: 0.00858 | 22.386 sec / 10 steps\n",
      "| epoch: 67 | step: 13390 | loss: 0.48171 | grad_norm: 0.00636 | 22.256 sec / 10 steps\n",
      "| epoch: 67 | step: 13400 | loss: 0.49763 | grad_norm: 0.00684 | 21.048 sec / 10 steps\n",
      "| epoch: 67 | step: 13410 | loss: 0.50487 | grad_norm: 0.00743 | 22.660 sec / 10 steps\n",
      "| epoch: 67 | step: 13420 | loss: 0.54518 | grad_norm: 0.00535 | 24.013 sec / 10 steps\n",
      "| epoch: 67 | step: 13430 | loss: 0.47549 | grad_norm: 0.00503 | 23.838 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 67 | step: 13440 | loss: 0.48571 | grad_norm: 0.00871 | 24.731 sec / 10 steps\n",
      "| epoch: 67 | step: 13450 | loss: 0.49259 | grad_norm: 0.00543 | 22.283 sec / 10 steps\n",
      "| epoch: 67 | step: 13460 | loss: 0.52916 | grad_norm: 0.00466 | 23.664 sec / 10 steps\n",
      "| epoch: 68 | step: 13470 | loss: 0.49020 | grad_norm: 0.00603 | 22.492 sec / 10 steps\n",
      "| epoch: 68 | step: 13480 | loss: 0.52480 | grad_norm: 0.00516 | 22.796 sec / 10 steps\n",
      "| epoch: 68 | step: 13490 | loss: 0.52658 | grad_norm: 0.00465 | 23.202 sec / 10 steps\n",
      "| epoch: 68 | step: 13500 | loss: 0.52627 | grad_norm: 0.00616 | 22.122 sec / 10 steps\n",
      "| epoch: 68 | step: 13510 | loss: 0.51590 | grad_norm: 0.00566 | 21.173 sec / 10 steps\n",
      "| epoch: 68 | step: 13520 | loss: 0.49775 | grad_norm: 0.00721 | 24.057 sec / 10 steps\n",
      "| epoch: 68 | step: 13530 | loss: 0.55826 | grad_norm: 0.00524 | 21.845 sec / 10 steps\n",
      "| epoch: 68 | step: 13540 | loss: 0.55832 | grad_norm: 0.00564 | 22.464 sec / 10 steps\n",
      "| epoch: 68 | step: 13550 | loss: 0.49088 | grad_norm: 0.00418 | 22.269 sec / 10 steps\n",
      "| epoch: 68 | step: 13560 | loss: 0.53644 | grad_norm: 0.00647 | 20.614 sec / 10 steps\n",
      "| epoch: 68 | step: 13570 | loss: 0.52400 | grad_norm: 0.00535 | 21.131 sec / 10 steps\n",
      "| epoch: 68 | step: 13580 | loss: 0.51713 | grad_norm: 0.00664 | 23.551 sec / 10 steps\n",
      "| epoch: 68 | step: 13590 | loss: 0.52865 | grad_norm: 0.00631 | 22.336 sec / 10 steps\n",
      "| epoch: 68 | step: 13600 | loss: 0.51811 | grad_norm: 0.00536 | 24.285 sec / 10 steps\n",
      "| epoch: 68 | step: 13610 | loss: 0.48030 | grad_norm: 0.00379 | 23.128 sec / 10 steps\n",
      "| epoch: 68 | step: 13620 | loss: 0.53253 | grad_norm: 0.00649 | 22.697 sec / 10 steps\n",
      "| epoch: 68 | step: 13630 | loss: 0.51291 | grad_norm: 0.00855 | 21.519 sec / 10 steps\n",
      "| epoch: 68 | step: 13640 | loss: 0.57671 | grad_norm: 0.01054 | 22.154 sec / 10 steps\n",
      "| epoch: 68 | step: 13650 | loss: 0.53797 | grad_norm: 0.00579 | 23.300 sec / 10 steps\n",
      "| epoch: 68 | step: 13660 | loss: 0.50361 | grad_norm: 0.00578 | 23.677 sec / 10 steps\n",
      "| epoch: 69 | step: 13670 | loss: 0.54786 | grad_norm: 0.01016 | 22.418 sec / 10 steps\n",
      "| epoch: 69 | step: 13680 | loss: 0.50756 | grad_norm: 0.00464 | 23.058 sec / 10 steps\n",
      "| epoch: 69 | step: 13690 | loss: 0.53489 | grad_norm: 0.00535 | 22.473 sec / 10 steps\n",
      "| epoch: 69 | step: 13700 | loss: 0.53304 | grad_norm: 0.00762 | 22.892 sec / 10 steps\n",
      "| epoch: 69 | step: 13710 | loss: 0.50794 | grad_norm: 0.00590 | 23.149 sec / 10 steps\n",
      "| epoch: 69 | step: 13720 | loss: 0.53099 | grad_norm: 0.00591 | 21.599 sec / 10 steps\n",
      "| epoch: 69 | step: 13730 | loss: 0.48340 | grad_norm: 0.00723 | 23.535 sec / 10 steps\n",
      "| epoch: 69 | step: 13740 | loss: 0.55576 | grad_norm: 0.00610 | 23.617 sec / 10 steps\n",
      "| epoch: 69 | step: 13750 | loss: 0.53269 | grad_norm: 0.00705 | 22.315 sec / 10 steps\n",
      "| epoch: 69 | step: 13760 | loss: 0.49346 | grad_norm: 0.00495 | 23.592 sec / 10 steps\n",
      "| epoch: 69 | step: 13770 | loss: 0.56461 | grad_norm: 0.00606 | 23.424 sec / 10 steps\n",
      "| epoch: 69 | step: 13780 | loss: 0.53154 | grad_norm: 0.00637 | 23.937 sec / 10 steps\n",
      "| epoch: 69 | step: 13790 | loss: 0.52504 | grad_norm: 0.00617 | 22.468 sec / 10 steps\n",
      "| epoch: 69 | step: 13800 | loss: 0.52632 | grad_norm: 0.00761 | 20.881 sec / 10 steps\n",
      "| epoch: 69 | step: 13810 | loss: 0.51505 | grad_norm: 0.01150 | 21.875 sec / 10 steps\n",
      "| epoch: 69 | step: 13820 | loss: 0.52824 | grad_norm: 0.00574 | 21.697 sec / 10 steps\n",
      "| epoch: 69 | step: 13830 | loss: 0.54408 | grad_norm: 0.00894 | 21.460 sec / 10 steps\n",
      "| epoch: 69 | step: 13840 | loss: 0.50501 | grad_norm: 0.00451 | 23.393 sec / 10 steps\n",
      "| epoch: 69 | step: 13850 | loss: 0.54084 | grad_norm: 0.00712 | 21.864 sec / 10 steps\n",
      "| epoch: 69 | step: 13860 | loss: 0.51604 | grad_norm: 0.00646 | 22.958 sec / 10 steps\n",
      "| epoch: 70 | step: 13870 | loss: 0.52549 | grad_norm: 0.00671 | 21.165 sec / 10 steps\n",
      "| epoch: 70 | step: 13880 | loss: 0.51047 | grad_norm: 0.00642 | 23.176 sec / 10 steps\n",
      "| epoch: 70 | step: 13890 | loss: 0.50499 | grad_norm: 0.00632 | 23.941 sec / 10 steps\n",
      "| epoch: 70 | step: 13900 | loss: 0.50250 | grad_norm: 0.00466 | 21.307 sec / 10 steps\n",
      "| epoch: 70 | step: 13910 | loss: 0.52115 | grad_norm: 0.00612 | 22.533 sec / 10 steps\n",
      "| epoch: 70 | step: 13920 | loss: 0.52068 | grad_norm: 0.00564 | 23.063 sec / 10 steps\n",
      "| epoch: 70 | step: 13930 | loss: 0.53965 | grad_norm: 0.00594 | 21.945 sec / 10 steps\n",
      "| epoch: 70 | step: 13940 | loss: 0.52162 | grad_norm: 0.00741 | 21.613 sec / 10 steps\n",
      "| epoch: 70 | step: 13950 | loss: 0.49887 | grad_norm: 0.00767 | 21.737 sec / 10 steps\n",
      "| epoch: 70 | step: 13960 | loss: 0.50000 | grad_norm: 0.00556 | 22.010 sec / 10 steps\n",
      "| epoch: 70 | step: 13970 | loss: 0.52274 | grad_norm: 0.00412 | 23.194 sec / 10 steps\n",
      "| epoch: 70 | step: 13980 | loss: 0.53994 | grad_norm: 0.00749 | 21.753 sec / 10 steps\n",
      "| epoch: 70 | step: 13990 | loss: 0.49295 | grad_norm: 0.00477 | 22.645 sec / 10 steps\n",
      "| epoch: 70 | step: 14000 | loss: 0.54252 | grad_norm: 0.00539 | 23.870 sec / 10 steps\n",
      "| epoch: 70 | step: 14010 | loss: 0.51894 | grad_norm: 0.01794 | 22.748 sec / 10 steps\n",
      "| epoch: 70 | step: 14020 | loss: 0.51417 | grad_norm: 0.00773 | 23.961 sec / 10 steps\n",
      "| epoch: 70 | step: 14030 | loss: 0.51613 | grad_norm: 0.00550 | 21.989 sec / 10 steps\n",
      "| epoch: 70 | step: 14040 | loss: 0.54386 | grad_norm: 0.00840 | 22.456 sec / 10 steps\n",
      "| epoch: 70 | step: 14050 | loss: 0.49933 | grad_norm: 0.01178 | 24.374 sec / 10 steps\n",
      "| epoch: 70 | step: 14060 | loss: 0.50776 | grad_norm: 0.00588 | 23.137 sec / 10 steps\n",
      "| epoch: 70 | step: 14070 | loss: 0.52996 | grad_norm: 0.00512 | 24.128 sec / 10 steps\n",
      "| epoch: 71 | step: 14080 | loss: 0.51990 | grad_norm: 0.00816 | 23.734 sec / 10 steps\n",
      "| epoch: 71 | step: 14090 | loss: 0.53408 | grad_norm: 0.00446 | 21.627 sec / 10 steps\n",
      "| epoch: 71 | step: 14100 | loss: 0.52330 | grad_norm: 0.00753 | 22.707 sec / 10 steps\n",
      "| epoch: 71 | step: 14110 | loss: 0.52401 | grad_norm: 0.00827 | 22.199 sec / 10 steps\n",
      "| epoch: 71 | step: 14120 | loss: 0.48348 | grad_norm: 0.00501 | 22.652 sec / 10 steps\n",
      "| epoch: 71 | step: 14130 | loss: 0.53156 | grad_norm: 0.00564 | 22.503 sec / 10 steps\n",
      "| epoch: 71 | step: 14140 | loss: 0.53100 | grad_norm: 0.00600 | 22.669 sec / 10 steps\n",
      "| epoch: 71 | step: 14150 | loss: 0.52222 | grad_norm: 0.00650 | 21.863 sec / 10 steps\n",
      "| epoch: 71 | step: 14160 | loss: 0.54562 | grad_norm: 0.01154 | 21.567 sec / 10 steps\n",
      "| epoch: 71 | step: 14170 | loss: 0.51941 | grad_norm: 0.00543 | 23.480 sec / 10 steps\n",
      "| epoch: 71 | step: 14180 | loss: 0.56307 | grad_norm: 0.01045 | 22.285 sec / 10 steps\n",
      "| epoch: 71 | step: 14190 | loss: 0.48559 | grad_norm: 0.00492 | 24.064 sec / 10 steps\n",
      "| epoch: 71 | step: 14200 | loss: 0.51665 | grad_norm: 0.00957 | 23.012 sec / 10 steps\n",
      "| epoch: 71 | step: 14210 | loss: 0.49178 | grad_norm: 0.01079 | 26.030 sec / 10 steps\n",
      "| epoch: 71 | step: 14220 | loss: 0.53928 | grad_norm: 0.00587 | 23.753 sec / 10 steps\n",
      "| epoch: 71 | step: 14230 | loss: 0.48951 | grad_norm: 0.00580 | 21.729 sec / 10 steps\n",
      "| epoch: 71 | step: 14240 | loss: 0.52868 | grad_norm: 0.00431 | 21.641 sec / 10 steps\n",
      "| epoch: 71 | step: 14250 | loss: 0.53091 | grad_norm: 0.00722 | 23.196 sec / 10 steps\n",
      "| epoch: 71 | step: 14260 | loss: 0.47467 | grad_norm: 0.00546 | 22.821 sec / 10 steps\n",
      "| epoch: 71 | step: 14270 | loss: 0.52221 | grad_norm: 0.00579 | 24.536 sec / 10 steps\n",
      "| epoch: 72 | step: 14280 | loss: 0.48974 | grad_norm: 0.00769 | 22.864 sec / 10 steps\n",
      "| epoch: 72 | step: 14290 | loss: 0.53909 | grad_norm: 0.00879 | 22.184 sec / 10 steps\n",
      "| epoch: 72 | step: 14300 | loss: 0.54221 | grad_norm: 0.00845 | 21.757 sec / 10 steps\n",
      "| epoch: 72 | step: 14310 | loss: 0.55638 | grad_norm: 0.00685 | 22.079 sec / 10 steps\n",
      "| epoch: 72 | step: 14320 | loss: 0.52410 | grad_norm: 0.00624 | 22.565 sec / 10 steps\n",
      "| epoch: 72 | step: 14330 | loss: 0.49721 | grad_norm: 0.00655 | 22.702 sec / 10 steps\n",
      "| epoch: 72 | step: 14340 | loss: 0.50780 | grad_norm: 0.00562 | 23.174 sec / 10 steps\n",
      "| epoch: 72 | step: 14350 | loss: 0.47873 | grad_norm: 0.00831 | 24.007 sec / 10 steps\n",
      "| epoch: 72 | step: 14360 | loss: 0.52513 | grad_norm: 0.00908 | 22.740 sec / 10 steps\n",
      "| epoch: 72 | step: 14370 | loss: 0.51158 | grad_norm: 0.00534 | 22.635 sec / 10 steps\n",
      "| epoch: 72 | step: 14380 | loss: 0.51095 | grad_norm: 0.00732 | 23.043 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 72 | step: 14390 | loss: 0.54100 | grad_norm: 0.00939 | 23.836 sec / 10 steps\n",
      "| epoch: 72 | step: 14400 | loss: 0.52438 | grad_norm: 0.00883 | 22.967 sec / 10 steps\n",
      "| epoch: 72 | step: 14410 | loss: 0.49748 | grad_norm: 0.00510 | 23.050 sec / 10 steps\n",
      "| epoch: 72 | step: 14420 | loss: 0.54175 | grad_norm: 0.01016 | 24.932 sec / 10 steps\n",
      "| epoch: 72 | step: 14430 | loss: 0.56062 | grad_norm: 0.00733 | 21.444 sec / 10 steps\n",
      "| epoch: 72 | step: 14440 | loss: 0.54934 | grad_norm: 0.02298 | 22.359 sec / 10 steps\n",
      "| epoch: 72 | step: 14450 | loss: 0.55636 | grad_norm: 0.00630 | 21.889 sec / 10 steps\n",
      "| epoch: 72 | step: 14460 | loss: 0.51883 | grad_norm: 0.00926 | 23.525 sec / 10 steps\n",
      "| epoch: 72 | step: 14470 | loss: 0.49432 | grad_norm: 0.00683 | 21.985 sec / 10 steps\n",
      "| epoch: 73 | step: 14480 | loss: 0.52013 | grad_norm: 0.00844 | 23.536 sec / 10 steps\n",
      "| epoch: 73 | step: 14490 | loss: 0.50509 | grad_norm: 0.01139 | 22.762 sec / 10 steps\n",
      "| epoch: 73 | step: 14500 | loss: 0.53574 | grad_norm: 0.00762 | 22.760 sec / 10 steps\n",
      "| epoch: 73 | step: 14510 | loss: 0.54594 | grad_norm: 0.00491 | 24.217 sec / 10 steps\n",
      "| epoch: 73 | step: 14520 | loss: 0.54469 | grad_norm: 0.00929 | 22.484 sec / 10 steps\n",
      "| epoch: 73 | step: 14530 | loss: 0.53183 | grad_norm: 0.00608 | 23.281 sec / 10 steps\n",
      "| epoch: 73 | step: 14540 | loss: 0.52293 | grad_norm: 0.00701 | 22.813 sec / 10 steps\n",
      "| epoch: 73 | step: 14550 | loss: 0.57268 | grad_norm: 0.00923 | 22.056 sec / 10 steps\n",
      "| epoch: 73 | step: 14560 | loss: 0.49631 | grad_norm: 0.00574 | 23.022 sec / 10 steps\n",
      "| epoch: 73 | step: 14570 | loss: 0.56608 | grad_norm: 0.00566 | 20.688 sec / 10 steps\n",
      "| epoch: 73 | step: 14580 | loss: 0.55044 | grad_norm: 0.00627 | 22.257 sec / 10 steps\n",
      "| epoch: 73 | step: 14590 | loss: 0.54213 | grad_norm: 0.00680 | 24.319 sec / 10 steps\n",
      "| epoch: 73 | step: 14600 | loss: 0.52298 | grad_norm: 0.00512 | 22.189 sec / 10 steps\n",
      "| epoch: 73 | step: 14610 | loss: 0.53185 | grad_norm: 0.00784 | 24.252 sec / 10 steps\n",
      "| epoch: 73 | step: 14620 | loss: 0.54996 | grad_norm: 0.01292 | 22.410 sec / 10 steps\n",
      "| epoch: 73 | step: 14630 | loss: 0.53805 | grad_norm: 0.00968 | 22.744 sec / 10 steps\n",
      "| epoch: 73 | step: 14640 | loss: 0.54088 | grad_norm: 0.00912 | 21.849 sec / 10 steps\n",
      "| epoch: 73 | step: 14650 | loss: 0.51105 | grad_norm: 0.00577 | 23.777 sec / 10 steps\n",
      "| epoch: 73 | step: 14660 | loss: 0.55841 | grad_norm: 0.00716 | 23.110 sec / 10 steps\n",
      "| epoch: 73 | step: 14670 | loss: 0.50609 | grad_norm: 0.00637 | 21.344 sec / 10 steps\n",
      "| epoch: 74 | step: 14680 | loss: 0.55927 | grad_norm: 0.00538 | 22.724 sec / 10 steps\n",
      "| epoch: 74 | step: 14690 | loss: 0.52372 | grad_norm: 0.00563 | 22.036 sec / 10 steps\n",
      "| epoch: 74 | step: 14700 | loss: 0.52338 | grad_norm: 0.00979 | 21.491 sec / 10 steps\n",
      "| epoch: 74 | step: 14710 | loss: 0.51648 | grad_norm: 0.00628 | 23.750 sec / 10 steps\n",
      "| epoch: 74 | step: 14720 | loss: 0.47411 | grad_norm: 0.00591 | 23.011 sec / 10 steps\n",
      "| epoch: 74 | step: 14730 | loss: 0.49332 | grad_norm: 0.00413 | 22.664 sec / 10 steps\n",
      "| epoch: 74 | step: 14740 | loss: 0.52128 | grad_norm: 0.00452 | 23.314 sec / 10 steps\n",
      "| epoch: 74 | step: 14750 | loss: 0.52679 | grad_norm: 0.00704 | 21.779 sec / 10 steps\n",
      "| epoch: 74 | step: 14760 | loss: 0.52443 | grad_norm: 0.00726 | 23.865 sec / 10 steps\n",
      "| epoch: 74 | step: 14770 | loss: 0.54046 | grad_norm: 0.00576 | 22.816 sec / 10 steps\n",
      "| epoch: 74 | step: 14780 | loss: 0.48102 | grad_norm: 0.00586 | 24.163 sec / 10 steps\n",
      "| epoch: 74 | step: 14790 | loss: 0.52784 | grad_norm: 0.00756 | 22.965 sec / 10 steps\n",
      "| epoch: 74 | step: 14800 | loss: 0.54324 | grad_norm: 0.00483 | 24.047 sec / 10 steps\n",
      "| epoch: 74 | step: 14810 | loss: 0.54172 | grad_norm: 0.00578 | 23.989 sec / 10 steps\n",
      "| epoch: 74 | step: 14820 | loss: 0.51894 | grad_norm: 0.00603 | 22.827 sec / 10 steps\n",
      "| epoch: 74 | step: 14830 | loss: 0.51134 | grad_norm: 0.00692 | 20.567 sec / 10 steps\n",
      "| epoch: 74 | step: 14840 | loss: 0.53311 | grad_norm: 0.00633 | 23.089 sec / 10 steps\n",
      "| epoch: 74 | step: 14850 | loss: 0.52043 | grad_norm: 0.00623 | 22.104 sec / 10 steps\n",
      "| epoch: 74 | step: 14860 | loss: 0.53588 | grad_norm: 0.00769 | 24.268 sec / 10 steps\n",
      "| epoch: 74 | step: 14870 | loss: 0.52141 | grad_norm: 0.01052 | 24.412 sec / 10 steps\n",
      "| epoch: 75 | step: 14880 | loss: 0.50597 | grad_norm: 0.00534 | 23.778 sec / 10 steps\n",
      "| epoch: 75 | step: 14890 | loss: 0.56038 | grad_norm: 0.00854 | 23.132 sec / 10 steps\n",
      "| epoch: 75 | step: 14900 | loss: 0.51913 | grad_norm: 0.00469 | 23.074 sec / 10 steps\n",
      "| epoch: 75 | step: 14910 | loss: 0.52142 | grad_norm: 0.00428 | 23.526 sec / 10 steps\n",
      "| epoch: 75 | step: 14920 | loss: 0.53219 | grad_norm: 0.00954 | 22.764 sec / 10 steps\n",
      "| epoch: 75 | step: 14930 | loss: 0.48031 | grad_norm: 0.00687 | 23.614 sec / 10 steps\n",
      "| epoch: 75 | step: 14940 | loss: 0.52575 | grad_norm: 0.01059 | 24.150 sec / 10 steps\n",
      "| epoch: 75 | step: 14950 | loss: 0.54791 | grad_norm: 0.00510 | 23.814 sec / 10 steps\n",
      "| epoch: 75 | step: 14960 | loss: 0.51283 | grad_norm: 0.01358 | 22.270 sec / 10 steps\n",
      "| epoch: 75 | step: 14970 | loss: 0.55335 | grad_norm: 0.00703 | 22.879 sec / 10 steps\n",
      "| epoch: 75 | step: 14980 | loss: 0.51062 | grad_norm: 0.00926 | 22.772 sec / 10 steps\n",
      "| epoch: 75 | step: 14990 | loss: 0.53011 | grad_norm: 0.00761 | 21.450 sec / 10 steps\n",
      "| epoch: 75 | step: 15000 | loss: 0.53319 | grad_norm: 0.00488 | 23.366 sec / 10 steps\n",
      "| epoch: 75 | step: 15010 | loss: 0.54201 | grad_norm: 0.00636 | 24.613 sec / 10 steps\n",
      "| epoch: 75 | step: 15020 | loss: 0.53525 | grad_norm: 0.00772 | 22.434 sec / 10 steps\n",
      "| epoch: 75 | step: 15030 | loss: 0.49935 | grad_norm: 0.00570 | 22.887 sec / 10 steps\n",
      "| epoch: 75 | step: 15040 | loss: 0.52283 | grad_norm: 0.00835 | 23.216 sec / 10 steps\n",
      "| epoch: 75 | step: 15050 | loss: 0.56272 | grad_norm: 0.01079 | 21.938 sec / 10 steps\n",
      "| epoch: 75 | step: 15060 | loss: 0.53562 | grad_norm: 0.00694 | 22.946 sec / 10 steps\n",
      "| epoch: 75 | step: 15070 | loss: 0.53089 | grad_norm: 0.00512 | 22.789 sec / 10 steps\n",
      "| epoch: 76 | step: 15080 | loss: 0.53330 | grad_norm: 0.00517 | 22.559 sec / 10 steps\n",
      "| epoch: 76 | step: 15090 | loss: 0.54207 | grad_norm: 0.00508 | 20.897 sec / 10 steps\n",
      "| epoch: 76 | step: 15100 | loss: 0.55228 | grad_norm: 0.01048 | 24.922 sec / 10 steps\n",
      "| epoch: 76 | step: 15110 | loss: 0.50206 | grad_norm: 0.00416 | 24.286 sec / 10 steps\n",
      "| epoch: 76 | step: 15120 | loss: 0.48972 | grad_norm: 0.01289 | 22.510 sec / 10 steps\n",
      "| epoch: 76 | step: 15130 | loss: 0.50198 | grad_norm: 0.00522 | 22.489 sec / 10 steps\n",
      "| epoch: 76 | step: 15140 | loss: 0.53173 | grad_norm: 0.00553 | 21.086 sec / 10 steps\n",
      "| epoch: 76 | step: 15150 | loss: 0.55911 | grad_norm: 0.01446 | 24.402 sec / 10 steps\n",
      "| epoch: 76 | step: 15160 | loss: 0.54774 | grad_norm: 0.00647 | 22.215 sec / 10 steps\n",
      "| epoch: 76 | step: 15170 | loss: 0.45135 | grad_norm: 0.00502 | 22.160 sec / 10 steps\n",
      "| epoch: 76 | step: 15180 | loss: 0.50784 | grad_norm: 0.00789 | 23.316 sec / 10 steps\n",
      "| epoch: 76 | step: 15190 | loss: 0.50592 | grad_norm: 0.00631 | 21.931 sec / 10 steps\n",
      "| epoch: 76 | step: 15200 | loss: 0.49011 | grad_norm: 0.00627 | 22.779 sec / 10 steps\n",
      "| epoch: 76 | step: 15210 | loss: 0.51286 | grad_norm: 0.00537 | 22.513 sec / 10 steps\n",
      "| epoch: 76 | step: 15220 | loss: 0.54726 | grad_norm: 0.00544 | 22.264 sec / 10 steps\n",
      "| epoch: 76 | step: 15230 | loss: 0.50663 | grad_norm: 0.00660 | 24.493 sec / 10 steps\n",
      "| epoch: 76 | step: 15240 | loss: 0.51857 | grad_norm: 0.00568 | 25.308 sec / 10 steps\n",
      "| epoch: 76 | step: 15250 | loss: 0.52729 | grad_norm: 0.00982 | 24.393 sec / 10 steps\n",
      "| epoch: 76 | step: 15260 | loss: 0.53032 | grad_norm: 0.00772 | 22.332 sec / 10 steps\n",
      "| epoch: 76 | step: 15270 | loss: 0.51871 | grad_norm: 0.00763 | 22.726 sec / 10 steps\n",
      "| epoch: 77 | step: 15280 | loss: 0.51894 | grad_norm: 0.00921 | 22.792 sec / 10 steps\n",
      "| epoch: 77 | step: 15290 | loss: 0.54364 | grad_norm: 0.00617 | 22.097 sec / 10 steps\n",
      "| epoch: 77 | step: 15300 | loss: 0.53034 | grad_norm: 0.00698 | 21.701 sec / 10 steps\n",
      "| epoch: 77 | step: 15310 | loss: 0.51204 | grad_norm: 0.00670 | 24.628 sec / 10 steps\n",
      "| epoch: 77 | step: 15320 | loss: 0.55589 | grad_norm: 0.00666 | 21.957 sec / 10 steps\n",
      "| epoch: 77 | step: 15330 | loss: 0.55454 | grad_norm: 0.01218 | 24.172 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 77 | step: 15340 | loss: 0.48095 | grad_norm: 0.00755 | 23.557 sec / 10 steps\n",
      "| epoch: 77 | step: 15350 | loss: 0.51662 | grad_norm: 0.00825 | 23.313 sec / 10 steps\n",
      "| epoch: 77 | step: 15360 | loss: 0.51219 | grad_norm: 0.01481 | 25.027 sec / 10 steps\n",
      "| epoch: 77 | step: 15370 | loss: 0.52371 | grad_norm: 0.00627 | 22.571 sec / 10 steps\n",
      "| epoch: 77 | step: 15380 | loss: 0.54328 | grad_norm: 0.00928 | 22.246 sec / 10 steps\n",
      "| epoch: 77 | step: 15390 | loss: 0.50028 | grad_norm: 0.00633 | 23.802 sec / 10 steps\n",
      "| epoch: 77 | step: 15400 | loss: 0.53439 | grad_norm: 0.01560 | 22.084 sec / 10 steps\n",
      "| epoch: 77 | step: 15410 | loss: 0.47864 | grad_norm: 0.00666 | 23.495 sec / 10 steps\n",
      "| epoch: 77 | step: 15420 | loss: 0.53715 | grad_norm: 0.00507 | 23.976 sec / 10 steps\n",
      "| epoch: 77 | step: 15430 | loss: 0.45727 | grad_norm: 0.00380 | 23.166 sec / 10 steps\n",
      "| epoch: 77 | step: 15440 | loss: 0.53411 | grad_norm: 0.00495 | 22.485 sec / 10 steps\n",
      "| epoch: 77 | step: 15450 | loss: 0.53922 | grad_norm: 0.00611 | 20.421 sec / 10 steps\n",
      "| epoch: 77 | step: 15460 | loss: 0.52467 | grad_norm: 0.00586 | 23.552 sec / 10 steps\n",
      "| epoch: 77 | step: 15470 | loss: 0.54415 | grad_norm: 0.00782 | 23.349 sec / 10 steps\n",
      "| epoch: 78 | step: 15480 | loss: 0.52555 | grad_norm: 0.00471 | 22.588 sec / 10 steps\n",
      "| epoch: 78 | step: 15490 | loss: 0.52104 | grad_norm: 0.00670 | 22.232 sec / 10 steps\n",
      "| epoch: 78 | step: 15500 | loss: 0.52408 | grad_norm: 0.00618 | 22.552 sec / 10 steps\n",
      "| epoch: 78 | step: 15510 | loss: 0.53206 | grad_norm: 0.01299 | 23.412 sec / 10 steps\n",
      "| epoch: 78 | step: 15520 | loss: 0.48962 | grad_norm: 0.00406 | 24.051 sec / 10 steps\n",
      "| epoch: 78 | step: 15530 | loss: 0.52614 | grad_norm: 0.00680 | 23.604 sec / 10 steps\n",
      "| epoch: 78 | step: 15540 | loss: 0.47828 | grad_norm: 0.00589 | 22.659 sec / 10 steps\n",
      "| epoch: 78 | step: 15550 | loss: 0.53492 | grad_norm: 0.00806 | 21.455 sec / 10 steps\n",
      "| epoch: 78 | step: 15560 | loss: 0.46084 | grad_norm: 0.00876 | 22.395 sec / 10 steps\n",
      "| epoch: 78 | step: 15570 | loss: 0.57279 | grad_norm: 0.03372 | 22.009 sec / 10 steps\n",
      "| epoch: 78 | step: 15580 | loss: 0.52179 | grad_norm: 0.00901 | 21.205 sec / 10 steps\n",
      "| epoch: 78 | step: 15590 | loss: 0.52259 | grad_norm: 0.00789 | 23.382 sec / 10 steps\n",
      "| epoch: 78 | step: 15600 | loss: 0.53356 | grad_norm: 0.01313 | 22.679 sec / 10 steps\n",
      "| epoch: 78 | step: 15610 | loss: 0.51147 | grad_norm: 0.00780 | 22.794 sec / 10 steps\n",
      "| epoch: 78 | step: 15620 | loss: 0.53498 | grad_norm: 0.01373 | 23.608 sec / 10 steps\n",
      "| epoch: 78 | step: 15630 | loss: 0.53052 | grad_norm: 0.00743 | 23.304 sec / 10 steps\n",
      "| epoch: 78 | step: 15640 | loss: 0.48469 | grad_norm: 0.00901 | 24.618 sec / 10 steps\n",
      "| epoch: 78 | step: 15650 | loss: 0.46051 | grad_norm: 0.00817 | 24.728 sec / 10 steps\n",
      "| epoch: 78 | step: 15660 | loss: 0.50609 | grad_norm: 0.01111 | 22.659 sec / 10 steps\n",
      "| epoch: 78 | step: 15670 | loss: 0.53403 | grad_norm: 0.00827 | 22.933 sec / 10 steps\n",
      "| epoch: 79 | step: 15680 | loss: 0.48102 | grad_norm: 0.00567 | 22.351 sec / 10 steps\n",
      "| epoch: 79 | step: 15690 | loss: 0.50016 | grad_norm: 0.02057 | 22.265 sec / 10 steps\n",
      "| epoch: 79 | step: 15700 | loss: 0.52795 | grad_norm: 0.00837 | 24.489 sec / 10 steps\n",
      "| epoch: 79 | step: 15710 | loss: 0.50449 | grad_norm: 0.00771 | 22.920 sec / 10 steps\n",
      "| epoch: 79 | step: 15720 | loss: 0.50991 | grad_norm: 0.00578 | 21.378 sec / 10 steps\n",
      "| epoch: 79 | step: 15730 | loss: 0.53459 | grad_norm: 0.01030 | 24.375 sec / 10 steps\n",
      "| epoch: 79 | step: 15740 | loss: 0.53709 | grad_norm: 0.00711 | 22.505 sec / 10 steps\n",
      "| epoch: 79 | step: 15750 | loss: 0.48370 | grad_norm: 0.01726 | 24.101 sec / 10 steps\n",
      "| epoch: 79 | step: 15760 | loss: 0.52909 | grad_norm: 0.00508 | 21.555 sec / 10 steps\n",
      "| epoch: 79 | step: 15770 | loss: 0.50985 | grad_norm: 0.00367 | 20.988 sec / 10 steps\n",
      "| epoch: 79 | step: 15780 | loss: 0.53522 | grad_norm: 0.00658 | 23.200 sec / 10 steps\n",
      "| epoch: 79 | step: 15790 | loss: 0.50758 | grad_norm: 0.00657 | 23.234 sec / 10 steps\n",
      "| epoch: 79 | step: 15800 | loss: 0.53146 | grad_norm: 0.00961 | 21.698 sec / 10 steps\n",
      "| epoch: 79 | step: 15810 | loss: 0.52954 | grad_norm: 0.00632 | 23.381 sec / 10 steps\n",
      "| epoch: 79 | step: 15820 | loss: 0.54222 | grad_norm: 0.00479 | 22.474 sec / 10 steps\n",
      "| epoch: 79 | step: 15830 | loss: 0.51819 | grad_norm: 0.00532 | 22.408 sec / 10 steps\n",
      "| epoch: 79 | step: 15840 | loss: 0.51531 | grad_norm: 0.00677 | 23.924 sec / 10 steps\n",
      "| epoch: 79 | step: 15850 | loss: 0.53182 | grad_norm: 0.00410 | 23.391 sec / 10 steps\n",
      "| epoch: 79 | step: 15860 | loss: 0.51198 | grad_norm: 0.00778 | 24.389 sec / 10 steps\n",
      "| epoch: 79 | step: 15870 | loss: 0.53201 | grad_norm: 0.01105 | 22.083 sec / 10 steps\n",
      "| epoch: 80 | step: 15880 | loss: 0.53447 | grad_norm: 0.00611 | 22.889 sec / 10 steps\n",
      "| epoch: 80 | step: 15890 | loss: 0.52358 | grad_norm: 0.00516 | 21.864 sec / 10 steps\n",
      "| epoch: 80 | step: 15900 | loss: 0.50180 | grad_norm: 0.01083 | 23.676 sec / 10 steps\n",
      "| epoch: 80 | step: 15910 | loss: 0.54155 | grad_norm: 0.00659 | 25.107 sec / 10 steps\n",
      "| epoch: 80 | step: 15920 | loss: 0.52339 | grad_norm: 0.00426 | 21.147 sec / 10 steps\n",
      "| epoch: 80 | step: 15930 | loss: 0.52388 | grad_norm: 0.00812 | 21.518 sec / 10 steps\n",
      "| epoch: 80 | step: 15940 | loss: 0.50216 | grad_norm: 0.00651 | 23.019 sec / 10 steps\n",
      "| epoch: 80 | step: 15950 | loss: 0.49927 | grad_norm: 0.00661 | 22.607 sec / 10 steps\n",
      "| epoch: 80 | step: 15960 | loss: 0.50588 | grad_norm: 0.00779 | 21.906 sec / 10 steps\n",
      "| epoch: 80 | step: 15970 | loss: 0.51953 | grad_norm: 0.00756 | 23.646 sec / 10 steps\n",
      "| epoch: 80 | step: 15980 | loss: 0.55058 | grad_norm: 0.00872 | 21.468 sec / 10 steps\n",
      "| epoch: 80 | step: 15990 | loss: 0.52833 | grad_norm: 0.00530 | 24.053 sec / 10 steps\n",
      "| epoch: 80 | step: 16000 | loss: 0.49924 | grad_norm: 0.00700 | 22.965 sec / 10 steps\n",
      "| epoch: 80 | step: 16010 | loss: 0.51480 | grad_norm: 0.00615 | 24.841 sec / 10 steps\n",
      "| epoch: 80 | step: 16020 | loss: 0.52472 | grad_norm: 0.00629 | 21.281 sec / 10 steps\n",
      "| epoch: 80 | step: 16030 | loss: 0.53417 | grad_norm: 0.00891 | 22.695 sec / 10 steps\n",
      "| epoch: 80 | step: 16040 | loss: 0.52290 | grad_norm: 0.00519 | 26.128 sec / 10 steps\n",
      "| epoch: 80 | step: 16050 | loss: 0.49410 | grad_norm: 0.00691 | 23.809 sec / 10 steps\n",
      "| epoch: 80 | step: 16060 | loss: 0.53970 | grad_norm: 0.00851 | 20.880 sec / 10 steps\n",
      "| epoch: 80 | step: 16070 | loss: 0.53761 | grad_norm: 0.01677 | 23.237 sec / 10 steps\n",
      "| epoch: 80 | step: 16080 | loss: 0.53860 | grad_norm: 0.01750 | 20.939 sec / 10 steps\n",
      "| epoch: 81 | step: 16090 | loss: 0.55451 | grad_norm: 0.00717 | 24.083 sec / 10 steps\n",
      "| epoch: 81 | step: 16100 | loss: 0.53387 | grad_norm: 0.00962 | 21.023 sec / 10 steps\n",
      "| epoch: 81 | step: 16110 | loss: 0.51200 | grad_norm: 0.01084 | 23.540 sec / 10 steps\n",
      "| epoch: 81 | step: 16120 | loss: 0.51924 | grad_norm: 0.00465 | 21.040 sec / 10 steps\n",
      "| epoch: 81 | step: 16130 | loss: 0.52915 | grad_norm: 0.00987 | 22.646 sec / 10 steps\n",
      "| epoch: 81 | step: 16140 | loss: 0.52712 | grad_norm: 0.00613 | 24.252 sec / 10 steps\n",
      "| epoch: 81 | step: 16150 | loss: 0.48115 | grad_norm: 0.00489 | 22.576 sec / 10 steps\n",
      "| epoch: 81 | step: 16160 | loss: 0.54075 | grad_norm: 0.00520 | 21.414 sec / 10 steps\n",
      "| epoch: 81 | step: 16170 | loss: 0.53061 | grad_norm: 0.00667 | 23.435 sec / 10 steps\n",
      "| epoch: 81 | step: 16180 | loss: 0.50226 | grad_norm: 0.01303 | 22.652 sec / 10 steps\n",
      "| epoch: 81 | step: 16190 | loss: 0.49994 | grad_norm: 0.00646 | 22.229 sec / 10 steps\n",
      "| epoch: 81 | step: 16200 | loss: 0.49673 | grad_norm: 0.00464 | 23.818 sec / 10 steps\n",
      "| epoch: 81 | step: 16210 | loss: 0.51333 | grad_norm: 0.00489 | 22.479 sec / 10 steps\n",
      "| epoch: 81 | step: 16220 | loss: 0.54369 | grad_norm: 0.00727 | 24.196 sec / 10 steps\n",
      "| epoch: 81 | step: 16230 | loss: 0.54004 | grad_norm: 0.00678 | 22.907 sec / 10 steps\n",
      "| epoch: 81 | step: 16240 | loss: 0.51063 | grad_norm: 0.00806 | 21.528 sec / 10 steps\n",
      "| epoch: 81 | step: 16250 | loss: 0.52450 | grad_norm: 0.00607 | 22.325 sec / 10 steps\n",
      "| epoch: 81 | step: 16260 | loss: 0.46941 | grad_norm: 0.00497 | 22.991 sec / 10 steps\n",
      "| epoch: 81 | step: 16270 | loss: 0.47543 | grad_norm: 0.00411 | 24.281 sec / 10 steps\n",
      "| epoch: 81 | step: 16280 | loss: 0.51955 | grad_norm: 0.00467 | 23.147 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 82 | step: 16290 | loss: 0.57686 | grad_norm: 0.00786 | 21.490 sec / 10 steps\n",
      "| epoch: 82 | step: 16300 | loss: 0.45852 | grad_norm: 0.00657 | 23.760 sec / 10 steps\n",
      "| epoch: 82 | step: 16310 | loss: 0.47924 | grad_norm: 0.00681 | 24.918 sec / 10 steps\n",
      "| epoch: 82 | step: 16320 | loss: 0.51596 | grad_norm: 0.00881 | 23.873 sec / 10 steps\n",
      "| epoch: 82 | step: 16330 | loss: 0.49979 | grad_norm: 0.00513 | 21.855 sec / 10 steps\n",
      "| epoch: 82 | step: 16340 | loss: 0.50387 | grad_norm: 0.00835 | 22.418 sec / 10 steps\n",
      "| epoch: 82 | step: 16350 | loss: 0.51918 | grad_norm: 0.00665 | 21.644 sec / 10 steps\n",
      "| epoch: 82 | step: 16360 | loss: 0.52677 | grad_norm: 0.00668 | 22.021 sec / 10 steps\n",
      "| epoch: 82 | step: 16370 | loss: 0.55177 | grad_norm: 0.00548 | 21.712 sec / 10 steps\n",
      "| epoch: 82 | step: 16380 | loss: 0.50254 | grad_norm: 0.00474 | 24.036 sec / 10 steps\n",
      "| epoch: 82 | step: 16390 | loss: 0.55023 | grad_norm: 0.00974 | 21.511 sec / 10 steps\n",
      "| epoch: 82 | step: 16400 | loss: 0.50938 | grad_norm: 0.00424 | 23.590 sec / 10 steps\n",
      "| epoch: 82 | step: 16410 | loss: 0.52271 | grad_norm: 0.00434 | 22.982 sec / 10 steps\n",
      "| epoch: 82 | step: 16420 | loss: 0.57273 | grad_norm: 0.00487 | 21.842 sec / 10 steps\n",
      "| epoch: 82 | step: 16430 | loss: 0.51570 | grad_norm: 0.01306 | 23.211 sec / 10 steps\n",
      "| epoch: 82 | step: 16440 | loss: 0.50796 | grad_norm: 0.00511 | 22.495 sec / 10 steps\n",
      "| epoch: 82 | step: 16450 | loss: 0.51429 | grad_norm: 0.01222 | 21.924 sec / 10 steps\n",
      "| epoch: 82 | step: 16460 | loss: 0.50364 | grad_norm: 0.00340 | 23.204 sec / 10 steps\n",
      "| epoch: 82 | step: 16470 | loss: 0.49291 | grad_norm: 0.00421 | 24.090 sec / 10 steps\n",
      "| epoch: 82 | step: 16480 | loss: 0.54308 | grad_norm: 0.01187 | 21.686 sec / 10 steps\n",
      "| epoch: 83 | step: 16490 | loss: 0.52936 | grad_norm: 0.00626 | 22.471 sec / 10 steps\n",
      "| epoch: 83 | step: 16500 | loss: 0.47804 | grad_norm: 0.00672 | 22.501 sec / 10 steps\n",
      "| epoch: 83 | step: 16510 | loss: 0.52143 | grad_norm: 0.00447 | 22.617 sec / 10 steps\n",
      "| epoch: 83 | step: 16520 | loss: 0.55249 | grad_norm: 0.01295 | 23.972 sec / 10 steps\n",
      "| epoch: 83 | step: 16530 | loss: 0.54987 | grad_norm: 0.00981 | 24.438 sec / 10 steps\n",
      "| epoch: 83 | step: 16540 | loss: 0.54520 | grad_norm: 0.00611 | 22.598 sec / 10 steps\n",
      "| epoch: 83 | step: 16550 | loss: 0.51579 | grad_norm: 0.00629 | 23.291 sec / 10 steps\n",
      "| epoch: 83 | step: 16560 | loss: 0.47090 | grad_norm: 0.00561 | 23.933 sec / 10 steps\n",
      "| epoch: 83 | step: 16570 | loss: 0.50378 | grad_norm: 0.00543 | 24.066 sec / 10 steps\n",
      "| epoch: 83 | step: 16580 | loss: 0.53324 | grad_norm: 0.01520 | 21.307 sec / 10 steps\n",
      "| epoch: 83 | step: 16590 | loss: 0.50638 | grad_norm: 0.00758 | 24.181 sec / 10 steps\n",
      "| epoch: 83 | step: 16600 | loss: 0.52531 | grad_norm: 0.00839 | 22.271 sec / 10 steps\n",
      "| epoch: 83 | step: 16610 | loss: 0.53967 | grad_norm: 0.00833 | 22.564 sec / 10 steps\n",
      "| epoch: 83 | step: 16620 | loss: 0.54785 | grad_norm: 0.00620 | 23.135 sec / 10 steps\n",
      "| epoch: 83 | step: 16630 | loss: 0.51886 | grad_norm: 0.00687 | 20.992 sec / 10 steps\n",
      "| epoch: 83 | step: 16640 | loss: 0.53812 | grad_norm: 0.00602 | 22.450 sec / 10 steps\n",
      "| epoch: 83 | step: 16650 | loss: 0.53878 | grad_norm: 0.02073 | 22.961 sec / 10 steps\n",
      "| epoch: 83 | step: 16660 | loss: 0.49648 | grad_norm: 0.00635 | 23.702 sec / 10 steps\n",
      "| epoch: 83 | step: 16670 | loss: 0.54114 | grad_norm: 0.01031 | 22.286 sec / 10 steps\n",
      "| epoch: 83 | step: 16680 | loss: 0.54403 | grad_norm: 0.00748 | 23.558 sec / 10 steps\n",
      "| epoch: 84 | step: 16690 | loss: 0.57137 | grad_norm: 0.00651 | 24.675 sec / 10 steps\n",
      "| epoch: 84 | step: 16700 | loss: 0.50342 | grad_norm: 0.00413 | 22.605 sec / 10 steps\n",
      "| epoch: 84 | step: 16710 | loss: 0.54470 | grad_norm: 0.00602 | 21.956 sec / 10 steps\n",
      "| epoch: 84 | step: 16720 | loss: 0.52256 | grad_norm: 0.00570 | 31.120 sec / 10 steps\n",
      "| epoch: 84 | step: 16730 | loss: 0.54061 | grad_norm: 0.00848 | 24.502 sec / 10 steps\n",
      "| epoch: 84 | step: 16740 | loss: 0.53342 | grad_norm: 0.00820 | 22.611 sec / 10 steps\n",
      "| epoch: 84 | step: 16750 | loss: 0.54230 | grad_norm: 0.00737 | 23.612 sec / 10 steps\n",
      "| epoch: 84 | step: 16760 | loss: 0.55505 | grad_norm: 0.00487 | 22.034 sec / 10 steps\n",
      "| epoch: 84 | step: 16770 | loss: 0.47556 | grad_norm: 0.00672 | 23.024 sec / 10 steps\n",
      "| epoch: 84 | step: 16780 | loss: 0.49445 | grad_norm: 0.00608 | 22.672 sec / 10 steps\n",
      "| epoch: 84 | step: 16790 | loss: 0.53435 | grad_norm: 0.01010 | 22.099 sec / 10 steps\n",
      "| epoch: 84 | step: 16800 | loss: 0.51615 | grad_norm: 0.01162 | 22.997 sec / 10 steps\n",
      "| epoch: 84 | step: 16810 | loss: 0.52936 | grad_norm: 0.00736 | 24.656 sec / 10 steps\n",
      "| epoch: 84 | step: 16820 | loss: 0.51808 | grad_norm: 0.00424 | 22.569 sec / 10 steps\n",
      "| epoch: 84 | step: 16830 | loss: 0.51818 | grad_norm: 0.00732 | 22.375 sec / 10 steps\n",
      "| epoch: 84 | step: 16840 | loss: 0.51270 | grad_norm: 0.00445 | 23.016 sec / 10 steps\n",
      "| epoch: 84 | step: 16850 | loss: 0.52894 | grad_norm: 0.00469 | 22.486 sec / 10 steps\n",
      "| epoch: 84 | step: 16860 | loss: 0.49694 | grad_norm: 0.00321 | 22.916 sec / 10 steps\n",
      "| epoch: 84 | step: 16870 | loss: 0.53937 | grad_norm: 0.00589 | 22.393 sec / 10 steps\n",
      "| epoch: 84 | step: 16880 | loss: 0.53481 | grad_norm: 0.00599 | 23.112 sec / 10 steps\n",
      "| epoch: 85 | step: 16890 | loss: 0.51897 | grad_norm: 0.00511 | 22.962 sec / 10 steps\n",
      "| epoch: 85 | step: 16900 | loss: 0.50511 | grad_norm: 0.00628 | 24.152 sec / 10 steps\n",
      "| epoch: 85 | step: 16910 | loss: 0.53927 | grad_norm: 0.00732 | 23.768 sec / 10 steps\n",
      "| epoch: 85 | step: 16920 | loss: 0.53598 | grad_norm: 0.00746 | 21.318 sec / 10 steps\n",
      "| epoch: 85 | step: 16930 | loss: 0.54643 | grad_norm: 0.00482 | 23.151 sec / 10 steps\n",
      "| epoch: 85 | step: 16940 | loss: 0.50680 | grad_norm: 0.00400 | 24.435 sec / 10 steps\n",
      "| epoch: 85 | step: 16950 | loss: 0.51991 | grad_norm: 0.00448 | 23.451 sec / 10 steps\n",
      "| epoch: 85 | step: 16960 | loss: 0.48367 | grad_norm: 0.00573 | 23.169 sec / 10 steps\n",
      "| epoch: 85 | step: 16970 | loss: 0.51597 | grad_norm: 0.00559 | 21.971 sec / 10 steps\n",
      "| epoch: 85 | step: 16980 | loss: 0.52005 | grad_norm: 0.01142 | 22.476 sec / 10 steps\n",
      "| epoch: 85 | step: 16990 | loss: 0.50416 | grad_norm: 0.00712 | 23.795 sec / 10 steps\n",
      "| epoch: 85 | step: 17000 | loss: 0.49362 | grad_norm: 0.00586 | 25.170 sec / 10 steps\n",
      "| epoch: 85 | step: 17010 | loss: 0.52729 | grad_norm: 0.00497 | 22.806 sec / 10 steps\n",
      "| epoch: 85 | step: 17020 | loss: 0.53718 | grad_norm: 0.00587 | 22.390 sec / 10 steps\n",
      "| epoch: 85 | step: 17030 | loss: 0.55842 | grad_norm: 0.00851 | 24.584 sec / 10 steps\n",
      "| epoch: 85 | step: 17040 | loss: 0.50632 | grad_norm: 0.00766 | 23.231 sec / 10 steps\n",
      "| epoch: 85 | step: 17050 | loss: 0.54419 | grad_norm: 0.00647 | 22.931 sec / 10 steps\n",
      "| epoch: 85 | step: 17060 | loss: 0.49706 | grad_norm: 0.00559 | 22.026 sec / 10 steps\n",
      "| epoch: 85 | step: 17070 | loss: 0.51792 | grad_norm: 0.00561 | 22.230 sec / 10 steps\n",
      "| epoch: 85 | step: 17080 | loss: 0.52923 | grad_norm: 0.01059 | 23.958 sec / 10 steps\n",
      "| epoch: 86 | step: 17090 | loss: 0.55531 | grad_norm: 0.01112 | 23.352 sec / 10 steps\n",
      "| epoch: 86 | step: 17100 | loss: 0.50505 | grad_norm: 0.00414 | 23.518 sec / 10 steps\n",
      "| epoch: 86 | step: 17110 | loss: 0.55449 | grad_norm: 0.00635 | 21.625 sec / 10 steps\n",
      "| epoch: 86 | step: 17120 | loss: 0.51764 | grad_norm: 0.00713 | 21.478 sec / 10 steps\n",
      "| epoch: 86 | step: 17130 | loss: 0.52679 | grad_norm: 0.00648 | 21.978 sec / 10 steps\n",
      "| epoch: 86 | step: 17140 | loss: 0.52418 | grad_norm: 0.00523 | 23.668 sec / 10 steps\n",
      "| epoch: 86 | step: 17150 | loss: 0.52784 | grad_norm: 0.00660 | 22.172 sec / 10 steps\n",
      "| epoch: 86 | step: 17160 | loss: 0.53735 | grad_norm: 0.00614 | 22.399 sec / 10 steps\n",
      "| epoch: 86 | step: 17170 | loss: 0.52914 | grad_norm: 0.00632 | 21.519 sec / 10 steps\n",
      "| epoch: 86 | step: 17180 | loss: 0.51338 | grad_norm: 0.00385 | 20.907 sec / 10 steps\n",
      "| epoch: 86 | step: 17190 | loss: 0.53421 | grad_norm: 0.00740 | 24.110 sec / 10 steps\n",
      "| epoch: 86 | step: 17200 | loss: 0.54867 | grad_norm: 0.00832 | 22.974 sec / 10 steps\n",
      "| epoch: 86 | step: 17210 | loss: 0.54315 | grad_norm: 0.00627 | 20.932 sec / 10 steps\n",
      "| epoch: 86 | step: 17220 | loss: 0.52239 | grad_norm: 0.00516 | 22.615 sec / 10 steps\n",
      "| epoch: 86 | step: 17230 | loss: 0.49306 | grad_norm: 0.00432 | 22.989 sec / 10 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 86 | step: 17240 | loss: 0.47624 | grad_norm: 0.00526 | 23.873 sec / 10 steps\n",
      "| epoch: 86 | step: 17250 | loss: 0.52139 | grad_norm: 0.00612 | 23.844 sec / 10 steps\n",
      "| epoch: 86 | step: 17260 | loss: 0.47978 | grad_norm: 0.00543 | 24.115 sec / 10 steps\n",
      "| epoch: 86 | step: 17270 | loss: 0.53190 | grad_norm: 0.00658 | 21.417 sec / 10 steps\n",
      "| epoch: 86 | step: 17280 | loss: 0.51314 | grad_norm: 0.00455 | 22.816 sec / 10 steps\n",
      "| epoch: 87 | step: 17290 | loss: 0.48545 | grad_norm: 0.00544 | 23.383 sec / 10 steps\n",
      "| epoch: 87 | step: 17300 | loss: 0.52809 | grad_norm: 0.00992 | 23.713 sec / 10 steps\n",
      "| epoch: 87 | step: 17310 | loss: 0.52069 | grad_norm: 0.00824 | 24.537 sec / 10 steps\n",
      "| epoch: 87 | step: 17320 | loss: 0.50930 | grad_norm: 0.00662 | 21.571 sec / 10 steps\n",
      "| epoch: 87 | step: 17330 | loss: 0.53250 | grad_norm: 0.00996 | 21.636 sec / 10 steps\n",
      "| epoch: 87 | step: 17340 | loss: 0.48517 | grad_norm: 0.00590 | 24.079 sec / 10 steps\n",
      "| epoch: 87 | step: 17350 | loss: 0.53296 | grad_norm: 0.00651 | 23.023 sec / 10 steps\n",
      "| epoch: 87 | step: 17360 | loss: 0.51661 | grad_norm: 0.00521 | 21.897 sec / 10 steps\n",
      "| epoch: 87 | step: 17370 | loss: 0.48848 | grad_norm: 0.01154 | 22.603 sec / 10 steps\n",
      "| epoch: 87 | step: 17380 | loss: 0.49911 | grad_norm: 0.00982 | 23.279 sec / 10 steps\n",
      "| epoch: 87 | step: 17390 | loss: 0.48621 | grad_norm: 0.00609 | 21.826 sec / 10 steps\n",
      "| epoch: 87 | step: 17400 | loss: 0.53103 | grad_norm: 0.00841 | 23.046 sec / 10 steps\n",
      "| epoch: 87 | step: 17410 | loss: 0.48964 | grad_norm: 0.00533 | 23.853 sec / 10 steps\n",
      "| epoch: 87 | step: 17420 | loss: 0.52019 | grad_norm: 0.00462 | 22.848 sec / 10 steps\n",
      "| epoch: 87 | step: 17430 | loss: 0.52481 | grad_norm: 0.00497 | 24.025 sec / 10 steps\n",
      "| epoch: 87 | step: 17440 | loss: 0.49812 | grad_norm: 0.00448 | 22.979 sec / 10 steps\n",
      "| epoch: 87 | step: 17450 | loss: 0.50712 | grad_norm: 0.00357 | 22.387 sec / 10 steps\n",
      "| epoch: 87 | step: 17460 | loss: 0.53791 | grad_norm: 0.00654 | 23.267 sec / 10 steps\n",
      "| epoch: 87 | step: 17470 | loss: 0.51590 | grad_norm: 0.00792 | 26.925 sec / 10 steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11588\\1640530557.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11588\\2166629395.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model_name, check_step)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 기울기를 1로 cliping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train(model_name, check_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20463744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
